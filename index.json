[{"authors":["shadi-albarqouni"],"categories":null,"content":"Shadi Albarqouni is a Palestinian-German Computer Scientist. He received his B.Sc. and M.Sc. in Electrical Engineering from the IU Gaza, Palestine, in 2005, and 2010, respectively. In 2012, he received a prestigious DAAD research grant to pursue his Ph.D. at the Chair for Computer Aided Medical Procedures (CAMP), Technical University of Munich (TUM), Germany. During his Ph.D., Albarqouni worked with Prof. Nassir Navab on developing machine learning algorithms to handle noisy labels, coming from crowdsourcing, in medical imaging. Albarqouni received his Ph.D. in Computer Science with summa cum laude in 2017.\nSince then, Albarqouni has been working as a Senior Research Scientist \u0026amp; Team Lead at CAMP leading the Medical Image Analysis (MedIA) team with an emphasis on developing deep learning methods for medical applications. In 2019, he received the P.R.I.M.E. fellowship for one-year international mobility. During the period from Nov. 2019 to Jul. 2020, worked as a Visiting Scientist at the Department of Information Technology and Electrical Engineering (D-ITET) at ETH Zürich, Switzerland. He worked with Prof. Ender Konukoglu on Modeling Uncertainty in Medical Imaging, in particular, the one associated with inter-/intra-raters variability. During the period Aug.-Oct. 2020, Albarqouni worked as a Visting Scientist at the Department of Computing at Imperial College London, United Kingdom. He worked with Prof. Daniel Rueckert on Federated Learning.\nSince Nov. 2020, Albarqouni is holding an AI Young Investigator Group Leader position at Helmholtz AI. The aim of Albarqouni Lab is to develop innovative deep Federated Learning algorithms that can distill and share the knowledge among AI agents in a robust and privacy-preserved fashion.\nAlbarqouni has around 100 peer-reviewed publications in both Medical Imaging Computing and Computer Vision published in high impacted journals and top-tier conferences. He serves as a reviewer for many journals, e.g., IEEE TPAMI, MedIA, IEEE TMI, IEEE JBHI, IJCARS and Pattern Recognition, and top-tier conferences, e.g., ECCV, MICCAI, MIDL, BMVC, IPCAI, and ISBI among others. He is also an active member of ELLIS, AGYA, MICCAI, BMVA, IEEE EMBS, IEEE CS, and ESR society. Since 2015, he has been serving as a PC member for a couple of MICCAI workshops, e.g., COMPAY, and DART among others. Since 2019, Albarqouni has been serving as an Area Chair in Advance Machine Learning Theory at MICCAI.\nHis current research interests include Interpretable ML, Robustness, Uncertainty, and recently Federated Learning. He is also interested in Entrepreneurship and Startups for Innovative Medical Solutions.\n","date":1609459200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1608826908,"objectID":"59358bd427e968aae33a13f16b2440a6","permalink":"https://albarqouni.github.io/authors/shadi-albarqouni/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/shadi-albarqouni/","section":"authors","summary":"Shadi Albarqouni is a Palestinian-German Computer Scientist. He received his B.Sc. and M.Sc. in Electrical Engineering from the IU Gaza, Palestine, in 2005, and 2010, respectively. In 2012, he received a prestigious DAAD research grant to pursue his Ph.","tags":null,"title":"Shadi Albarqouni","type":"authors"},{"authors":["tariq-bdair"],"categories":null,"content":"Biography Tariq Bdair is a Software Architect \u0026amp; Technical Leader. He possesses vast experience in software development in the industry for more than 8 years. He has developed a large number of applications using different programming languages using state-of-art technologies, software engineering methodologies, and software-development-life-cycle (SDLC). Since 2018 he is pursuing his Ph.D. at the Chair for Computer Aided Medical Procedures (CAMP), Technical University of Munich (TUM), Germany, under the supervision of Prof. Nassir Navab. His research interest includes Semi-Supervised Learning, Medical Images Segmentation, and Federated Learning.\n","date":1577836800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1586645150,"objectID":"c506f8e4b236a230a2bb74f90d5e55bf","permalink":"https://albarqouni.github.io/authors/tariq-bdair/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/tariq-bdair/","section":"authors","summary":"Biography Tariq Bdair is a Software Architect \u0026amp; Technical Leader. He possesses vast experience in software development in the industry for more than 8 years. He has developed a large number of applications using different programming languages using state-of-art technologies, software engineering methodologies, and software-development-life-cycle (SDLC).","tags":null,"title":"Tariq Bdair","type":"authors"},{"authors":["admin"],"categories":null,"content":"Shadi Albarqouni is a Palestinian-German Computer Scientist. He received his B.Sc. and M.Sc. in Electrical Engineering from the IU Gaza, Palestine, in 2005, and 2010, respectively. In 2012, he received a prestigious DAAD research grant to pursue his Ph.D. at the Chair for Computer Aided Medical Procedures (CAMP), Technical University of Munich (TUM), Germany. During his Ph.D., Albarqouni worked with Prof. Nassir Navab on developing machine learning algorithms to handle noisy labels, coming from crowdsourcing, in medical imaging. Albarqouni received his Ph.D. in Computer Science with summa cum laude in 2017.\nSince then, Albarqouni has been working as a Senior Research Scientist \u0026amp; Team Lead at CAMP leading the Medical Image Analysis (MedIA) team with an emphasis on developing deep learning methods for medical applications. In 2019, he received the P.R.I.M.E. fellowship for one-year international mobility. During the period from Nov. 2019 to Jul. 2020, worked as a Visiting Scientist at the Department of Information Technology and Electrical Engineering (D-ITET) at ETH Zürich, Switzerland. He worked with Prof. Ender Konukoglu on Modeling Uncertainty in Medical Imaging, in particular, the one associated with inter-/intra-raters variability. During the period Aug.-Oct. 2020, Albarqouni worked as a Visting Scientist at the Department of Computing at Imperial College London, United Kingdom. He worked with Prof. Daniel Rueckert on Federated Learning.\nSince Nov. 2020, Albarqouni is holding an AI Young Investigator Group Leader position at Helmholtz AI. The aim of Albarqouni Lab is to develop innovative deep Federated Learning algorithms that can distill and share the knowledge among AI agents in a robust and privacy-preserved fashion.\nAlbarqouni has around 100 peer-reviewed publications in both Medical Imaging Computing and Computer Vision published in high impacted journals and top-tier conferences. He serves as a reviewer for many journals, e.g., IEEE TPAMI, MedIA, IEEE TMI, IEEE JBHI, IJCARS and Pattern Recognition, and top-tier conferences, e.g., ECCV, MICCAI, MIDL, BMVC, IPCAI, and ISBI among others. He is also an active member of ELLIS, AGYA, MICCAI, BMVA, IEEE EMBS, IEEE CS, and ESR society. Since 2015, he has been serving as a PC member for a couple of MICCAI workshops, e.g., COMPAY, and DART among others. Since 2019, Albarqouni has been serving as an Area Chair in Advance Machine Learning Theory at MICCAI.\nHis current research interests include Interpretable ML, Robustness, Uncertainty, and recently Federated Learning. He is also interested in Entrepreneurship and Startups for Innovative Medical Solutions.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1604687686,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://albarqouni.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Shadi Albarqouni is a Palestinian-German Computer Scientist. He received his B.Sc. and M.Sc. in Electrical Engineering from the IU Gaza, Palestine, in 2005, and 2010, respectively. In 2012, he received a prestigious DAAD research grant to pursue his Ph.","tags":null,"title":"Shadi Albarqouni","type":"authors"},{"authors":["cosmin-bercea"],"categories":null,"content":"Biography Cosmin Ionuț Bercea is a PhD Student at the Albarqouni Lab led by Shadi Albarqouni at Helmholtz AI and the Technical University of Munich (TUM). His aim is to develop innovative deep Federated Learning algorithms for anomaly detection in medical images in a robust and privacy-preserved fashion.\nHe was granted a study scholarship by the e.V. Villigst and received his B.Sc. and M.Sc. in Computer Science from the Friedrich-Alexander University of Erlangen-Nuremberg (FAU), Germany, in 2015, and 2018, respectively. During his master, Bercea did an internship abroad at the Computer Vision Centre (CVC) in Barcelona, Spain, where he developed machine learning algorithms for action recognition. He completed his master thesis at Siemens Healthineers, where he worked with Olivier Pauly, Florin C. Ghesu and Prof. Andreas K. Maier on developing novel machine learning algorithms using shared memory for medical imaging. Since then, Bercea has worked as a research engineer at Bosch Corporate Research where he developed perception and behaviour prediction machine learning algorithms for self-driving cars. His current research interests include Precision Medicine, Anomaly Detection, Image Understanding and Federated Learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9fe423251e4fabd7efe3cf629eec9af8","permalink":"https://albarqouni.github.io/authors/cosmin-bercea/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/cosmin-bercea/","section":"authors","summary":"Biography Cosmin Ionuț Bercea is a PhD Student at the Albarqouni Lab led by Shadi Albarqouni at Helmholtz AI and the Technical University of Munich (TUM). His aim is to develop innovative deep Federated Learning algorithms for anomaly detection in medical images in a robust and privacy-preserved fashion.","tags":null,"title":"Cosmin Bercea","type":"authors"},{"authors":["nicola-rieke"],"categories":null,"content":"Biography Nicola Rieke is a senior solution architect at NVIDIA for deep learning in healthcare and an active member of the medical imaging research community (e.g. Area Chair for MICCAI and IPCAI, organizer of various academic workshops). Throughout her studies and professional career, she has been working in the intersection of mathematics, medicine and computer science. In particular, she investigates real-time machine learning approaches for computer-assisted surgical interventions and federated learning for digital health. She holds a PhD from the Technical University of Munich, published various peer reviewed papers and was honored with the MICCAI Young Scientist Award.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1604687686,"objectID":"0beeece57f244832fea0cd348e7421be","permalink":"https://albarqouni.github.io/authors/nicola-rieke/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/nicola-rieke/","section":"authors","summary":"Biography Nicola Rieke is a senior solution architect at NVIDIA for deep learning in healthcare and an active member of the medical imaging research community (e.g. Area Chair for MICCAI and IPCAI, organizer of various academic workshops).","tags":null,"title":"Nicola Rieke","type":"authors"},{"authors":["zehra-hayirci"],"categories":null,"content":"Biography Zehra Hayirci is a PhD student at Helmholtz AI Center Munich. Her research interests include federated learning, machine learning for healthcare and computer vision. She received her MSc. degree in Informatics from TU Munich and BSc. degree in computer engineering from Middle East Technical University, Ankara. During her masters studies, she was a visiting researcher at National Institute of Informatics, Tokyo.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"73bf3b146cfc7df1121aa11f7b9a66de","permalink":"https://albarqouni.github.io/authors/zehra-hayirci/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zehra-hayirci/","section":"authors","summary":"Biography Zehra Hayirci is a PhD student at Helmholtz AI Center Munich. Her research interests include federated learning, machine learning for healthcare and computer vision. She received her MSc. degree in Informatics from TU Munich and BSc.","tags":null,"title":"Zehra Hayirci","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"04c510427b40893bf6b4d923881e627b","permalink":"https://albarqouni.github.io/resume/experience/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resume/experience/","section":"resume","summary":"","tags":null,"title":"Experience","type":"resume"},{"authors":null,"categories":null,"content":"A new consortium of the EU Innovative Medicines Initiative (IMI) will establish the biggest database of pathology images to accelerate the development of artificial intelligence in medicine. To take AI development in pathology to the next level, a European consortium combining leading European research centres, hospitals as well as major pharmaceutical industries, is going to develop a repository for the sharing of pathology data. The 6-year, €70 million project called BIGPICTURE, will herald a new era in pathology.\nBackground Pathology is the cornerstone of the workup of many diseases such as cancer, autoimmune diseases, of the follow up after transplantation and is also critical for the evaluation of the safety of drugs. It’s based on the examination of tissue samples (slides) under the microscope. However, despite its pivotal role, it still relies heavily on the qualitative interpretation by a qualified pathologist.\nWhile the microscope symbolizes the profession, the digitalisation of slides in recent years ignited a revolution: not only images can now be shared and accessed from distant locations, they can also be processed by computers. This opens the door for artificial intelligence (AI) applications to assist the pathologist and help study diseases, find better treatments and contribute to the 3Rs (replace, reduce, and refine animal use in research). However, the development of robust AI applications requires large amounts of data, which in the case of pathology means a huge collection of digital slides and the medical data necessary for their interpretation. Sharing these has so far remained challenging due to the data storage capacity required to host a sufficiently large collection and to concerns regarding the confidential character of the medical information.\nTo allow the fast development of AI in pathology, the BIGPICTURE project aims to create the first European, ethical and GDPR-compliant (General Data Protection Regulation), quality-controlled platform, in which both large-scale data and AI algorithms will coexist. The BIGPICTURE platform will be developed in a sustainable and inclusive way by connecting communities of pathologists, researchers, AI developers, patients, and industry parties.\nTU Munich In this project, Prof. Nassir Navab, and Dr. Shadi Albarqouni from TU Munich, together with Owkin will be leading and contributing to the development of Federated Deep Learning algorithms leveraging massive amounts of data, distributed in multiple sources, in a privacy-preserved fashion. This will enable deep learning models to be trained using sensitive data that cannot be made publicly available due to GPDR or sensitivity, e.g. rare diseases. Please visit the website of BIGPICTURE for further details.\nIntended results The project is divided into four main aspects that concern the large-scale collection of data. First, an infrastructure (hardware and software) must be created to store, share and process millions of images that can be gigabytes each. Second, legal and ethical constraints must be put in place to ensure adequate usage of data while fully respecting patient’s privacy and data confidentiality. Then, an initial set of 3 million digital slides from humans and laboratory animals will be collected and stored into the repository to provide data for the development of pathology AI tools. Finally, functionalities that aid the use of the database as well as the processing of images for diagnostic and research purposes will be developed.\nConsortium BIGPICTURE is a public-private partnership funded by IMI, with representation from academic institutions, small- and medium-sized enterprises (SMEs), public organisations and pharmaceutical companies, together with a large network slide contributing partners. The consortium partners involved in the project are:\nAcademic institutions: Radboud University Medical Center (NL), Linköping University (SE), Leeds Teaching Hospitals NHS Trust (UK), University Medical Centre Utrecht (NL), Uppsala University (SE, ELIXIR node), Haute Ecole Spécialisé de Suisse Occidentale (CH), Technical University Eindhoven (NL), University of Warwick (UK), [Technical University of Munich (DE), Medical University Graz (AT), Institut Pasteur (FR), University of Liege (BE), University of Semmelweis (HU), National Cancer Institute (NL), Region Östergötland (SE), Medical University Vienna (AT), University of Marburg (DE), Helsingin ja Uudenmaan sairaanhoitopiirin kuntayhtymä (FI),\nPharmaceutical companies: Novartis Pharma AG (CH), Janssen Pharmaceutica NV (BE), Bayer AG (DE), Boehringer Ingelheim International GmbH (DE), Novo Nordisk A/S (DK), Pfizer (US), Genentech – Roche (US), Sanofi Aventis recherche et Développement (FR), Institut de Recherches Internationales Servier (FR), and UCB Biopharma SRL (BE).\nOther public \u0026amp; private organisations: CSC – IT Center for Science Finland (FI, ELIXIR node), Biobanks and biomolecular resources research infrastructure (AT), Azienda Ospedaliera Per L’Emergenza Cannizzaro (IT), Fraunhofer-Gesellschaft zur Förderung der angewandten Forschung e.V.(DE), Deutsches Institut für Normung E.V. (DE), European Institute for Innovation through Health Data (BE), European Society of Pathology (BE), Digital pathology association (US), GBG Forschungs Gmbh (DE), ttopstart (NL), Sectra AB (SE), Cytomine SCRLFS (BE), Stichting Lygature (NL), Owkin (FR), Deciphex (IE), MedicalPhit (NL), Timelex (BE),\nBIGPICTURE starts on 1st February 2021 and will run for 6 years. However, the platform is meant to last, and the consortium will elaborate sustainability plans to maintain and continue to develop the platform beyond this term.\nAcknowledgment of support and disclaimer This project has received funding from the Innovative Medicines Initiative 2 Joint Undertaking under grant agreement No 945358. This Joint Undertaking receives support from the European Union’s Horizon 2020 research and innovation program and EFPIA. www.imi.europe.eu\nThis communication reflects the consortium’s view. Neither IMI nor the European Union or EFPIA are responsible for any use that may be made of the information contained therein.\n","date":1611964800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611964800,"objectID":"b409365e235005434f4cb7487d7610dd","permalink":"https://albarqouni.github.io/project/bigpicture/","publishdate":"2021-01-30T00:00:00Z","relpermalink":"/project/bigpicture/","section":"project","summary":"The 6-year, €70 million project called BIGPICTURE will herald a new era in pathology","tags":["Deep Learning","Federated Learning","Medical Imaging"],"title":"BigPicture Project","type":"project"},{"authors":["Christoph Baur","Stefan Denner","Benedikt Wiestler","Nassir Navab","Shadi Albarqouni"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586703928,"objectID":"9bf024e8514e25d9aadc45574e2c79fe","permalink":"https://albarqouni.github.io/publication/baur-2020-autoencoders/","publishdate":"2020-11-12T11:42:51.082771Z","relpermalink":"/publication/baur-2020-autoencoders/","section":"publication","summary":"Deep unsupervised representation learning has recently led to new approaches in the field of Unsupervised Anomaly Detection (UAD) in brain MRI. The main principle behind these works is to learn a model of normal anatomy by learning to compress and recover healthy data. This allows to spot abnormal structures from erroneous recoveries of compressed, potentially anomalous samples. The concept is of great interest to the medical image analysis community as it i) relieves from the need of vast amounts of manually segmented training data---a necessity for and pitfall of current supervised Deep Learning---and ii) theoretically allows to detect arbitrary, even rare pathologies which supervised approaches might fail to find. To date, the experimental design of most works hinders a valid comparison, because i) they are evaluated against different datasets and different pathologies, ii) use different image resolutions and iii) different model architectures with varying complexity. The intent of this work is to establish comparability among recent methods by utilizing a single architecture, a single resolution and the same dataset(s). Besides providing a ranking of the methods, we also try to answer questions like i) how many healthy training subjects are needed to model normality and ii) if the reviewed approaches are also sensitive to domain shift. Further, we identify open challenges and provide suggestions for future community efforts and research directions.","tags":["Medical Imaging","Deep Learning","Radiology","Generative Adversarial Networks","Anomaly Detection"],"title":"Autoencoders for Unsupervised Anomaly Segmentation in Brain MR Images: A Comparative Study","type":"publication"},{"authors":["Agnieszka Tomczak","Slobodan Ilic","Gaby Marquardt","Thomas Engel","Frank Forster","Nassir Navab","Shadi Albarqouni"],"categories":[],"content":"","date":1608595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608826908,"objectID":"519d1b22b6505a4c474a02a5ce20d86e","permalink":"https://albarqouni.github.io/publication/tomczak-2020-multi/","publishdate":"2020-12-24T16:21:48.629922Z","relpermalink":"/publication/tomczak-2020-multi/","section":"publication","summary":"oking stained images preserving the inter-cellular structures, crucial for the medical experts to perform classification. We achieve better structure preservation by adding auxiliary tasks of segmentation and direct reconstruction. Segmentation enforces that the network learns to generate correct nucleus and cytoplasm shape, while direct reconstruction enforces reliable translation between the matching images across domains. Besides, we build a robust domain agnostic latent space by injecting the target domain label directly to the generator, i.e., bypassing the encoder. It allows the encoder to extract features independently of the target domain and enables an automated domain invariant classification of the white blood cells. We validated our method on a large dataset composed of leukocytes of 24 patients, achieving state-of-the-art performance on both digital staining and classification tasks.","tags":["Histology","Domain Adaptation","Medical Imaging"],"title":"Multi-task multi-domain learning for digital staining and classification of leukocytes","type":"publication"},{"authors":["Roger D Soberanis-Mukul","Nassir Navab","Shadi Albarqouni"],"categories":[],"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608826906,"objectID":"b9c187dc22493f0c1b0c21a0341b35f0","permalink":"https://albarqouni.github.io/publication/soberanis-2020-uncertainty/","publishdate":"2020-12-24T16:21:46.548878Z","relpermalink":"/publication/soberanis-2020-uncertainty/","section":"publication","summary":"Organ segmentation in CT volumes is an important pre-processing step in many computer assisted intervention and diagnosis methods. In recent years, convolutional neural networks have dominated the state of the art in this task. However, since this problem presents a challenging environment due to high variability in the organ’s shape and similarity between tissues, the generation of false negative and false positive regions in the output segmentation is a common issue. Recent works have shown that the uncertainty analysis of the model can provide us with useful information about potential errors in the segmentation. In this context, we proposed a segmentation refinement method based on uncertainty analysis and graph convolutional networks. We employ the uncertainty levels of the convolutional network in a particular input volume to formulate a semi-supervised graph learning problem that is solved by training a graph convolutional network. To test our method we refine the initial output of a 2D U-Net. We validate our framework with the NIH pancreas dataset and the spleen dataset of the medical segmentation decathlon. We show that our method outperforms the state-of-the-art CRF refinement method by improving the dice score by 1% for the pancreas and 2% for spleen, with respect to the original U-Net’s prediction. Finally, we perform a sensitivity analysis on the parameters of our proposal and discuss the applicability to other CNN architectures, the results, and current limitations of the model for future work in this research direction. For reproducibility purposes, we make our code publicly available at https://github.com/rodsom22/gcn_refinement","tags":["Deep Learning","Medical Imaging","Graph Convolutional Networks","Uncertainty","Prior Knowledge"],"title":"An Uncertainty-Driven GCN Refinement Strategy for Organ Segmentation","type":"publication"},{"authors":["Shadi Albarqouni","Linda Mai Bui","Michael Schrapp","Slobodan Ilic"],"categories":[],"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608826908,"objectID":"85d8f635ea273c21c3eb5f72665d2e10","permalink":"https://albarqouni.github.io/publication/albarqouni-2020-ascertaining/","publishdate":"2020-12-24T16:21:48.339239Z","relpermalink":"/publication/albarqouni-2020-ascertaining/","section":"publication","summary":"","tags":[],"title":"Ascertaining the Pose of an X-Ray Unit Relative to an Object on the Basis of a Digital Model of the Object","type":"publication"},{"authors":["Sergey Zakharov","Shadi Albarqouni","Linda Mai Bui","Slobodan Ilic"],"categories":[],"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608826908,"objectID":"fb99f9ddc49922e11d2dcd7086771f48","permalink":"https://albarqouni.github.io/publication/zakharov-2020-determining/","publishdate":"2020-12-24T16:21:48.236445Z","relpermalink":"/publication/zakharov-2020-determining/","section":"publication","summary":"","tags":[],"title":"Determining a Pose of an Object in the Surroundings of the Object by Means of Multi-Task Learning","type":"publication"},{"authors":["Nicola Rieke","Jonny Hancox","Wenqi Li","Fausto Milletari","Holger Roth","Shadi Albarqouni","Spyridon Bakas","Mathieu N Galtier","Bennett Landman","Klaus Maier-Hein"," others"],"categories":null,"content":"","date":1600041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"af03c8910bbd9dce49bff53d0623dca5","permalink":"https://albarqouni.github.io/publication/rieke-2020-future/","publishdate":"2020-04-04T21:37:29.912346Z","relpermalink":"/publication/rieke-2020-future/","section":"publication","summary":"Data-driven Machine Learning has emerged as a promising approach for building accurate and robust statistical models from medical data, which is collected in huge volumes by modern healthcare systems. Existing medical data is not fully exploited by ML primarily because it sits in data silos and privacy concerns restrict access to this data. However, without access to sufficient data, ML will be prevented from reaching its full potential and, ultimately, from making the transition from research to clinical practice. This paper considers key factors contributing to this issue, explores how Federated Learning (FL) may provide a solution for the future of digital health and highlights the challenges and considerations that need to be addressed.","tags":["Federated Learning"],"title":"The Future of Digital Health with Federated Learning","type":"publication"},{"authors":["Mai Bui","Tolga Birdal","Haowen Deng","Shadi Albarqouni","Leonidas Guibas","Slobodan Ilic","Nassir Navab"],"categories":[],"content":"","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587921960,"objectID":"475ba0c3011bf271bdd06905c944049b","permalink":"https://albarqouni.github.io/publication/bui-20206-d/","publishdate":"2020-11-12T11:42:51.185054Z","relpermalink":"/publication/bui-20206-d/","section":"publication","summary":"We present a multimodal camera relocalization framework that captures ambiguities and uncertainties with continuous mixture models defined on the manifold of camera poses. In highly ambiguous environments, which can easily arise due to symmetries and repetitive structures in the scene, computing one plausible solution (what most state-of-the-art methods currently regress) may not be sufficient. Instead we predict multiple camera pose hypotheses as well as the respective uncertainty for each prediction. Towards this aim, we use Bingham distributions, to model the orientation of the camera pose, and a multivariate Gaussian to model the position, with an end-to-end deep neural network. By incorporating a Winner-Takes-All training scheme, we finally obtain a mixture model that is well suited for explaining ambiguities in the scene, yet does not suffer from mode collapse, a common problem with mixture density networks. We introduce a new dataset specifically designed to foster camera localization research in ambiguous environments and exhaustively evaluate our method on synthetic as well as real data on both ambiguous scenes and on non-ambiguous benchmark datasets.","tags":["Computer Vision","Pose Estimation","Uncertainty"],"title":"6D Camera Relocalization in Ambiguous Scenes via Continuous Multimodal Inference","type":"publication"},{"authors":["Mhd Hasan Sarhan","Nassir Navab","Abouzar Eslami","Shadi Albarqouni"],"categories":null,"content":"","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"f221a78d9d44379f049e24d06aa4214c","permalink":"https://albarqouni.github.io/publication/sarhan-2020-fairness/","publishdate":"2020-04-04T21:37:29.902484Z","relpermalink":"/publication/sarhan-2020-fairness/","section":"publication","summary":"Learning discriminative powerful representations is a crucial step for machine learning systems. Introducing invariance against arbitrary nuisance or sensitive attributes while performing well on specific tasks is an important problem in representation learning. This is mostly approached by purging the sensitive information from learned representations. In this paper, we propose a novel disentanglement approach to invariant representation problem. We disentangle the meaningful and sensitive representations by enforcing orthogonality constraints as a proxy for independence. We explicitly enforce the meaningful representation to be agnostic to sensitive information by entropy maximization. The proposed approach is evaluated on five publicly available datasets and compared with state of the art methods for learning fairness and invariance achieving the state of the art performance on three datasets and comparable performance on the rest. Further, we perform an ablative study to evaluate the effect of each component.","tags":["Deep Learning","Fairness","Powerful Representation","Computer Vision"],"title":"Fairness by Learning Orthogonal Disentangled Representations","type":"publication"},{"authors":["Shadi Albarqouni"],"categories":null,"content":"Disclaimer: I am neither a radiologist nor a clinician. I am a computer scientist who have been working on medical image computing for a while. I tried to summairze the key findings reported in almost 15 papers published in the Radiology Society in North America (RSNA) in the last two months.\n Intro about COVID-19 \nCT Imaging features Key CT findings have been studied and investigated by Guan et al. [10] in a large cohort of 1099 patients with confirmed COVID-19, and Chung et al. [1] in a group of 21 patients infected with COVID-19 in China. Their key results that majority of RT-PCR confirmed patients (some are asymptomatic) show typical CT findings such as the presence of bilateral ground-glass opacities (GGO) and/or consolidation, with a rounded morphology and a peripheral lung distribution (cf. Fig.1). In another cohort of 104 patients, from the cruise ship “Diamond Princess”, Inui et al. [4] have reported similar findings of lung opacities and airway abnormalities in both asymptomatic and symptomatic cases. In addition to the key characteristics of peripheral GGO, Caruso et al. [7] have also observed an association with sub-segmental vessel enlargement (\u0026gt; 3 mm) in his cohort of 158 participants from Italy (cf Fig.2).\nFig.1: Image adopted from Chung et al. [1]\n Of 21 patients with the 2019 novel coronavirus, 15 (71%) had involvement of more than two lobes at chest CT, 12 (57%) had ground-glass opacities, seven (33%) had opacities with a rounded morphology, seven (33%) had a peripheral distribution of disease, six (29%) had consolidation with ground-glass opacities, and four (19%) had crazy-paving pattern. [1]\n Fig.2: Image adopted from Caruso et al. [7]\nSurprisingly, 14% of the patients (3 out of 21), studied by Chang et al. [1], show negative CT findings in their initial chest CT scan. Follow-up scans, however, show rounded peripheral ground-glass opacity (cf. Fig.3). Xie et al. [3] and Caruso et al. [7] have also reported similar percentages of 4% (7 out of 167), and 3% (2 out of 62), respectively, of their cohorts who show no findings in their CT scans.\nFig.3: Image adopted from Chung et al. [1]\nContrary, Xie et al. [3] found out that 3% of their cohort (5 out of 167), who had initially negative RT-PCR, show positive Chest CT with similar findings of viral pneumonia reported by Chung et al. [1] (cf Fig.4). A few days later, and after repeated swap tests, the RT-PCR had become positive. This has been. also confirmed in another cohort, reported by Fang et al. [8], where the percentage was around 29% (15 out of 51).\nFig.4: Adopted from Xie et al. [3]\nChest CT vs. RT-PCR Given the aforementioned key characteristics of COVID-19, the low sensitivity of the RT-PCR test (42–71%) [6], and the long mean interval time between the initial negative to positive RT-PCR (5.1 +/- 1.5 days), clinicians and researchers have investigated whether diagnostic imaging features could be used an alternative to RT-PCR in screening.\n In patients at high risk for 2019-nCoV infection, chest CT evidence of viral pneumonia may precede positive negative RT-PCR test results. [3]\n A few recent studies (Ai et al. [6], Caruso et al. [7], Fang et al. [8]) have investigated the correlation of Chest CT findings and RT-PCR test reporting high sensitivity of 97–98% for Chest CT in diagnosing COVID-19. Detailed evaluation metrics against the RT-PCR are reported below. Interestingly, 98% of the patients (56 out of 57), reported by Ai et al. [6], who had initially positive CT findings show positive RT-PCR within 6 days (cf Fig.5). Such interesting results suggest chest CT could be considered for screening.\n In a series of 51 patients with chest CT and RT-PCR assay performed within 3 days, the sensitivity of CT for COVID-19 infection was 98% compared to RT-PCR sensitivity of 71% (p\u0026lt;.001) [2]\n    Papers Sample Size Country Sensitivity Specificity Accuracy     Caruso et al. [7] 58 Rome, Italy 97% 56% 72%   Ai et al. [6] 1014 Wuhan, China 97% 25% 68%   Fang et al. [8] 51 Shanghai, China 98% N/A N/A    Fig.5: Adopted from Ai et al. [6]\nDiscrepancy between CT findings and RT-PCR motivated clinicians and researchers to analyze the serial CT findings over time (Wang and Dong et al. [9], Pan et al. [12]) and study the relationship to duration of infection (Bernheim et al. [11]). As reported in their analysis (Fig. 6, 7,8), the appearance of GGO and Consolidations varies over time explaining the discrepancy in the sensitivity. Both studies suggest, however, pathology quantification might help in the prognosis.\n The extent of CT abnormalities progressed rapidly after the onset of symptoms, peaked around 6–11 days, and followed by persistence of high levels in lung abnormalities. The temporal changes of the diverse CT manifestations followed a specific pattern, which might indicate the progression and recovery of the illness. [7]\n Fig. 6: Adopted from Wang and Dong et al. [9]\n Recognizing imaging patterns based on infection time course is paramount for not only understanding the pathophysiology and natural history of infection, but also for helping to predict patient progression and potential complication development. [11]\n Fig. 7: Adopted from Bernheim et al. [11]\nFig.8: Adopted from Pan et al. [12]\nChest X-ray vs. RT-PCR Given the limited resources, and to minimize the risk of cross-infection [14], and contamination, clinicians and researchers have investigated whether a readily available diagnostic imaging, namely X-ray, could be used as a first-line triage tool, and help in detecting abnormalities associated with COVID-19 in Chest X-rays, in particular, for asymptomatic patients.\nOne of the interesting studies reported by Wong et al. [13] who have studied the appearance of COVID-19 in Chest X-ray, and its correlation with the key findings in the CT scans. Besides, they have investigated the correlation of Chest X-ray and RT-PCR test.\nIn their cohort of 64 patients from Hong Kong, they observed similar key characteristics, appeared in CT scans, such as bilateral, peripheral ground-glass opacity, and/or consolidations (cf. Fig. 9).\nFig. 9: Adapted from Wong et al. [13]\nIn contrast to the high sensitivity reported for the CT scans, Wong et al. [13] reported a sensitivity of 69% for Chest X-ray, compared to 91% for the initial RT-PCR. The Chest X-ray abnormalities preceded the positive RT-PCR only in 9% (6 out of 64 patients). Examples on the latter scenario are demonstrated in Fig. 10 (A, and B).\nFig. 10: Adapted from Wong et al. [13]\nThe remarkable low sensitivity indicates a high number of False Negative suggesting further investigation of the abnormalities change over time. Fig.11 shows the changes of severity score in Chest X-ray, where the peak score was reported in 10–12 days since symptoms onset.\nFig.11: Adopted from Wong et al. [13]\nSurprisingly, 86% of the patients (24 out of 28) who had initial positive Chest X-ray, show positive findings on the CT as well. Whereas only one patient of the rest shows no findings in the Chest X-ray, the CT shows peripheral GOO (cf. Fig.12). These results suggest Chest X-ray might be helpful in monitoring and prognosis, but not recommended for screening.\nFig. 12: Adapted from Wong et al. [13]\n At this time, CT screening for the detection of COVID-19 is not recommended by most radiological societies. However, we anticipate that the use of CT in clinical management as well as incidental findings potentially attributable to COVID-19 will evolve. [15]\n Community Acquired Pneumonia (CAP) vs. COVID-19 So far, previous studies report high sensitivity in diagnosing COVID-19 from CT scans, however, with remarkable low specificity, e.g. 25%, and 56% in Ai et al. [6], and Caruso et al. [7], respectively. In other words, radiologists might misinterpret the CT scan and diagnose the patient with COVID-19.\n These studies have shown that COVID-19 often produces a CT pattern resembling organizing pneumonia, notably peripheral ground-glass opacities (GGO) and nodular or mass-like GGO that are often bilateral and multilobar ( 11). However, additional imaging findings have also been reported including linear, curvilinear or perilobular opacities, consolidation, and diffuse GGO, which can mimic several disease processes including other infections, inhalational exposures, and drug toxicities ( 12 –15). [15]\n To assess the performance of radiologists in differentiating COVID-19 from other viral infections, Bai and Hsieh et al. [16] collected a cohort of 424 chest CT scans; 52% with positive COVID-19 by RT-PCR test, and 48% with positive Respiratory Pathogen Panel for viral pneumonia. The cohort was blindly reviewed by three radiologists from China, and a subset of 58 patients were reviewed by four radiologists from the US. Overall, their results demonstrate that radiologists can distinguish COVID-19 from other viral pneumonia with moderate to high sensitivity 67–93%, and high specificity 93–100%. Misinterpreted cases show either subtle or atypical findings in their CT scans (cf. Fig. 13). Key differences have been also reported by the radiologists.\n Compared to non-COVID-19 pneumonia, COVID-19 pneumonia was more likely to have a peripheral distribution (80% vs. 57%, p\u0026lt;0.001), ground-glass opacity (91% vs. 68%, p\u0026lt;0.001), fine reticular opacity (56% vs. 22%, p\u0026lt;0.001), and vascular thickening (59% vs. 22%, p\u0026lt;0.001), but less likely to have a central+peripheral distribution (14.% vs. 35%, p\u0026lt;0.001), pleural effusion (4.1 vs. 39%, p\u0026lt;0.001) and lymphadenopathy (2.7% vs. 10.2%, p\u0026lt;0.001).\n Fig.13: Adapted from Bai and Hsieh et al. [16]\nTo reduce the reporting variability and uncertainty which might arise due to incidental findings with other viral infections, e.g. influenza-A, Simpson and Kay et al. [15] put together a nice piece of work and suggestions on standardized CT reporting language of COVID-19, which could be considered as a good reference for structured reporting. Examples of suggested reporting languages along with a few chest CT images are demonstrated in Fig. 14–16.\nFig.14: Adopted from Simpson and Kay et al. [15]\nFig. 15: Adapted from Simpson and Kay et al. [15]\nFig. 16: Adapted from Simpson and Kay et al. [15]\n Future direction includes development of an artificial intelligence classifier that can further augment radiologist performance in combination with clinical information. [16]\n From my point of view, AI has the potential to:  identify the asymptomatic carriers of COVID-19 detect and quantify the abnormalities in serial Chest CT/X-rays scans for prognosis purpose distinguish CAP from COVID-19 using Chest CT scans, and additional clinical information; age, gender, previous disorders, …etc.  References: [1] Chung, M., Bernheim, A., Mei, X., Zhang, N., Huang, M., Zeng, X., Cui, J., Xu, W., Yang, Y., Fayad, Z.A. and Jacobi, A., 2020. CT imaging features of 2019 novel coronavirus (2019-nCoV). Radiology, 295(1), pp.202–207. ( PDF)\n[2] Fang, Y., Zhang, H., Xie, J., Lin, M., Ying, L., Pang, P. and Ji, W., 2020. Sensitivity of chest CT for COVID-19: comparison to RT-PCR. Radiology, p.200432. ( PDF)\n[3] Xie, X., Zhong, Z., Zhao, W., Zheng, C., Wang, F. and Liu, J., 2020. Chest CT for typical 2019-nCoV pneumonia: relationship to negative RT-PCR testing. Radiology, p.200343. ( PDF)\n[4] Inui S, Fujikawa A, Jitsu M, Kunishima N, Watanabe S, Suzuki Y, Umeda S, Uwabe Y. Chest CT findings in cases from the cruise ship “Diamond Princess” with coronavirus disease 2019 (COVID-19). Radiology: Cardiothoracic Imaging. 2020 Mar 17;2(2):e200110. ( PDF)\n[5] Simpson, S., Kay, F.U., Abbara, S., Bhalla, S., Chung, J.H., Chung, M., Henry, T.S., Kanne, J.P., Kligerman, S., Ko, J.P. and Litt, H., 2020. Radiological Society of North America Expert Consensus Statement on Reporting Chest CT Findings Related to COVID-19. Endorsed by the Society of Thoracic Radiology, the American College of Radiology, and RSNA. Radiology: Cardiothoracic Imaging, 2(2), p.e200152. ( PDF)\n[6] Ai, T., Yang, Z., Hou, H., Zhan, C., Chen, C., Lv, W., Tao, Q., Sun, Z. and Xia, L., 2020. Correlation of chest CT and RT-PCR testing in coronavirus disease 2019 (COVID-19) in China: a report of 1014 cases. Radiology, p.200642. ( PDF)\n[7] Caruso, D., Zerunian, M., Polici, M., Pucciarelli, F., Polidori, T., Rucci, C., Guido, G., Bracci, B., de Dominicis, C. and Laghi, A., 2020. Chest CT features of COVID-19 in Rome, Italy. Radiology, p.201237. ( PDF)\n[8] Fang, Y., Zhang, H., Xie, J., Lin, M., Ying, L., Pang, P. and Ji, W., 2020. Sensitivity of chest CT for COVID-19: comparison to RT-PCR. Radiology, p.200432. ( PDF)\n[9] Wang, Y., Dong, C., Hu, Y., Li, C., Ren, Q., Zhang, X., Shi, H. and Zhou, M., 2020. Temporal changes of CT findings in 90 patients with COVID-19 pneumonia: a longitudinal study. Radiology, p.200843. ( PDF)\n[10] Guan, W.J., Ni, Z.Y., Hu, Y., Liang, W.H., Ou, C.Q., He, J.X., Liu, L., Shan, H., Lei, C.L., Hui, D.S. and Du, B., 2020. Clinical characteristics of coronavirus disease 2019 in China. New England Journal of Medicine. ( PDF)\n[11] Bernheim, A., Mei, X., Huang, M., Yang, Y., Fayad, Z.A., Zhang, N., Diao, K., Lin, B., Zhu, X., Li, K. and Li, S., 2020. Chest CT findings in coronavirus disease-19 (COVID-19): relationship to duration of infection. Radiology, p.200463. ( PDF)\n[12] Pan, F., Ye, T., Sun, P., Gui, S., Liang, B., Li, L., Zheng, D., Wang, J., Hesketh, R.L., Yang, L. and Zheng, C., 2020. Time course of lung changes on chest CT during recovery from 2019 novel coronavirus (COVID-19) pneumonia. Radiology, p.200370. ( PDF)\n[13] Wong, H.Y.F., Lam, H.Y.S., Fong, A.H.T., Leung, S.T., Chin, T.W.Y., Lo, C.S.Y., Lui, M.M.S., Lee, J.C.Y., Chiu, K.W.H., Chung, T. and Lee, E.Y.P., 2020. Frequency and distribution of chest radiographic findings in COVID-19 positive patients. Radiology, p.201160. ( PDF)\n[14] American College of Radiology, 2020. ACR recommendations for the use of chest radiography and computed tomography (CT) for suspected COVID-19 infection. ACR website.\n[15] Simpson, S., Kay, F.U., Abbara, S., Bhalla, S., Chung, J.H., Chung, M., Henry, T.S., Kanne, J.P., Kligerman, S., Ko, J.P. and Litt, H., 2020. Radiological Society of North America Expert Consensus Statement on Reporting Chest CT Findings Related to COVID-19. Endorsed by the Society of Thoracic Radiology, the American College of Radiology, and RSNA. Radiology: Cardiothoracic Imaging, 2(2), p.e200152. ( PDF)\n[16] Bai, H.X., Hsieh, B., Xiong, Z., Halsey, K., Choi, J.W., Tran, T.M.L., Pan, I., Shi, L.B., Wang, D.C., Mei, J. and Jiang, X.L., 2020. Performance of radiologists in differentiating COVID-19 from viral pneumonia on chest CT. Radiology, p.200823. ( PDF)\n","date":1587340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587921960,"objectID":"ddbeafe5154a0fa104a1b1a13edca921","permalink":"https://albarqouni.github.io/talk/covid19/","publishdate":"2020-04-20T00:00:00Z","relpermalink":"/talk/covid19/","section":"talk","summary":"Disclaimer: I am neither a radiologist nor a clinician. I am a computer scientist who have been working on medical image computing for a while. I tried to summairze the key findings reported in almost 15 papers published in the Radiology Society in North America (RSNA) in the last two months.","tags":null,"title":"Medium Blog: Journey through COVID-19 RSNA Papers","type":"talk"},{"authors":null,"categories":null,"content":"Deep Learning (DL) has emerged as a leading technology for accomplishing many challenging tasks showing outstanding performance in a broad range of computer vision and medical applications. However, this success comes at the cost of collecting and processing a massive amount of data, which often are not accessible, in Healthcare, due to privacy issues. Federated Learning (FL) has been recently introduced to allow training DL models without sharing the data. Instead, DL models at local hubs, i.e. hospitals, share only the trained parameters with a centralized DL model, which is, in return, responsible for updating the local DL models as well.\nOur golas in this project is to develop novel models and algorithms for a ground-breaking new generation of deep FL, which can distill the knowledge from local hubs, i.e. hospitals, and edges, i.e. wearable devices, to provide personalized healthcare services.\nThe principal challenges, to overcome, concern the nature of medical data, namely data heterogeneity; severe class-imbalance, few amounts of annotated data, inter-/intra-scanners variability (domain shift), inter-/intra-observer variability (noisy annotations), system heterogeneity, and privacy issues (see the example below).\nCollaboration:    Funding:  Soon  ","date":1586131200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"97608cdd2a37e1780fe46591f07dbfdf","permalink":"https://albarqouni.github.io/project/federated-learning/","publishdate":"2020-04-06T00:00:00Z","relpermalink":"/project/federated-learning/","section":"project","summary":"One of our recent and promising projects.","tags":["Deep Learning","Federated Learning","Medical Imaging"],"title":"Deep Federated Learning in Healthcare","type":"project"},{"authors":null,"categories":null,"content":"Today\u0026rsquo;s clinical procedures often generate a large amount of digital images requiring close inspection. Manual examination by physicians is time-consuming and machine learning in computer vision and pattern recognition is playing an increasing role in medical applications. In contrast to pure machine learning methods, crowdsourcing can be used for processing big data sets, utilising the collective brainpower of huge crowds. Since individuals in the crowd are usually no medical experts, preparation of medical data as well as an appropriate visualization to the user becomes indispensable. The concept of gamification typically allows for embedding non-game elements in a serious game environment, providing an incentive for persistent engagement to the crowd. Medical image analysis empowered by the masses is still rare and only a few applications successfully use the crowd for solving medical problems. The goal of this project is to bring the gamification and crowdsourcing to the Medical Imaging community.\nCollaboration: Funding: ","date":1586131200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"b5491160f8d0d20a3309b4aeb126b09d","permalink":"https://albarqouni.github.io/project/learn-from-crowds/","publishdate":"2020-04-06T00:00:00Z","relpermalink":"/project/learn-from-crowds/","section":"project","summary":"Crowdsourcing, Gamification","tags":["Deep Learning","Medical Imaging"],"title":"Learn from Crowds","type":"project"},{"authors":null,"categories":null,"content":"Together with our clinical and industry partners, we realized that there is a need to incorporate domain-specific knowledge and let the model Learn from a Prior Knowledge. We first investigated modeling general priors, i.e., manifold assumptions, to learn powerful representations. Such representations achieved state-of-the-art on benchmark datasets, such as e IDRiD for Diabetic Retinopathy Early Detection (Sarhan et al. 2019), and 7 Scenes for Camera Relocalization (Bui et al. 2017). Then, we started looking into the laplacian graph, where prior knowledge can be modeled as a soft constraint, i.e., regularization, to learn feature representation that follows such manifold defined by graphs. We have shown in our ISBI (Kazi et al. 2019a), MICCAI (Kazi et al. 2019b), and IPMI (Kazi et al. 2019) papers that leveraging prior knowledge such as proximity of ages, gender, and a few lab results, are of high importance in Alzheimer classification.\nCollaboration:    Funding:  Siemens AG  ","date":1586131200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"1ebf6ce4df5833004681d9a40bc6ab1b","permalink":"https://albarqouni.github.io/project/learn-from-graph/","publishdate":"2020-04-06T00:00:00Z","relpermalink":"/project/learn-from-graph/","section":"project","summary":"Manifold Learning, Graph Convolutional Networks","tags":["Deep Learning","Medical Imaging"],"title":"Learn from Prior Knowledge","type":"project"},{"authors":null,"categories":null,"content":"To build domain-agnostic models that are generalizable to a different domain, i.e., scanners, we have investigated three directions; First, Style Transfer, where the style/color of the source domain is transferred to match the target one. Such style transfer is performed in the high-dimensional image space using adversarial learning, as shown in our papers on Histology Imaging (Lahiani et al. 2019a, Lahiani et al. 2019b, Shaban et al. 2019). Second, Domain Adaptation, where the distance between the features of the source and target domains are minimized. Such distance can be optimized in a supervised fashion, i.e., class aware, using angular cosine distance as shown in our paper on MS Lesion Segmentation in MR Imaging (Baur et al. 2017), or in an unsupervised way, i.e., class agnostic, using adversarial learning as explained in our article on Left atrium Segmentation in Ultrasound Imaging (Degel et al. 2018). Yet, another exciting direction that has been recently investigated in our paper (Lahiani et al. 2019c) is to disentangle the feature that is responsible for the style and color from the one responsible for the semantics.\nCollaboration:  Eldad Klaiman, Roche Diagnostics GmbH Georg Schummers and Matthias Friedrichs, TOMTEC Imaging Systems GmbH   ","date":1586131200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"ae4866b40d21e360ab5ec913e479210d","permalink":"https://albarqouni.github.io/project/learn-to-adapt/","publishdate":"2020-04-06T00:00:00Z","relpermalink":"/project/learn-to-adapt/","section":"project","summary":"Domain Adaptation, Style Transfer","tags":["Deep Learning","Domain Adaptation","Medical Imaging","Stain Normalization","Generative Adversarial Networks","Powerful Representation","Style Transfer"],"title":"Learn to Adapt","type":"project"},{"authors":null,"categories":null,"content":"To build models that are transferable to different tasks or different data distributions, i.e., non i.i.d., we have investigated meta-learning approaches such as prototypical networks (PN) (Snell et al. 2017). PN learns a class prototype from very few amounts of labeled data, e.g., 1-5 shots, and use the learned prototypes to perform the classification tasks. In the context of medical imaging, we were first to introduce Few-Shot Learning into the MIC community. We have shown in our recent ICML Workshop paper (Ayyad et al. 2019) that our novel Semi-Supervised Few-Shot Learning achieves the state-of-the-art on benchmark datasets; Omniglot, miniImageNet, and TieredImageNet. Further, we have demonstrated in our recent paper (Parida et al. 2019) that such concepts can be utilized in medical imaging segmentation with an extremely low budget of annotated data, e.g., bounding boxes, and better generalization capability, i.e., to new organs or anomalies, however, at the cost of less accurate segmentation. Yet, our proposed models have great potential in clinical practice where a novel application could come in, and only a very few annotations are required, to perform segmentation tasks. Further, such a learning paradigm has a great potential in Federated Learning, where the data acquired at different hospitals capture heterogeneous and non i.i.d data, i.e., various tasks, making proposed models suitable for such a problem.\nCollaboration:  Prof. Mohamed Elhoseiny, Facebook AI Research  ","date":1586131200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"12b37d7857150b87855b9c4c1a5d2f4c","permalink":"https://albarqouni.github.io/project/learn-to-learn/","publishdate":"2020-04-06T00:00:00Z","relpermalink":"/project/learn-to-learn/","section":"project","summary":"Meta-Learning, Few-Shot Learning","tags":["Deep Learning","Medical Imaging","Powerful Representation","Meta Learning","Semi-Supervised Learning"],"title":"Learn to Learn","type":"project"},{"authors":null,"categories":null,"content":"To build explainable AI models that are interpretable for our end-users, i.e., clinicians, we have investigated two research directions. First, we have utilized some visualization techniques to explain and interpret \u0026ldquo;black box\u0026rdquo; models by propagating back the gradient of the class of interest to the image space where you can see the relevant semantics, so-called Gradient Class Activation Maps (GradCAM). Sooner, we found out such techniques do not produce meaningful results. In other words, irrelevant semantics could be highly activated in GradCAM, yielding unreliable explanation tools. To overcome such a problem, we have introduced a robust optimization loss in our MICCAI paper (Khakzar et al. 2019), which generated adversarial examples enforcing the network to only focus on relevant features and probably correlated with other examples belonging to the same class.\nSecond, we have investigated designing and building explainable models by i) uncertainty quantification and ii) disentangled feature representation. In the first category, we started understanding the uncertainty estimates generated by Monte-Carlo Dropout, the approximate of Bayesian Neural Networks, and other techniques, e.g. PointNet, in Camera Relocalization problem (Bui et al. 2018), to shed light on the ambiguity present in the dataset. We took a step further, and use such uncertainty estimates to refine the segmentation in an unsupervised fashion (Soberanis-Mukul et al. 2019, Bui et al. 2019).\nRecently, we have investigated modeling the labels uncertainty, which is related to the inter-/intra-observer variability, and produced a metric to quantify such uncertainty. We have shown in our paper (Tomczack et al. 2019) that such uncertainty can be rather disentangled from the model and data uncertainties, so-called, epistemic, and aleatoric uncertainties, respectively. We believe such uncertainty is of high importance to the referral systems. In the second category, we have studied the variational methods, and disentangled representations, where the assumption here that some generative factors, e.g., color, shape, and pathology, will be captured in the lower-dimensional latent space, and one can easily go through the manifold and generate tons of example by sampling from the posterior distribution. We were among the firsts who introduce such concepts in medical imaging by investigating the influence of residual blocks and adversarial learning on disentangled representation (Sarhan et al. 2019). Our hypothesis that better reconstruction fidelity would force the network to model high resolution, which might have a positive influence on the disentangled representation, in particular, some pathologies.\nCollaboration:  Dr. Abouzar Eslami, Carl Zeiss Meditec AG PD. Dr. Slobodan Ilic, Siemens AG  Funding:  Siemens AG  ","date":1586131200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"968b9cefba0f1548b9cc84e4b66b960f","permalink":"https://albarqouni.github.io/project/learn-to-reason-and-explain/","publishdate":"2020-04-06T00:00:00Z","relpermalink":"/project/learn-to-reason-and-explain/","section":"project","summary":"Interpretable ML, Disentangled Representation, Fairness","tags":["Deep Learning","Medical Imaging","Powerful Representation","Interpretability","Fairness"],"title":"Learn to Reason and Explain","type":"project"},{"authors":null,"categories":null,"content":"We started investigating Convolutional Neural Networks for Object Recognition in a supervised fashion, for example, mitotic figure detection in histology imaging (Albarqouni et al. 2016), Catheter electrodes detection and depth estimation in Interventional Imaging (Baur et al. 2016), femur fracture detection in radiology (Kazi et al. 2017), in-depth layer X-ray synthesis (Albarqouni et al. 2017), and pose estimation of mobile X-rays (Bui et al. 2017). One of the first work which has been highly recognized and featured in the media is AggNet (Albarqouni et al. 2016) for Mitotic figure detection in Histology Images. Although the network architecture was shallow, it was trained using millions of multi-scale RGB patches of histology images, achieving outstanding performance (ranked 3rd among 15 participants in AMIDA13 challenge).\nDuring our work, we found out such data-driven models demand a massive amount of annotated data, which might not be available in medical imaging and can not be mitigated by simple data augmentation. Besides, we found out such models are so sensitive to domain shift, i.e., different scanner, and methods such as domain adaptation is required. Therefore, we have focused our research directions to develop fully-automated, high accurate solutions that save export labor and efforts, and mitigate the challenges in medical imaging. For example, i) the availability of a few annotated data, ii) low inter-/intra-observers agreement, iii) high-class imbalance, iv) inter-/intra-scanners variability and v) domain shift.\nTo mitigate the problem of limited annotated data, we developed models that Learn from a Few Examples by i) leveraging the massive amount of unlabeled data via semi-supervised techniques (Baur and Albarqouni et al. 2017), ii) utilizing weakly labeled data, which is way cheaper than densely one (Kazi et al. 2017), iii) generating more examples through modeling the data distribution (Baur et al. 2018), and finally by iv) investigating unsupervised approaches (Baur et al. 2018, Baur et al. 2019).\nCollaboration:  Prof. Peter Nöel, Department of Radiology, University of Pennsylvania, USA Prof. Guillaume Landry, Department of Radiation Oncology, Medical Center of the University of Munich, Germany Dr. Benedikt Wiestler, TUM Neuroradiologie, Klinikum rechts der Isar, Germany Prof. Dr. med. Sonja Kirchhoff, Klinikum rechts der Isar, Germany Prof. Diana Mateus, Ecole Centrale Nantes, France Prof. Andreas Maier, Friedrich-Alexander-Universität Erlangen-Nürnberg, Germany Prof. Pascal Fallavollita, Ottawa University, Canada  Funding:  Siemens Healthineers Siemens AG  ","date":1586131200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"7df768280b9290f6b789747d6ff94ae4","permalink":"https://albarqouni.github.io/project/learn-to-recognize/","publishdate":"2020-04-06T00:00:00Z","relpermalink":"/project/learn-to-recognize/","section":"project","summary":"Detection, Classification, Segmentation, Anomaly Detection, Semi-/Weakly-Supervised Learning","tags":["Deep Learning","Histology","Hematology","Radiology","Anomaly Detection","Weakly-Supervised Learning","Semi-Supervised Learning","Medical Imaging"],"title":"Learn to Recognize","type":"project"},{"authors":null,"categories":null,"content":"Deep Learning has emerged as a leading technology for accomplishing many challenging tasks showing outstanding performance in a broad range of applications in computer vision and medical applications. Despite its success and merit in recent state-of-the-art methods, DL tools still lack in robustness hindering its adoption in medical applications. Modeling uncertainty, through Bayesian Inference and Monte-Carlo dropout, has been successfully introduced to computer vision for better understanding the underlying deep learning models. In this proposal, we investigate modeling the uncertainty for medical applications given the well-known challenges in medical image analysis, namely severe class-imbalance, few amounts of labeled data, domain shift, and noisy annotations.\nCollaboration: Prof. Ender Konukoglu, Department of Information Technology and Electrical Engineerng, ETH Zurich.\nProf. Daniel Rueckert, Department of Computing, Imperial College London\nProf. Nassir Navab, Faculty of Informatics, Technical University of Munich\nFunding: This project is supported by the PRIME programme of the German Academic Exchange Service (DAAD) with funds from the German Federal Ministry of Education and Research (BMBF).\n","date":1586131200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"5a1b2510545c5354e234c00c606521da","permalink":"https://albarqouni.github.io/project/uncertainty/","publishdate":"2020-04-06T00:00:00Z","relpermalink":"/project/uncertainty/","section":"project","summary":"DAAD PRIME Fellowship at ETH Zürich and Imperial College London","tags":["Deep Learning","Uncertainty","Medical Imaging"],"title":"Modelling Uncertainty in Deep Learning for Medical Applications","type":"project"},{"authors":null,"categories":null,"content":"  Telemedicine in Palestine  from Shadi Nabil Albarqouni  Collaboration: Funding: ","date":1586131200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"19cd5f5c4a3a753af6529105894bf7ce","permalink":"https://albarqouni.github.io/project/telemedicine/","publishdate":"2020-04-06T00:00:00Z","relpermalink":"/project/telemedicine/","section":"project","summary":"  Telemedicine in Palestine  from Shadi Nabil Albarqouni  Collaboration: Funding: ","tags":["Medical Imaging","Telemedicine"],"title":"Telemedicine in Palestine","type":"project"},{"authors":null,"categories":null,"content":"Camera pose estimation is the term for determining the 6-DoF rotation and translation parameters of a camera. It is now a key technology in enabling multitudes of applications such as augmented reality, autonomous driving, human computer interaction and robot guidance. For decades, vision scholars have worked on finding the unique solution of this problem. Yet, this trend is witnessing a fundamental change. The recent school of thought has begun to admit that for our highly complex and ambiguous real environments, obtaining a single solution is not sufficient. This has led to a paradigm shift towards estimating rather a range of solutions in the form of full probability or at least explaining the uncertainty of camera pose estimates. Thanks to the advances in Artificial Intelligence, this important problem can now be tackled via machine learning algorithms that can discover rich and powerful representations for the data at hand. In collaboration, TU Munich and Stanford University plan to devise and implement generative methods that can explain uncertainty and ambiguity in pose predictions. In particular, our aim is to bridge the gap between 6DoF pose estimation either from 2D images/3D point sets and uncertainty quantification through multimodal variational deep methods.\nCollaboration:  Dr. Tolga Birdal, Prof. Leonidas Guibas, Stanford University\n Mai Bui, Dr. Shadi Albarqouni, Prof. Nassir Navab, Technical University of Munich\nFunding: This project is funded by the Bavaria California Technology Center ( BaCaTeC)\n ","date":1586131200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"d44aed938a1c20ed2b3862cd53197f69","permalink":"https://albarqouni.github.io/project/bacatec/","publishdate":"2020-04-06T00:00:00Z","relpermalink":"/project/bacatec/","section":"project","summary":"Collaboration project with Stanford University.","tags":["Deep Learning","Computer Vision","Uncertainty"],"title":"Uncertainty Aware Methods for Camera Pose Estimation and Relocalization","type":"project"},{"authors":[],"categories":null,"content":"","date":1585746000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"d532a048a4bf20fd1422372c3b5a3ab8","permalink":"https://albarqouni.github.io/talk/dart2020/","publishdate":"2020-04-01T13:00:00Z","relpermalink":"/talk/dart2020/","section":"talk","summary":"","tags":["Deep Learning","Domain Adaptation"],"title":"Organizing Committee Member at MICCAI DART 2020","type":"talk"},{"authors":[],"categories":null,"content":"","date":1585746000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"bda331adadf80e82740cd67b6f2e1b4c","permalink":"https://albarqouni.github.io/talk/dcl2020/","publishdate":"2020-04-01T13:00:00Z","relpermalink":"/talk/dcl2020/","section":"talk","summary":"","tags":["Deep Learning","Domain Adaptation"],"title":"Organizing Committee Member at MICCAI DCL 2020","type":"talk"},{"authors":["Roger D Soberanis-Mukul","Nassir Navab","Shadi Albarqouni"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"a5ceab1981dca0d1550c23165140ca96","permalink":"https://albarqouni.github.io/publication/soberanis-2019-uncertainty/","publishdate":"2020-04-04T21:37:29.560639Z","relpermalink":"/publication/soberanis-2019-uncertainty/","section":"publication","summary":"Organ segmentation is an important pre-processing step in many computer assisted intervention and diagnosis methods. In recent years, CNNs have dominated the state of the art in this task. Organ segmentation scenarios present a challenging environment for these methods due to high variability in shape and similarity with background. This leads to the generation of false negative and false positive regions in the output segmentation. In this context, the uncertainty analysis of the model can provide us with useful information about potentially misclassified elements. In this work we propose a method based on uncertainty analysis and graph convolutional networks as a post-processing step for segmentation. For this, we employ the uncertainty levels of the CNN to formulate a semi-supervised graph learning problem that is solved by training a GCN on the low uncertainty elements. Finally, we evaluate the full graph on the trained GCN to get the refined segmentation. We test our framework in refining the output of pancreas and spleen segmentation models. We show that the framework can increase the average dice score in 1% and 2% respectively for these problems. Finally, we discuss the results and current limitations of the model that lead to future work in this research direction","tags":["Deep Learning","Medical Imaging","Graph Convolutional Networks","Uncertainty","Prior Knowledge"],"title":"Uncertainty-based graph convolutional networks for organ segmentation refinement","type":"publication"},{"authors":["Amal Lahiani","Irina Klaman","Nassir Navab","Shadi Albarqouni","Eldad Klaiman"],"categories":null,"content":"","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"67e0ffa1725d19e7697882b8eb207953","permalink":"https://albarqouni.github.io/publication/lahiani-2020-seamless/","publishdate":"2020-04-04T21:37:29.846347Z","relpermalink":"/publication/lahiani-2020-seamless/","section":"publication","summary":"Stain virtualization is an application with growing interest in digital pathology allowing simulation of stained tissue images thus saving lab and tissue resources. Thanks to the success of Generative Adversarial Networks (GANs) and the progress of unsupervised learning, unsupervised style transfer GANs have been successfully used to generate realistic, clinically meaningful and interpretable images. The large size of high resolution Whole Slide Images (WSIs) presents an additional computational challenge. This makes tilewise processing necessary during training and inference of deep learning networks. Instance normalization has a substantial positive effect in style transfer GAN applications but with tilewise inference, it has the tendency to cause a tiling artifact in reconstructed WSIs. In this paper we propose a novel perceptual embedding consistency (PEC) loss forcing the network to learn color, contrast and brightness invariant features in the latent space and hence substantially reducing the aforementioned tiling artifact. Our approach results in more seamless reconstruction of the virtual WSIs. We validate our method quantitatively by comparing the virtually generated images to their corresponding consecutive real stained images.We compare our results to state-of-the-art unsupervised style transfer methods and to the measures obtained from consecutive real stained tissue slide images. We demonstrate our hypothesis about the effect of the PEC loss by comparing model robustness to color, contrast and brightness perturbations and visualizing bottleneck embeddings. We validate the robustness of the bottleneck feature maps by measuring their sensitivity to the different perturbations and using them in a tumor segmentation task. Additionally, we propose a preliminary validation of the virtual staining application by comparing interpretation of 2 pathologists on real and virtual tiles and inter-pathologist agreement","tags":["Stain Normalization","Histology","Powerful Representation","Domain Adaptation","Style Transfer"],"title":"Seamless Virtual Whole Slide Image Synthesis and Validation Using Perceptual Embedding Consistency","type":"publication"},{"authors":[],"categories":null,"content":"","date":1579251600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"6c5b3f3b2f9aed0dde1629d435656ebf","permalink":"https://albarqouni.github.io/talk/ulm2019/","publishdate":"2020-01-17T09:00:00Z","relpermalink":"/talk/ulm2019/","section":"talk","summary":"","tags":["Deep Learning","Federated Learning"],"title":"Invited Talk: Towards Deep Federated Learning in Healthcare","type":"talk"},{"authors":[],"categories":null,"content":"","date":1579086900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"b70a28827b88bd561f8bc44698c40d95","permalink":"https://albarqouni.github.io/talk/eth2020/","publishdate":"2020-01-15T11:15:00Z","relpermalink":"/talk/eth2020/","section":"talk","summary":"","tags":["Uncertainty","Deep Learning"],"title":"Modelling Labels Uncertainty in Medical Imaging","type":"talk"},{"authors":[],"categories":null,"content":"","date":1578474000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"c561a857c76d5f707210688f4ac2acf4","permalink":"https://albarqouni.github.io/talk/ai4h/","publishdate":"2020-01-08T09:00:00Z","relpermalink":"/talk/ai4h/","section":"talk","summary":"","tags":["Deep Learning","Domain Adaptation"],"title":"Keynote Speaker: AI in Healthcare","type":"talk"},{"authors":["Roger D Soberanis-Mukul","Maxime Kayser","Anna-Maria Zvereva","Peter Klare","Nassir Navab","Shadi Albarqouni"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"5aadbbb1af81ef362d2c4dcb466819f2","permalink":"https://albarqouni.github.io/publication/soberanis-2020-learning/","publishdate":"2020-04-04T21:37:29.802052Z","relpermalink":"/publication/soberanis-2020-learning/","section":"publication","summary":"Colorectal polyps are abnormalities in the colon tissue that can develop into colorectal cancer. The survival rate for patients is higher when the disease is detected at an early stage and polyps can be removed before they develop into malignant tumors. Deep learning methods have become the state of art in automatic polyp detection. However, the performance of current models heavily relies on the size and quality of the training datasets. Endoscopic video sequences tend to be corrupted by different artifacts affecting visibility and hence, the detection rates. In this work, we analyze the effects that artifacts have in the polyp localization problem. For this, we evaluate the RetinaNet architecture, originally defined for object localization. We also define a model inspired by the learning without forgetting framework, which allows us to employ artifact detection knowledge in the polyp localization problem. Finally, we perform several experiments to analyze the influence of the artifacts in the performance of these models. To our best knowledge, this is the first extensive analysis of the influence of artifact in polyp localization and the first work incorporating learning without forgetting ideas for simultaneous artifact and polyp localization tasks.","tags":["Learning without forgetting","Endoscopy","Medical Imaging","Deep Learning"],"title":"A learning without forgetting approach to incorporate artifact knowledge in polyp localization tasks","type":"publication"},{"authors":["Sharib Ali","Felix Zhou","Barbara Braden","Adam Bailey","Suhui Yang","Guanju Cheng","Pengyi Zhang","Xiaoqiong Li","Maxime Kayser","Roger D Soberanis-Mukul"," others"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"e04971bbb3386a771acdfadeae94673f","permalink":"https://albarqouni.github.io/publication/ali-2020-objective/","publishdate":"2020-04-04T21:37:29.796122Z","relpermalink":"/publication/ali-2020-objective/","section":"publication","summary":"We present a comprehensive analysis of the submissions to the first edition of the Endoscopy Artefact Detection challenge (EAD). Using crowd-sourcing, this initiative is a step towards understanding the limitations of existing state-of-the-art computer vision methods applied to endoscopy and promoting the development of new approaches suitable for clinical translation. Endoscopy is a routine imaging technique for the detection, diagnosis and treatment of diseases in hollow-organs; the esophagus, stomach, colon, uterus and the bladder. However the nature of these organs prevent imaged tissues to be free of imaging artefacts such as bubbles, pixel saturation, organ specularity and debris, all of which pose substantial challenges for any quantitative analysis. Consequently, the potential for improved clinical outcomes through quantitative assessment of abnormal mucosal surface observed in endoscopy videos is presently not realized accurately. The EAD challenge promotes awareness of and addresses this key bottleneck problem by investigating methods that can accurately classify, localize and segment artefacts in endoscopy frames as critical prerequisite tasks. Using a diverse curated multi-institutional, multi-modality, multi-organ dataset of video frames, the accuracy and performance of 23 algorithms were objectively ranked for artefact detection and segmentation. The ability of methods to generalize to unseen datasets was also evaluated. The best performing methods (top 15%) propose deep learning strategies to reconcile variabilities in artefact appearance with respect to size, modality, occurrence and organ type. However, no single method outperformed across all tasks. Detailed analyses reveal the shortcomings of current training strategies and highlight the need for developing new optimal metrics to accurately quantify the clinical applicability of methods.","tags":["Endoscopy","Deep Learning","Medical Imaging"],"title":"An objective comparison of detection and segmentation algorithms for artefacts in clinical endoscopy","type":"publication"},{"authors":["Roger David Soberanis Mukul","Nassir Navab","Shadi Albarqouni"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608826908,"objectID":"29158d9a40115126dacf20159ad64209","permalink":"https://albarqouni.github.io/publication/mukul-2020-uncertainty/","publishdate":"2020-12-24T16:21:48.471513Z","relpermalink":"/publication/mukul-2020-uncertainty/","section":"publication","summary":"","tags":[],"title":"An Uncertainty-Driven GCN Refinement Strategy for Organ Segmentation","type":"publication"},{"authors":["Ario Sadafi","Asya Makhro","Anna Bogdanova","Nassir Navab","Tingying Peng","Shadi Albarqouni","Carsten Marr"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608826907,"objectID":"389facceb32015ded7162bf3b694c21b","permalink":"https://albarqouni.github.io/publication/sadafi-2020-attention/","publishdate":"2020-12-24T16:21:47.276339Z","relpermalink":"/publication/sadafi-2020-attention/","section":"publication","summary":"","tags":[],"title":"Attention Based Multiple Instance Learning for Classification of Blood Cell Disorders","type":"publication"},{"authors":["Christoph Baur","Benedikt Wiestler","Shadi Albarqouni","Nassir Navab"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605181371,"objectID":"41737a11783b0abda852c4d4710e5456","permalink":"https://albarqouni.github.io/publication/baur-2020-bayesian/","publishdate":"2020-11-12T11:42:51.585644Z","relpermalink":"/publication/baur-2020-bayesian/","section":"publication","summary":"","tags":[],"title":"Bayesian Skip-Autoencoders for Unsupervised Hyperintense Anomaly Detection in High Resolution Brain Mri","type":"publication"},{"authors":["Nadav Shapira","Julia Fokuhl","Manuel Schultheiß","Stefanie Beck","Felix K Kopp","Daniela Pfeiffer","Julia Dangelmaier","Gregor Pahn","Andreas P Sauter","Bernhard Renger"," others"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"c1def93831c54e13e2f21e7e884cb56d","permalink":"https://albarqouni.github.io/publication/shapira-2020-benefit/","publishdate":"2020-04-04T21:37:29.907307Z","relpermalink":"/publication/shapira-2020-benefit/","section":"publication","summary":"Dual Energy CT is a modern imaging technique that is utilized in clinical practice to acquire spectral information for various diagnostic purposes including the identification, classification, and characterization of different liver lesions. It provides additional information that, when compared to the information available from conventional CT datasets, has the potential to benefit existing computer vision techniques by improving their accuracy and reliability. In order to evaluate the additional value of spectral versus conventional datasets when being used as input for machine learning algorithms, we implemented a weakly-supervised Convolutional Neural Network (CNN) that learns liver lesion localization and classification without pixel-level ground truth annotations. We evaluated the lesion classification (healthy, cyst, hypodense metastasis) and localization performance of the network for various conventional and spectral input datasets obtained from the same CT scan. The best results for lesion localization were found for the spectral datasets with distances of 8.22 ± 10.72 mm, 8.78 ± 15.21 mm and 8.29 ± 12.97 mm for iodine maps, 40 keV and 70 keV virtual mono-energetic images, respectively, while lesion localization distances of 10.58 ± 17.65 mm were measured for the conventional dataset. In addition, the 40 keV virtual mono-energetic datasets achieved the highest overall lesion classification accuracy of 0.899 compared to 0.854 measured for the conventional datasets. The enhanced localization and classification results that we observed for spectral CT data demonstrates that combining machine-learning technology with spectral CT information may improve the clinical workflow as well as the diagnostic accuracy.","tags":["Deep Learning","Medical Imaging","Radiology","Weakly-Supervised Learning"],"title":"Benefit of dual energy CT for lesion localization and classification with convolutional neural networks","type":"publication"},{"authors":["Shadi Albarqouni","Spyridon Bakas","Konstantinos Kamnitsas"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605169472,"objectID":"ea33e814915f9b32ade092b1121a8eb3","permalink":"https://albarqouni.github.io/publication/albarqouni-2020-domain/","publishdate":"2020-11-12T08:24:32.854496Z","relpermalink":"/publication/albarqouni-2020-domain/","section":"publication","summary":"","tags":[],"title":"Domain Adaptation and Representation Transfer, and Distributed and Collaborative Learning","type":"publication"},{"authors":["Salome Kazeminia","Christoph Baur","Arjan Kuijper","Bram van Ginneken","Nassir Navab","Shadi Albarqouni","Anirban Mukhopadhyay"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608826901,"objectID":"76e9984b4d3973dece116480e218969e","permalink":"https://albarqouni.github.io/publication/kazeminia-2020-gans/","publishdate":"2020-12-24T16:21:41.704466Z","relpermalink":"/publication/kazeminia-2020-gans/","section":"publication","summary":"","tags":[],"title":"GANs for medical image analysis","type":"publication"},{"authors":["Mohammad Eslami","Solale Tabarestani","Shadi Albarqouni","Ehsan Adeli","Nassir Navab","Malek Adjouadi"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"599665bb970f001c187a3cd46caa93af","permalink":"https://albarqouni.github.io/publication/eslami-2020-image/","publishdate":"2020-04-04T21:37:29.567074Z","relpermalink":"/publication/eslami-2020-image/","section":"publication","summary":"Chest X-ray radiography is one of the earliest medical imaging technologies and remains one of the most widely-used for diagnosis, screening, and treatment follow up of diseases related to lungs and heart. The literature in this field of research reports many interesting studies dealing with the challenging tasks of bone suppression and organ segmentation but performed separately, limiting any learning that comes with the consolidation of parameters that could optimize both processes. This study, and for the first time, introduces a multitask deep learning model that generates simultaneously the bone-suppressed image and the organ-segmented image, enhancing the accuracy of tasks, minimizing the number of parameters needed by the model and optimizing the processing time, all by exploiting the interplay between the network parameters to benefit the performance of both tasks. The architectural design of this model, which relies on a conditional generative adversarial network, reveals the process on how the well-established pix2pix network (image-to-image network) is modified to fit the need for multitasking and extending it to the new image-to-images architecture. The developed source code of this multitask model is shared publicly on Github as the first attempt for providing the two-task pix2pix extension, a supervised/paired/aligned/registered image-to-images translation which would be useful in many multitask applications. Dilated convolutions are also used to improve the results through a more effective receptive field assessment. The comparison with state-of-the-art algorithms along with ablation study and a demonstration video are provided to evaluate efficacy and gauge the merits of the proposed approach.","tags":["Deep Learning","Medical Imaging","Radiology","Generative Adversarial Networks"],"title":"Image-to-Images Translation for Multi-Task Organ Segmentation and Bone Suppression in Chest X-Ray Radiography","type":"publication"},{"authors":["Yousef Yeganeh","Azade Farshad","Nassir Navab","Shadi Albarqouni"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608826907,"objectID":"541242f112d685d63b06027c00ecbfc3","permalink":"https://albarqouni.github.io/publication/yeganeh-2020-inverse/","publishdate":"2020-12-24T16:21:47.379132Z","relpermalink":"/publication/yeganeh-2020-inverse/","section":"publication","summary":"","tags":[],"title":"Inverse Distance Aggregation for Federated Learning with Non-IID Data","type":"publication"},{"authors":["Nadav Shapira","Julia Fokuhl","Manuel Schultheiß","Stefanie Beck","Felix K Kopp","Daniela Pfeiffer","Julia Dangelmaier","Gregor Pahn","Andreas P Sauter","Bernhard Renger"," others"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"a5074abadb034110e002ded46287ecea","permalink":"https://albarqouni.github.io/publication/shapira-2020-liver/","publishdate":"2020-04-04T21:37:29.759921Z","relpermalink":"/publication/shapira-2020-liver/","section":"publication","summary":"","tags":["Deep Learning","Medical Imaging","Radiology","Weakly-Supervised Learning"],"title":"Liver lesion localisation and classification with convolutional neural networks: a comparison between conventional and spectral computed tomography","type":"publication"},{"authors":["Roger D Soberanis-Mukul","Shadi Albarqouni","Nassir Navab"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608826907,"objectID":"783d2d7d7e88ea6a3905d3d1aeaebab9","permalink":"https://albarqouni.github.io/publication/soberanis-2020-polyp/","publishdate":"2020-12-24T16:21:47.591088Z","relpermalink":"/publication/soberanis-2020-polyp/","section":"publication","summary":"","tags":[],"title":"Polyp-artifact relationship analysis using graph inductive learned representations","type":"publication"},{"authors":["Amelia Jiménez-Sánchez","Anees Kazi","Shadi Albarqouni","Chlodwig Kirchhoff","Peter Biberthaler","Nassir Navab","Sonja Kirchhoff","Diana Mateus"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608826906,"objectID":"65c75930d108260e16534ad309319c4e","permalink":"https://albarqouni.github.io/publication/jimenez-2020-precise/","publishdate":"2020-12-24T16:21:46.648944Z","relpermalink":"/publication/jimenez-2020-precise/","section":"publication","summary":"","tags":[],"title":"Precise proximal femur fracture classification for interactive training and surgical planning.","type":"publication"},{"authors":["Arianne Tran","Jakob Weiss","Shadi Albarqouni","Shahrooz Faghi Roohi","Nassir Navab"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608826908,"objectID":"e8ac6b9eb9ee8f237b7c8a6e5c0b8a87","permalink":"https://albarqouni.github.io/publication/tran-2020-retinal/","publishdate":"2020-12-24T16:21:47.929032Z","relpermalink":"/publication/tran-2020-retinal/","section":"publication","summary":"","tags":[],"title":"Retinal Layer Segmentation Reformulated as OCT Language Processing","type":"publication"},{"authors":["Tariq Bdair","Nassir Navab","Shadi Albarqouni"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"cfc117cc954c03b829ae3c1cf8d2c98e","permalink":"https://albarqouni.github.io/publication/bdair-2020-roam/","publishdate":"2020-11-12T11:42:50.983492Z","relpermalink":"/publication/bdair-2020-roam/","section":"publication","summary":"","tags":[],"title":"ROAM: Random Layer Mixup for Semi-Supervised Learning in Medical Imaging","type":"publication"},{"authors":["Christoph Baur","Benedikt Wiestler","Shadi Albarqouni","Nassir Navab"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605181371,"objectID":"30f155e891c31f4347e4e058f4f517ab","permalink":"https://albarqouni.github.io/publication/baur-2020-scale/","publishdate":"2020-11-12T11:42:51.683723Z","relpermalink":"/publication/baur-2020-scale/","section":"publication","summary":"","tags":[],"title":"Scale-Space Autoencoders for Unsupervised Anomaly Segmentation in Brain MRI","type":"publication"},{"authors":["Christoph Baur","Robert Graf","Benedikt Wiestler","Shadi Albarqouni","Nassir Navab"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605181372,"objectID":"c5254cce9893210ece6a24a906691339","permalink":"https://albarqouni.github.io/publication/baur-2020-steganomaly/","publishdate":"2020-11-12T11:42:52.50426Z","relpermalink":"/publication/baur-2020-steganomaly/","section":"publication","summary":"","tags":[],"title":"SteGANomaly: Inhibiting CycleGAN Steganography for Unsupervised Anomaly Detection in Brain MRI","type":"publication"},{"authors":["Maxime Kayser","Roger D Soberanis-Mukul","Anna-Maria Zvereva","Peter Klare","Nassir Navab","Shadi Albarqouni"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608826907,"objectID":"d20299afcfe8e11576ea91dd889c4ba9","permalink":"https://albarqouni.github.io/publication/kayser-2020-understanding/","publishdate":"2020-12-24T16:21:47.483723Z","relpermalink":"/publication/kayser-2020-understanding/","section":"publication","summary":"","tags":[],"title":"Understanding the effects of artifacts on automated polyp detection and incorporating that knowledge via learning without forgetting","type":"publication"},{"authors":[],"categories":null,"content":"","date":1576488600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"8c0468fb2e4f8bd1af9dba05eb8ee4fb","permalink":"https://albarqouni.github.io/talk/haicu2019/","publishdate":"2019-12-16T09:30:00Z","relpermalink":"/talk/haicu2019/","section":"talk","summary":"","tags":["Federated Learning","Deep Learning"],"title":"Invited Talk: Towards Deep Federated Learning in Healthcare","type":"talk"},{"authors":[],"categories":null,"content":"","date":1574853300,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"16d89e4c8ead1de372ff428f8e70af31","permalink":"https://albarqouni.github.io/talk/guc2019/","publishdate":"2019-11-27T11:15:00Z","relpermalink":"/talk/guc2019/","section":"talk","summary":"","tags":["Federated Learning","Deep Learning"],"title":"Keynote Speaker: Towards Deep Federated Learning in Healthcare","type":"talk"},{"authors":[],"categories":null,"content":"","date":1570982400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"22c907a262a7b6762b165942441934d0","permalink":"https://albarqouni.github.io/talk/dart2019/","publishdate":"2019-10-13T16:00:00Z","relpermalink":"/talk/dart2019/","section":"talk","summary":"","tags":["Deep Learning"],"title":"Organizing Committee Member at MICCAI DART 2019","type":"talk"},{"authors":[],"categories":null,"content":"","date":1570971600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"93fece8ef760943bf553d52a52e1271c","permalink":"https://albarqouni.github.io/talk/compay2019/","publishdate":"2019-10-13T13:00:00Z","relpermalink":"/talk/compay2019/","section":"talk","summary":"","tags":["Deep Learning"],"title":"Organizing Committee Member at MICCAI COMPAY 2019","type":"talk"},{"authors":[],"categories":null,"content":"","date":1568883600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"54c7f7d017ae38623c85f7ddcb728fc1","permalink":"https://albarqouni.github.io/talk/icann2019/","publishdate":"2019-09-19T09:00:00Z","relpermalink":"/talk/icann2019/","section":"talk","summary":"Deep Learning (DL) has emerged as a leading technology in computer science for accomplishing many challenging tasks. This technology shows an outstanding performance in a broad range of computer vision and medical applications. However, this success comes at the cost of collecting and processing a massive amount of data, which are in healthcare often inaccessible due to privacy issues.\nFederated Learning is a new technology that allows training DL models without sharing the data. Using Federated Learning, DL models at local hospitals share only the trained parameters with a centralized DL model, which is, in return, responsible for updating the local DL models as well. Yet, a couple of well-known challenges in the medical imaging community, e.g., heterogeneity, domain shift, scarify of labeled data and handling multi-modal data, might hinder the utilization of Federated Learning.\nIn this talk, a couple of proposed methods, to tackle the challenges above, will be presented paving the way to researchers to integrate such methods into the privacy-preserved federated learning.","tags":["Federated Learning","Deep Learning"],"title":"Keynote Speaker: Towards Deep Federated Learning in Healthcare","type":"talk"},{"authors":null,"categories":null,"content":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you\u0026rsquo;ll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.toml file.\n```python import pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head() ```  renders as\nimport pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head()  Math Academic supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.toml file.\nTo render inline or block math, wrap your LaTeX math with $...$ or $$...$$, respectively.\nExample math block:\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |} {\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$  renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left |\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right |^2}$$\nExample inline math $\\nabla F(\\mathbf{x}_{n})$ renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the \\\\ math linebreak:\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\\ 1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$  renders as\n$$f(k;p_0^) = \\begin{cases} p_0^ \u0026amp; \\text{if }k=1, \\\n1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$\nDiagrams Academic supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ```  renders as\ngraph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2]  An example sequence diagram:\n```mermaid sequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! ```  renders as\nsequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good!  An example Gantt diagram:\n```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ```  renders as\ngantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d  An example class diagram:\n```mermaid classDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() } ```  renders as\nclassDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() }  An example state diagram:\n```mermaid stateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*] ```  renders as\nstateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*]  Todo lists You can even write your todo lists in Academic too:\n- [x] Write math example - [x] Write diagram example - [ ] Do something else  renders as\n Write math example Write diagram example Do something else  Tables Represent your data in tables:\n| First Header | Second Header | | ------------- | ------------- | | Content Cell | Content Cell | | Content Cell | Content Cell |  renders as\n   First Header Second Header     Content Cell Content Cell   Content Cell Content Cell    Asides Academic supports a shortcode for asides, also referred to as notices, hints, or alerts. By wrapping a paragraph in {{% alert note %}} ... {{% /alert %}}, it will render as an aside.\n{{% alert note %}} A Markdown aside is useful for displaying notices, hints, or definitions to your readers. {{% /alert %}}  renders as\n A Markdown aside is useful for displaying notices, hints, or definitions to your readers.   Icons Academic enables you to use a wide range of icons from Font Awesome and Academicons in addition to emojis.\nHere are some examples using the icon shortcode to render icons:\n{{\u0026lt; icon name=\u0026quot;terminal\u0026quot; pack=\u0026quot;fas\u0026quot; \u0026gt;}} Terminal {{\u0026lt; icon name=\u0026quot;python\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} Python {{\u0026lt; icon name=\u0026quot;r-project\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} R  renders as\n  Terminal\n Python\n R\nDid you find this page helpful? Consider sharing it 🙌 ","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"https://albarqouni.github.io/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/writing-technical-content/","section":"post","summary":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.","tags":null,"title":"Writing technical content in Academic","type":"post"},{"authors":["Mai Bui","Sergey Zakharov","Shadi Albarqouni","Slobodan Ilic"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"b3ce5d9873861195a953564d6a6a3350","permalink":"https://albarqouni.github.io/publication/bui-2019-method/","publishdate":"2020-04-04T21:37:29.817201Z","relpermalink":"/publication/bui-2019-method/","section":"publication","summary":"","tags":null,"title":"Method for determining a pose of an object in an environment of the object using multi task learning and control device","type":"publication"},{"authors":[],"categories":null,"content":"","date":1560690000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"8350a113436487b310d1929df5908582","permalink":"https://albarqouni.github.io/talk/zeiss2019/","publishdate":"2019-06-16T13:00:00Z","relpermalink":"/talk/zeiss2019/","section":"talk","summary":"","tags":["AI in Healthcare","Deep Learning"],"title":"Keynote Speaker: Deep Learning in Medical Imaging","type":"talk"},{"authors":[],"categories":[],"content":"Brief Progress of  Academic | Documentation\n Dataset  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Pathology Quantification:  To be able to quantify the pathologies in thorax CT scans, one needs to segment the pathologies, and probably classify them into common ones characterizing the COVID-19, e.g.,  Ground Glass Opacity (GGO)  Consolidations  Scarr  Pleueral Effusion      Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604687686,"objectID":"1dec7c58881137190e2218f07d266519","permalink":"https://albarqouni.github.io/slides/_example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/_example/","section":"slides","summary":"Brief Progress of  Academic | Documentation\n Dataset  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Pathology Quantification:  To be able to quantify the pathologies in thorax CT scans, one needs to segment the pathologies, and probably classify them into common ones characterizing the COVID-19, e.","tags":["Deep Learning","Medical Imaging","Radiology","COVID-19"],"title":"AI meets COVID-19","type":"slides"},{"authors":["Shadi Albarqouni"],"categories":[],"content":"from IPython.core.display import Image Image('https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png')  print(\u0026quot;Welcome to Academic!\u0026quot;)  Welcome to Academic!  Install Python and JupyterLab  Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb  The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata ( front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post's title date: 2019-09-01 # Put any other Academic metadata here... ---  Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=.  Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"https://albarqouni.github.io/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":["Shadi Albarqouni"],"categories":[],"content":"Seminar on\nFederated Learning in Healthcare  Shadi Albarqouni, PhD\nAI Young Investigator Group Leader at Helmholtz AI | TUM Junior Fellow at TU Munich\n Content  Brief introduction about Federated Learning It is the right thing at the right time! Course structure Registration    \n Deep Learning  success comes at the cost of collecting and processing a massive amount of data , which often are not accessible due to privacy issues .\n Federated Learning  has been recently introduced to allow training DL models without sharing the data .\n Taken from Rieke, N., Hancox, J., Li, W., Milletari, F., Roth, H.R., Albarqouni, S., Bakas, S., Galtier, M.N., Landman, B.A., Maier-Hein, K. and Ourselin, S., 2020. The future of digital health with federated learning. NPJ digital medicine, 3(1), pp.1-7. \n The principal challenges , to overcome, concern the nature of medical data, namely\n Data heterogeneity; severe class-imbalance, few amounts of annotated data, inter-/intra-scanners variability (domain shift), and inter-/intra-observer variability (noisy annotations)  System Heterogeneity   Privacy-Issues   Communication Efficiency     Right thing at the right time     \n  \n  \n  \n Course Structure  Basic Info.\n Type: Master Seminar (IN2107) Language: English SWS: 2 ECTS: 5 Webpage: https://albarqouni.github.io/courses/flhsose2021/ Time: Fridays, 10 - 12 Location: Virtual Event Requirements: Solid Background in Machine/Deep Learning   Tutors\n Objectives: Learn through read, understand, present, and discuss many scientific papers1 tackling the challenges present in Federated Learning.\n1Our pool of papers includes the ones published in NeurIPS, ICML, ICLR, IEEE TMI, MedIA, MICCAI, MIDL, and ISBI.\n Schedule:    Date Session     16.04.2021 Federated Learning; Challenges, Methods, and Future   30.04.2021 Data Heterogeneity I   14.05.2021 Data Heterogeneity II   28.05.2021 System Heterogeneity   11.06.2021 Privacy-Issues   25.06.2021 Explainability and Accountability     Evaluation:  Presentation (60%): The selected paper is presented to the other participants (30 minutes presentation plus 10 minutes Q\u0026amp;A)  Blog | Poster (30%): A blog post of 1000-1500 words excluding references should be submitted before the deadline  Attendance (10%): Students are expected to participate actively in all seminar sessions    Insights into the course evaluation?  If you are interested in this seminsr course, please write a brief motivation paragraph (few lines) showing your interest and your background in Machine/Deep Learning and send it with a subject “FLH_Motivation”, to Shadi Albarqouni (shadi.albarqouni@tum.de). Deadline is 16.02.2021.\nDon’t forget to register at TUM matching system 11.02 to 16.02.2021: register via matching.in.tum.de \n Questions? ","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604421259,"objectID":"bc37ca693c74155a9ab70a47ec0b1a17","permalink":"https://albarqouni.github.io/slides/federated/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/federated/","section":"slides","summary":"Introduction to Federated Learning in Healthcare","tags":["Deep Learning","Medical Imaging"],"title":"Preliminary Meeting for the seminar on Federated Learning in Healthcare","type":"slides"},{"authors":["Shadi Albarqouni"],"categories":[],"content":"Presentation and Blog Post Guidelines  Shadi Albarqouni\nAI Young Investigator Group Leader at Helmholtz AI | TUM Junior Fellow at TU Munich\n05.11.2020\n Presentation Guidelines  First Slide: Please have the title of the paper, authors (\u0026lt;3), conference proceedings, or journal, and the name of the presenter.   Example: The Future of Digital Health with Federated Learning Rieke et al., Nature Digital Medicine, 2020 Presenter: Firstname Surname    Introduction: In the first few slides, you need to introduce the subject to the audience. A brief background (big picture) and a few related works (more concise) would help you to position your paper in the big picture. It is quite important to talk about the key conclusions at the very beginning. The rationale for the paper, i.e. why you did the work?, has to be addressed by the end of the Intro. slides.    Methodology: You need to explain the method in details, if possible. Start with an overview of the framework (e.g. flow chart); input, output, and core components, before you dive deeper into the key contributions; e.g. design architecture, objective functions, \u0026hellip;etc. Details that might distract the audience can be moved to the backup slides. In short, explain how did you do it?    Experiments and Results: You need to explain the experimental designs; datasets, evaluation metrics, and training setup, and the rationale behind them, before you show the key results. The figures should be clearly labeled, e.g. explain the figures axes before you describe the results addressing the question what did you find?    Conclusion \u0026amp; Future Work: Discuss the results (your interpretation), before you list the concluding reamrks, learned lessons, and future research directions.    Group Discussion: This is the most important part where you need to list a few major things that you need to discuss with the group, for example:  How the paper could be improved? e.g. critique on the proposed method, design choices, missing experiments, or inappropriate evaluation metrics. How the paper could be applied in medical domain? e.g. challenges in healthcare. Have a look at the reviewers feedback, if available, e.g. openreview      Needless to Say:  Read the paper carefully, and look for complementary materials; blog posts, videos, or code repo. to better understand the paper. Be mindful of time. You have 30 mins for points 1-4, and 15 mins for point 5. As a rule of thumb, # slides \u0026lt; given time slot in mins. Build a compelling story and try to engage your audience. List the References in the footer of the corresponding slide Practice, practice, practice      Blog Post  Guidelines   The secret to getting into the deep learning community is high quality blogging. Read 5 different blog posts about the same subject and then try to synthesize your own view. Don’t just write something ok, either — take 3 or 4 full days on a post and try to make it as short and simple (yet complete) as possible. \u0026ndash; Andrew Trask, DeepMind\n  This is a free-style blog post! One can hardly enforce guidelines. However, I personally liked the Dos and Don\u0026rsquo;ts appeared in this blog post. Here some examples\n  What is Federated Learning?   Federated Learning: A Guide to Collaborative Training with Decentralized Sensitive Data – Part 1  Do we need deep graph neural networks?  FACT Diagnostic: How to Better Understand Trade-offs Involving Group Fairness Other blog sites: CMU ML Blog, BAIR Blog   Helpful resourses   How to read a paper? \u0026ndash;Three-pass method  How to read a research paper?  How to Read Scientific Papers Quickly \u0026amp; Efficiently  How to write a blog post from your journal article?  Advice for Better Blog Posts   Image source: phdcomics.com. @Jorge Cham\n Should you have any questions, please drop me an email at shadi.albarqouni@tum.de\n @ShadiAlbarqouni\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604687686,"objectID":"2fdc6c5144a76b3155f05cb357bef95c","permalink":"https://albarqouni.github.io/slides/pres/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/pres/","section":"slides","summary":"Introduction to Federated Learning in Healthcare","tags":["Deep Learning","Medical Imaging"],"title":"Preliminary Meeting for the seminar on Federated Learning in Healthcare","type":"slides"},{"authors":[],"categories":null,"content":"","date":1548493200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"534238b8e0af73aa70c54c4e2701af75","permalink":"https://albarqouni.github.io/talk/neocolam2019/","publishdate":"2019-01-26T09:00:00Z","relpermalink":"/talk/neocolam2019/","section":"talk","summary":"","tags":["AI in Healthcare","Deep Learning"],"title":"Keynote Speaker: Artificial Intelligence. Just Math?","type":"talk"},{"authors":["Hendrik Burwinkel","Anees Kazi","Gerome Vivar","Shadi Albarqouni","Guillaume Zahnd","Nassir Navab","Seyed-Ahmad Ahmadi"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"936897eb351900f9f4e302c00801fc2a","permalink":"https://albarqouni.github.io/publication/burwinkel-2019-adaptive/","publishdate":"2020-04-04T21:37:29.606224Z","relpermalink":"/publication/burwinkel-2019-adaptive/","section":"publication","summary":"Recently, Geometric Deep Learning (GDL) has been introduced as a novel and versatile framework for computer-aided disease classification. GDL uses patient meta-information such as age and gender to model patient cohort relations in a graph structure. Concepts from graph signal processing are leveraged to learn the optimal mapping of multi-modal features, e.g. from images to disease classes. Related studies so far have considered image features that are extracted in a pre-processing step. We hypothesize that such an approach prevents the network from optimizing feature representations towards achieving the best performance in the graph network. We propose a new network architecture that exploits an inductive end-to-end learning approach for disease classification, where filters from both the CNN and the graph are trained jointly. We validate this architecture against state-of-the-art inductive graph networks and demonstrate significantly improved classification scores on a modified MNIST toy dataset, as well as comparable classification results with higher stability on a chest X-ray image dataset. Additionally, we explain how the structural information of the graph affects both the image filters and the feature learning.","tags":["Deep Learning","Medical Imaging","Radiology","Prior Knowledge"],"title":"Adaptive image-feature learning for disease classification using inductive graph networks","type":"publication"},{"authors":["Mai Bui","Christoph Baur","Nassir Navab","Slobodan Ilic","Shadi Albarqouni"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"509af25ad09c0038d8fe907f6c3c1a5a","permalink":"https://albarqouni.github.io/publication/bui-2019-adversarial/","publishdate":"2020-04-04T21:37:29.699081Z","relpermalink":"/publication/bui-2019-adversarial/","section":"publication","summary":"Despite recent advances on the topic of direct camera pose regression using neural networks, accurately estimating the camera pose of a single RGB image still remains a challenging task. To address this problem, we introduce a novel framework based, in its core, on the idea of implicitly learning the joint distribution of RGB images and their corresponding camera poses using a discriminator network and adversarial learning. Our method allows not only to regress the camera pose from a single image, however, also offers a solely RGB-based solution for camera pose refinement using the discriminator network. Further, we show that our method can effectively be used to optimize the predicted camera poses and thus improve the localization accuracy. To this end, we validate our proposed method on the publicly available 7-Scenes dataset improving upon the results of direct camera pose regression methods.","tags":["Computer Vision","Pose Estimation","Uncertainty","Generative Adversarial Networks"],"title":"Adversarial Networks for Camera Pose Regression and Refinement","type":"publication"},{"authors":["Qian Wang","Fausto Milletari","Hien V Nguyen","Shadi Albarqouni","M Jorge Cardoso","Nicola Rieke","Ziyue Xu","Konstantinos Kamnitsas","Vishal Patel","Badri Roysam"," others"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"7e768b180bbcbc43a54aecafbea5b6ad","permalink":"https://albarqouni.github.io/publication/wang-2019-domain/","publishdate":"2020-04-04T21:37:29.753047Z","relpermalink":"/publication/wang-2019-domain/","section":"publication","summary":"","tags":null,"title":"Domain Adaptation and Representation Transfer and Medical Image Learning with Less Labels and Imperfect Data: First MICCAI Workshop, DART 2019, and First International Workshop, MIL3ID 2019, Shenzhen, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 13 and 17, 2019, Proceedings","type":"publication"},{"authors":["Maxime Kayser","Roger D Soberanis-Mukul","Shadi Albarqouni","Nassir Navab"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608826904,"objectID":"cf5d143b13e023572e1cc13676e336cd","permalink":"https://albarqouni.github.io/publication/kayser-2019-focal/","publishdate":"2020-12-24T16:21:44.128562Z","relpermalink":"/publication/kayser-2019-focal/","section":"publication","summary":"","tags":[],"title":"Focal loss for artefact detection in medical endoscopy","type":"publication"},{"authors":["Christoph Baur","Benedikt Wiestler","Shadi Albarqouni","Nassir Navab"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"c5d6bd14fbd6d8dffa19835bb0df099f","permalink":"https://albarqouni.github.io/publication/baur-2019-fusing/","publishdate":"2020-04-04T21:37:29.545662Z","relpermalink":"/publication/baur-2019-fusing/","section":"publication","summary":"Unsupervised Deep Learning for Medical Image Analysis is increasingly gaining attention, since it relieves from the need for annotating training data. Recently, deep generative models and representation learning have lead to new, exciting ways for unsupervised detection and delineation of biomarkers in medical images, such as lesions in brain MR. Yet, Supervised Deep Learning methods usually still perform better in these tasks, due to an optimization for explicit objectives. We aim to combine the advantages of both worlds into a novel framework for learning from both labeled \u0026 unlabeled data, and validate our method on the challenging task of White Matter lesion segmentation in brain MR images. The proposed framework relies on modeling normality with deep representation learning for Unsupervised Anomaly Detection, which in turn provides optimization targets for training a supervised segmentation model from unlabeled data. In our experiments we successfully use the method in a Semi-supervised setting for tackling domain shift, a well known problem in MR image analysis, showing dramatically improved generalization. Additionally, our experiments reveal that in a completely Unsupervised setting, the proposed pipeline even outperforms the Deep Learning driven anomaly detection that provides the optimization targets.","tags":["Medical Imaging","Deep Learning","Radiology","Generative Adversarial Networks","Anomaly Detection"],"title":"Fusing unsupervised and supervised deep learning for white matter lesion segmentation","type":"publication"},{"authors":["Anees Kazi","Shayan Shekarforoush","S Arvind Krishna","Hendrik Burwinkel","Gerome Vivar","Benedict Wiestler","Karsten Kortüm","Seyed-Ahmad Ahmadi","Shadi Albarqouni","Nassir Navab"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"62baf4996908c4893d6c4637b4037f9f","permalink":"https://albarqouni.github.io/publication/kazi-2019-graph/","publishdate":"2020-04-04T21:37:29.661628Z","relpermalink":"/publication/kazi-2019-graph/","section":"publication","summary":"Clinicians implicitly incorporate the complementarity of multi-modal data for disease diagnosis. Often a varied order of importance for this heterogeneous data is considered for personalized decisions. Current learning-based methods have achieved better performance with uniform attention to individual information, but a very few have focused on patient-specific attention learning schemes for each modality. Towards this, we introduce a model which not only improves the disease prediction but also focuses on learning patient-specific order of importance for multi-modal data elements. In order to achieve this, we take advantage of LSTM-based attention mechanism and graph convolutional networks (GCNs) to design our model. GCNs learn multi-modal but class-specific features from the entire population of patients, whereas the attention mechanism optimally fuses these multi-modal features into a final decision, separately for each patient. In this paper, we apply the proposed approach for disease prediction task for Parkinson’s and Alzheimer’s using two public medical datasets.","tags":["Graph Convolutional Networks","Deep Learning","Medical Imaging","Radiology","Prior Knowledge"],"title":"Graph Convolution Based Attention Model for Personalized Disease Prediction","type":"publication"},{"authors":["Anees Kazi","Shayan Shekarforoush","S Arvind Krishna","Hendrik Burwinkel","Gerome Vivar","Karsten Kortüm","Seyed-Ahmad Ahmadi","Shadi Albarqouni","Nassir Navab"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"5b8e4de2990f033d6bab149347f25fc4","permalink":"https://albarqouni.github.io/publication/kazi-2019-inceptiongcn/","publishdate":"2020-04-04T21:37:29.504879Z","relpermalink":"/publication/kazi-2019-inceptiongcn/","section":"publication","summary":"Geometric deep learning provides a principled and versatile manner for the integration of imaging and non-imaging modalities in the medical domain. Graph Convolutional Networks (GCNs) in particular have been explored on a wide variety of problems such as disease prediction, segmentation, and matrix completion by leveraging large, multimodal datasets. In this paper, we introduce a new spectral domain architecture for deep learning on graphs for disease prediction. The novelty lies in defining geometric 'inception modules' which are capable of capturing intra- and inter-graph structural heterogeneity during convolutions. We design filters with different kernel sizes to build our architecture. We show our disease prediction results on two publicly available datasets. Further, we provide insights on the behaviour of regular GCNs and our proposed model under varying input scenarios on simulated data.","tags":["Graph Convolutional Networks","Deep Learning","Medical Imaging","Radiology","Prior Knowledge"],"title":"InceptionGCN: receptive field aware graph convolutional network for disease prediction","type":"publication"},{"authors":["Mayar Lotfy","Raed M Shubair","Nassir Navab","Shadi Albarqouni"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"6bbea8ca7fb51b21cd206d1d77f9f1e5","permalink":"https://albarqouni.github.io/publication/lotfy-2019-investigation/","publishdate":"2020-04-04T21:37:29.71225Z","relpermalink":"/publication/lotfy-2019-investigation/","section":"publication","summary":"","tags":null,"title":"Investigation of Focal Loss in Deep Learning Models For Femur Fractures Classification","type":"publication"},{"authors":["Agnieszka Tomczack","Nassir Navab","Shadi Albarqouni"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"2fd9b803d4d55d7852522b6bd90c6805","permalink":"https://albarqouni.github.io/publication/tomczack-2019-learn/","publishdate":"2020-04-04T21:37:29.654672Z","relpermalink":"/publication/tomczack-2019-learn/","section":"publication","summary":"","tags":null,"title":"Learn to estimate labels uncertainty for quality assurance","type":"publication"},{"authors":["Abhijeet Parida","Arianne Tran","Nassir Navab","Shadi Albarqouni"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"06d66b02c7c7458a16d44f5c3401da4b","permalink":"https://albarqouni.github.io/publication/parida-2019-learn/","publishdate":"2020-04-04T21:37:29.647747Z","relpermalink":"/publication/parida-2019-learn/","section":"publication","summary":"Semantic segmentation is an import task in the medical field to identify the exact extent and orientation of significant structures like organs and pathology. Deep neural networks can perform this task well by leveraging the information from a large well-labeled data-set. This paper aims to present a method that mitigates the necessity of an extensive well-labeled data-set. This method also addresses semi-supervision by enabling segmentation based on bounding box annotations, avoiding the need for full pixel-level annotations. The network presented consists of a single U-Net based unbranched architecture that generates a few-shot segmentation for an unseen human organ using just 4 example annotations of that specific organ. The network is trained by alternately minimizing the nearest neighbor loss for prototype learning and a weighted cross-entropy loss for segmentation learning to perform a fast 3D segmentation with a median score of 54.64%.","tags":["Deep Learning","Medical Imaging","Powerful Representation","Meta Learning","Semi-Supervised Learning","Weakly-Supervised Learning"],"title":"Learn to Segment Organs with a Few Bounding Boxes","type":"publication"},{"authors":["Mhd Hasan Sarhan","Abouzar Eslami","Nassir Navab","Shadi Albarqouni"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"17a28539f532419622a433c1e9d4e4f3","permalink":"https://albarqouni.github.io/publication/sarhan-2019-learning/","publishdate":"2020-04-04T21:37:29.516031Z","relpermalink":"/publication/sarhan-2019-learning/","section":"publication","summary":"Learning Interpretable representation in medical applications is becoming essential for adopting data-driven models into clinical practice. It has been recently shown that learning a disentangled feature representation is important for a more compact and explainable representation of the data. In this paper, we introduce a novel adversarial variational autoencoder with a total correlation constraint to enforce independence on the latent representation while preserving the reconstruction fidelity. Our proposed method is validated on a publicly available dataset showing that the learned disentangled representation is not only interpretable, but also superior to the state-of-the-art methods. We report a relative improvement of 81.50% in terms of disentanglement, 11.60% in clustering, and 2% in supervised classification with a few amounts of labeled data.","tags":["Medical Imaging","Deep Learning","Interpretability","Generative Adversarial Networks","Powerful Representation"],"title":"Learning interpretable disentangled representations using adversarial vaes","type":"publication"},{"authors":["Ashkan Khakzar","Shadi Albarqouni","Nassir Navab"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"4e29ce65179b120805eabcabdabfb06a","permalink":"https://albarqouni.github.io/publication/khakzar-2019-learning/","publishdate":"2020-04-04T21:37:29.555255Z","relpermalink":"/publication/khakzar-2019-learning/","section":"publication","summary":"Neural networks are proven to be remarkably successful for classification and diagnosis in medical applications. However, the ambiguity in the decision-making process and the interpretability of the learned features is a matter of concern. In this work, we propose a method for improving the feature interpretability of neural network classifiers. Initially, we propose a baseline convolutional neural network with state of the art performance in terms of accuracy and weakly supervised localization. Subsequently, the loss is modified to integrate robustness to adversarial examples into the training process. In this work, feature interpretability is quantified via evaluating the weakly supervised localization using the ground truth bounding boxes. Interpretability is also visually assessed using class activation maps and saliency maps. The method is applied to NIH ChestX-ray14, the largest publicly available chest x-rays dataset. We demonstrate that the adversarially robust optimization paradigm improves feature interpretability both quantitatively and visually.","tags":["Medical Imaging","Deep Learning","Interpretability","Generative Adversarial Networks"],"title":"Learning Interpretable Features via Adversarially Robust Optimization","type":"publication"},{"authors":["Sai Gokul Hariharan","Christian Kaethner","Norbert Strobel","Markus Kowarschik","Shadi Albarqouni","Rebecca Fahrig","Nassir Navab"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"119eb5cdf9aa24ed08d1576a2fc84447","permalink":"https://albarqouni.github.io/publication/hariharan-2019-learning/","publishdate":"2020-04-04T21:37:29.668033Z","relpermalink":"/publication/hariharan-2019-learning/","section":"publication","summary":"","tags":["Radiology"],"title":"Learning-based x-ray image denoising utilizing model-based image simulations","type":"publication"},{"authors":["Hongen Liao","Simone Balocco","Guijin Wang","Feng Zhang","Yongpan Liu","Zijian Ding","Luc Duong","Renzo Phellan","Guillaume Zahnd","Katharina Breininger"," others"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"11f61c4e1c317f397254ed5f3b748129","permalink":"https://albarqouni.github.io/publication/liao-2019-machine/","publishdate":"2020-04-04T21:37:29.718265Z","relpermalink":"/publication/liao-2019-machine/","section":"publication","summary":"","tags":null,"title":"Machine Learning and Medical Engineering for Cardiovascular Health and Intravascular Imaging and Computer Assisted Stenting: First International Workshop, MLMECH 2019, and 8th Joint International Workshop, CVII-STENT 2019, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 13, 2019, Proceedings","type":"publication"},{"authors":["Bodo Kaiser","Shadi Albarqouni"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"10f64d18f662166f3065da495663536c","permalink":"https://albarqouni.github.io/publication/kaiser-2019-mri/","publishdate":"2020-04-04T21:37:29.463906Z","relpermalink":"/publication/kaiser-2019-mri/","section":"publication","summary":"","tags":null,"title":"MRI to CT Translation with GANs","type":"publication"},{"authors":["Mhd Hasan Sarhan","Shadi Albarqouni","Mehmet Yigitsoy","Nassir Navab","Abouzar Eslami"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"0a7cfa2ccd0bf1d93d941cdacb5055c8","permalink":"https://albarqouni.github.io/publication/sarhan-2019-multi/","publishdate":"2020-04-04T21:37:29.550326Z","relpermalink":"/publication/sarhan-2019-multi/","section":"publication","summary":"Deep learning techniques are recently being used in fundus image analysis and diabetic retinopathy detection. Microaneurysms are an important indicator of diabetic retinopathy progression. We introduce a two-stage deep learning approach for microaneurysms segmentation using multiple scales of the input with selective sampling and embedding triplet loss. The model first segments on two scales and then the segmentations are refined with a classification model. To enhance the discriminative power of the classification model, we incorporate triplet embedding loss with a selective sampling routine. The model is evaluated quantitatively to assess the segmentation performance and qualitatively to analyze the model predictions. This approach introduces a 30.29% relative improvement over the fully convolutional neural network.","tags":["Deep Learning","Medical Imaging","Ophthalmology","Powerful Representation","Prior Knowledge"],"title":"Multi-scale Microaneurysms Segmentation Using Embedding Triplet Loss","type":"publication"},{"authors":["Amal Lahiani","Nassir Navab","Shadi Albarqouni","Eldad Klaiman"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"cdc19e151d33dd5ccd1566f96482fb56","permalink":"https://albarqouni.github.io/publication/lahiani-2019-perceptual/","publishdate":"2020-04-04T21:37:29.705071Z","relpermalink":"/publication/lahiani-2019-perceptual/","section":"publication","summary":"Style transfer is a field with growing interest and use cases in deep learning. Recent work has shown Generative Adversarial Networks(GANs) can be used to create realistic images of virtually stained slide images in digital pathology with clinically validated interpretability. Digital pathology images are typically of extremely high resolution, making tilewise analysis necessary for deep learning applications. It has been shown that image generators with instance normalization can cause a tiling artifact when a large image is reconstructed from the tilewise analysis. We introduce a novel perceptual embedding consistency loss significantly reducing the tiling artifact created in the reconstructed whole slide image (WSI). We validate our results by comparing virtually stained slide images with consecutive real stained tissue slide images. We also demonstrate that our model is more robust to contrast, color and brightness perturbations by running comparative sensitivity analysis tests.","tags":["Stain Normalization","Histology","Powerful Representation","Domain Adaptation","Style Transfer"],"title":"Perceptual Embedding Consistency for Seamless Reconstruction of Tilewise Style Transfer","type":"publication"},{"authors":["Sai Gokul Hariharan","Christian Kaethner","Norbert Strobel","Markus Kowarschik","Julie DiNitto","Shadi Albarqouni","Rebecca Fahrig","Nassir Navab"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"5778560efafe3d7529f5597e72a6225b","permalink":"https://albarqouni.github.io/publication/hariharan-2019-preliminary/","publishdate":"2020-04-04T21:37:29.510413Z","relpermalink":"/publication/hariharan-2019-preliminary/","section":"publication","summary":"","tags":["Radiology"],"title":"Preliminary results of DSA denoising based on a weighted low-rank approach using an advanced neurovascular replication system","type":"publication"},{"authors":["Anees Kazi","Shayan Shekarforoush","Karsten Kortuem","Shadi Albarqouni","Nassir Navab"," others"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"38f4f0bf9861259d994d6e61ac206ca1","permalink":"https://albarqouni.github.io/publication/kazi-2019-self/","publishdate":"2020-04-04T21:37:29.457193Z","relpermalink":"/publication/kazi-2019-self/","section":"publication","summary":"Multi-modal data comprising imaging (MRI, fMRI, PET, etc.) and non-imaging (clinical test, demographics, etc.) data can be collected together and used for disease prediction. Such diverse data gives complementary information about the patient's condition to make an informed diagnosis. A model capable of leveraging the individuality of each multi-modal data is required for better disease prediction. We propose a graph convolution based deep model which takes into account the distinctiveness of each element of the multi-modal data. We incorporate a novel self-attention layer, which weights every element of the demographic data by exploring its relation to the underlying disease. We demonstrate the superiority of our developed technique in terms of computational speed and performance when compared to state-of-the-art methods. Our method outperforms other methods with a significant margin.","tags":["Graph Convolutional Networks","Deep Learning","Medical Imaging","Radiology","Prior Knowledge"],"title":"Self-attention equipped graph convolutions for disease prediction","type":"publication"},{"authors":["Ahmed Ayyad","Nassir Navab","Mohamed Elhoseiny","Shadi Albarqouni"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586703928,"objectID":"4320cc7fde18a3fdfac20e19e36df948","permalink":"https://albarqouni.github.io/publication/ayyad-2019-semi/","publishdate":"2020-04-04T21:37:29.499468Z","relpermalink":"/publication/ayyad-2019-semi/","section":"publication","summary":"Recent progress has shown that few-shot learning can be improved with access to unlabelled data, known as semi-supervised few-shot learning(SS-FSL). We introduce an SS-FSL approach, dubbed as Prototypical Random Walk Networks(PRWN), built on top of Prototypical Networks (PN). We develop a random walk semi-supervised loss that enables the network to learn representations that are compact and well-separated. Our work is related to the very recent development on graph-based approaches for few-shot learning. However, we show that compact and well-separated class representations can be achieved by modeling our prototypical random walk notion without needing additional graph-NN parameters or requiring a transductive setting where collective test set is provided. Our model outperforms prior art in most benchmarks with significant improvements in some cases. For example, in a mini-Imagenet 5-shot classification task, we obtain 69.65% accuracy to the 64.59% state-of-the-art. Our model, trained with 40% of the data as labelled, compares competitively against fully supervised prototypical networks, trained on 100% of the labels, even outperforming it in the 1-shot mini-Imagenet case with 50.89% to 49.4% accuracy. We also show that our model is resistant to distractors, unlabeled data that does not belong to any of the training classes, and hence reflecting robustness to labelled/unlabelled class distribution mismatch. We also performed a challenging discriminative power test, showing a relative improvement on top of the baseline of ≈14% on 20 classes on mini-Imagenet and ≈60% on 800 classes on Omniglot. Code will be made available.","tags":["Deep Learning","Powerful Representation","Meta Learning","Semi-Supervised Learning","Prior Knowledge"],"title":"Semi-Supervised Few-Shot Learning with Prototypical Random Walks","type":"publication"},{"authors":["M Tarek Shaban","Christoph Baur","Nassir Navab","Shadi Albarqouni"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"2880064c5944e9321b8c0c09fe811089","permalink":"https://albarqouni.github.io/xtarx.github.io/staingan/","publishdate":"2020-04-04T21:37:29.254779Z","relpermalink":"/xtarx.github.io/staingan/","section":"publication","summary":"Digitized Histological diagnosis is in increasing demand. However, color variations due to various factors are imposing obstacles to the diagnosis process. The problem of stain color variations is a well-defined problem with many proposed solutions. Most of these solutions are highly dependent on a reference template slide. We propose a deep-learning solution inspired by cycle consistency that is trained end-to-end, eliminating the need for an expert to pick a representative reference slide. Our approach showed superior results quantitatively and qualitatively against the state of the art methods. We further validated our method on a clinical use-case, namely Breast Cancer tumor classification, showing 16% increase in AUC","tags":["Stain Normalization","Generative Adversarial Networks","Histology","Domain Adaptation","Medical Imaging","Style Transfer"],"title":"Staingan: Stain style transfer for digital histological images","type":"publication"},{"authors":["Amelia Jiménez-Sánchez","Anees Kazi","Shadi Albarqouni","Chlodwig Kirchhoff","Peter Biberthaler","Nassir Navab","Diana Mateus","Sonja Kirchhoff"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"0e559961e3e8781d46dc712ecbbf2861","permalink":"https://albarqouni.github.io/publication/jimenez-2019-towards/","publishdate":"2020-04-04T21:37:29.494418Z","relpermalink":"/publication/jimenez-2019-towards/","section":"publication","summary":"We demonstrate the feasibility of a fully automatic computer-aided diagnosis (CAD) tool, based on deep learning, that localizes and classifies proximal femur fractures on X-ray images according to the AO classification. The proposed framework aims to improve patient treatment planning and provide support for the training of trauma surgeon residents. A database of 1347 clinical radiographic studies was collected. Radiologists and trauma surgeons annotated all fractures with bounding boxes, and provided a classification according to the AO standard. The proposed CAD tool for the classification of radiographs into types 'A', 'B' and 'not-fractured', reaches a F1-score of 87% and AUC of 0.95, when classifying fractures versus not-fractured cases it improves up to 94% and 0.98. Prior localization of the fracture results in an improvement with respect to full image classification. 100% of the predicted centers of the region of interest are contained in the manually provided bounding boxes. The system retrieves on average 9 relevant images (from the same class) out of 10 cases. Our CAD scheme localizes, detects and further classifies proximal femur fractures achieving results comparable to expert-level and state-of-the-art performance. Our auxiliary localization model was highly accurate predicting the region of interest in the radiograph. We further investigated several strategies of verification for its adoption into the daily clinical routine. A sensitivity analysis of the size of the ROI and image retrieval as a clinical use case were presented.","tags":["Deep Learning","Medical Imaging","Radiology","Weakly-Supervised Learning"],"title":"Towards an Interactive and Interpretable CAD System to Support Proximal Femur Fracture Classification","type":"publication"},{"authors":["Amal Lahiani","Jacob Gildenblat","Irina Klaman","Shadi Albarqouni","Nassir Navab","Eldad Klaiman"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"1fc0e9b4f92be0efede42f8b758a9538","permalink":"https://albarqouni.github.io/publication/lahiani-2019-virtualization/","publishdate":"2020-04-04T21:37:29.415809Z","relpermalink":"/publication/lahiani-2019-virtualization/","section":"publication","summary":"Histopathological evaluation of tissue samples is a key practice in patient diagnosis and drug development, especially in oncology. Historically, Hematoxylin and Eosin (H\u0026E) has been used by pathologists as a gold standard staining. However, in many cases, various target specific stains, including immunohistochemistry (IHC), are needed in order to highlight specific structures in the tissue. As tissue is scarce and staining procedures are tedious, it would be beneficial to generate images of stained tissue virtually. Virtual staining could also generate in-silico multiplexing of different stains on the same tissue segment. In this paper, we present a sample application that generates FAP-CK virtual IHC images from Ki67-CD8 real IHC images using an unsupervised deep learning approach based on CycleGAN. We also propose a method to deal with tiling artifacts caused by normalization layers and we validate our approach by comparing the results of tissue analysis algorithms for virtual and real images.","tags":["Stain Normalization","Histology","Powerful Representation","Domain Adaptation","Style Transfer"],"title":"Virtualization of tissue staining in digital pathology using an unsupervised deep learning approach","type":"publication"},{"authors":["Shadi Albarqouni","Mai Bui","Slobodan Ilic","Michael Schrapp"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"23f46dfd73f92fd943c4218a70c59207","permalink":"https://albarqouni.github.io/publication/albarqouni-2018-determination/","publishdate":"2020-04-04T21:37:29.810735Z","relpermalink":"/publication/albarqouni-2018-determination/","section":"publication","summary":"","tags":null,"title":"Determination of the pose of an x-ray unit relative to an object using a digital model of the object","type":"publication"},{"authors":[],"categories":null,"content":"","date":1536509100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"b71729093b69a83dc659bff31ec6e4c4","permalink":"https://albarqouni.github.io/talk/ecp2018/","publishdate":"2018-09-09T16:05:00Z","relpermalink":"/talk/ecp2018/","section":"talk","summary":"One of the major challenges facing researchers nowadays in applying deep learning (DL) models to Medical Image Analysis is the limited amount of annotated data. Collecting such ground-truth annotations requires domain knowledge (expertise), cost, and time, making it infeasible for large-scale databases. We presented a novel concept for training DL models from noisy annotations collected through crowdsourcing platforms, i.e., Amazon Mechanical Turk, Crowdflower, by introducing a robust aggregation layer to the convolutional neural networks. Our proposed method was validated on a publicly available database on Breast Cancer Histology Images showing interesting results of our robust aggregation method compared to baseline methods, i.e., Majority Voting. In follow-up work, we introduced a novel concept of an image to game-object translation in biomedical Imaging allowing medical images to be represented as star-shaped objects that can be easily embedded to readily available game canvas. The proposed method reduces the necessity of domain knowledge for annotations. Exciting and promising results were reported compared to the conventional crowdsourcing platforms.","tags":["Deep Learning","Histology","Crowdsourcing"],"title":"Invited Talk: Can Deep Learning Models be Trained with Annotations Collected via Crowdsourcing?","type":"talk"},{"authors":["Danail Stoyanov","Zeike Taylor","Simone Balocco","Raphael Sznitman","Anne Martel","Lena Maier-Hein","Luc Duong","Guillaume Zahnd","Stefanie Demirci","Shadi Albarqouni"," others"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608826906,"objectID":"03f2b56d7a8e8182b1d1c425f48fa4d9","permalink":"https://albarqouni.github.io/publication/stoyanov-20187-th/","publishdate":"2020-12-24T16:21:46.76303Z","relpermalink":"/publication/stoyanov-20187-th/","section":"publication","summary":"","tags":[],"title":"7th Joint International Workshop, CVII-STENT (Intravascular Imaging and Computer Assisted Stenting) 2018 and Third International Workshop, LABELS (Large-Scale Annotation of Biomedical Data and Expert Label Synthesis) 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Proceedings","type":"publication"},{"authors":["Sai Gokul Hariharan","Norbert Strobel","Christian Kaethner","Markus Kowarschik","Stefanie Demirci","Shadi Albarqouni","Rebecca Fahrig","Nassir Navab"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"85b6604f7cd31f63f10ff6b6a99cf553","permalink":"https://albarqouni.github.io/publication/hariharan-2018-photon/","publishdate":"2020-04-04T21:37:29.257418Z","relpermalink":"/publication/hariharan-2018-photon/","section":"publication","summary":"","tags":["Radiology"],"title":"A photon recycling approach to the denoising of ultra-low dose X-ray sequences","type":"publication"},{"authors":["Amelia Jiménez-Sánchez","Shadi Albarqouni","Diana Mateus"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"e5294dec4b66155b29bef19a5a915547","permalink":"https://albarqouni.github.io/publication/jimenez-2018-capsule/","publishdate":"2020-04-04T21:37:29.357861Z","relpermalink":"/publication/jimenez-2018-capsule/","section":"publication","summary":"A key component to the success of deep learning is the availability of massive amounts of training data. Building and annotating large datasets for solving medical image classification problems is today a bottleneck for many applications. Recently, capsule networks were proposed to deal with shortcomings of Convolutional Neural Networks (ConvNets). In this work, we compare the behavior of capsule networks against ConvNets under typical datasets constraints of medical image analysis, namely, small amounts of annotated data and class-imbalance. We evaluate our experiments on MNIST, Fashion-MNIST and medical (histological and retina images) publicly available datasets. Our results suggest that capsule networks can be trained with less amount of data for the same or better performance and are more robust to an imbalanced class distribution, which makes our approach very promising for the medical imaging community.","tags":["Deep Learning","Medical Imaging","Powerful Representation"],"title":"Capsule networks against medical imaging data challenges","type":"publication"},{"authors":["Christoph Baur","Benedikt Wiestler","Shadi Albarqouni","Nassir Navab"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"b1d25eddc07802287baaa41ee95f060b","permalink":"https://albarqouni.github.io/publication/baur-2018-deep/","publishdate":"2020-04-04T21:37:29.265269Z","relpermalink":"/publication/baur-2018-deep/","section":"publication","summary":"Reliably modeling normality and differentiating abnormal appearances from normal cases is a very appealing approach for detecting pathologies in medical images. A plethora of such unsupervised anomaly detection approaches has been made in the medical domain, based on statistical methods, content-based retrieval, clustering and recently also deep learning. Previous approaches towards deep unsupervised anomaly detection model patches of normal anatomy with variants of Autoencoders or GANs, and detect anomalies either as outliers in the learned feature space or from large reconstruction errors. In contrast to these patch-based approaches, we show that deep spatial autoencoding models can be efficiently used to capture normal anatomical variability of entire 2D brain MR images. A variety of experiments on real MR data containing MS lesions corroborates our hypothesis that we can detect and even delineate anomalies in brain MR images by simply comparing input images to their reconstruction. Results show that constraints on the latent space and adversarial training can further improve the segmentation performance over standard deep representation learning.","tags":["Deep Learning","Medical Imaging","Radiology","Generative Adversarial Networks","Anomaly Detection"],"title":"Deep autoencoding models for unsupervised anomaly segmentation in brain MR images","type":"publication"},{"authors":["Miguel Molina-Romero","Pedro A Gómez","Shadi Albarqouni","Jonathan I Sperl","Marion I Menzel","Bjoern H Menze"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"928992d4afa3353d23320abb869dde26","permalink":"https://albarqouni.github.io/publication/molina-2018-deep/","publishdate":"2020-04-04T21:37:29.393789Z","relpermalink":"/publication/molina-2018-deep/","section":"publication","summary":"","tags":null,"title":"Deep learning with synthetic data for free water elimination in diffusion MRI","type":"publication"},{"authors":["Markus A Degel","Nassir Navab","Shadi Albarqouni"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"687dbe5b6b4407c684333fa8b04adcc0","permalink":"https://albarqouni.github.io/publication/degel-2018-domain/","publishdate":"2020-04-04T21:37:29.298258Z","relpermalink":"/publication/degel-2018-domain/","section":"publication","summary":"Segmentation of the left atrium and deriving its size can help to predict and detect various cardiovascular conditions. Automation of this process in 3D Ultrasound image data is desirable, since manual delineations are time-consuming, challenging and observer-dependent. Convolutional neural networks have made improvements in computer vision and in medical image analysis. They have successfully been applied to segmentation tasks and were extended to work on volumetric data. In this paper we introduce a combined deep-learning based approach on volumetric segmentation in Ultrasound acquisitions with incorporation of prior knowledge about left atrial shape and imaging device. The results show, that including a shape prior helps the domain adaptation and the accuracy of segmentation is further increased with adversarial learning.","tags":["Ultrasound","Radiology","Domain Adaptation","Prior Knowledge"],"title":"Domain and geometry agnostic CNNs for left atrium segmentation in 3D ultrasound","type":"publication"},{"authors":["Shadi Albarqouni"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"1012a67081dbeb4e718d54d2aed3409b","permalink":"https://albarqouni.github.io/publication/albarqouni-2018-fine/","publishdate":"2020-04-04T21:37:29.40354Z","relpermalink":"/publication/albarqouni-2018-fine/","section":"publication","summary":"One of the major challenges currently facing researchers in applying deep learning (DL) models to medical image analysis is the limited amount of annotated data. Collecting such ground-truth annotations requires domain knowledge, cost, and time, making it infeasible for large-scale databases. Albarqouni et al. [S5] presented a novel concept for learning DL models from noisy annotations collected through crowdsourcing platforms (e.g., Amazon Mechanical Turk and Crowdflower) by introducing a robust aggregation layer to the convolutional neural networks (Figure S2). Their proposed method was validated on a publicly available database on breast cancer histology images, showing astonishing results of their robust aggregation method compared to the baseline of majority voting. In follow-up work, Albarqouni et al. [S6] introduced the novel concept of a translation from an image to a video game object for biomedical images. This technique allows medical images to be represented as star-shaped objects that can be easily embedded into a readily available game canvas. The proposed method reduces the necessity of domain knowledge for annotations. Exciting and promising results were reported compared to the conventional crowdsourcing platforms.","tags":["crowdsourcing"],"title":"Fine-Tuning Deep Learning by Crowd Participation","type":"publication"},{"authors":["Salome Kazeminia","Christoph Baur","Arjan Kuijper","Bram van Ginneken","Nassir Navab","Shadi Albarqouni","Anirban Mukhopadhyay"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"a23875ac0e4437cd2a79539025ec5123","permalink":"https://albarqouni.github.io/publication/kazeminia-2018-gans/","publishdate":"2020-04-04T21:37:29.398576Z","relpermalink":"/publication/kazeminia-2018-gans/","section":"publication","summary":"","tags":["Deep Learning","Medical Imaging","Generative Adversarial Networks"],"title":"GANs for medical image analysis","type":"publication"},{"authors":["Amal Lahiani","Jacob Gildenblat","Irina Klaman","Nassir Navab","Eldad Klaiman"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"5748235932078b5dc1d421870184351b","permalink":"https://albarqouni.github.io/publication/lahiani-2018-generalizing/","publishdate":"2020-04-04T21:37:29.616658Z","relpermalink":"/publication/lahiani-2018-generalizing/","section":"publication","summary":"","tags":["Histology","Medical Imaging"],"title":"Generalizing multistain immunohistochemistry tissue segmentation using one-shot color deconvolution deep neural networks","type":"publication"},{"authors":["Christoph Baur","Shadi Albarqouni","Nassir Navab"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"36f7c427f006f351db02313a8f7a820c","permalink":"https://albarqouni.github.io/publication/baur-2018-generating/","publishdate":"2020-04-04T21:37:29.363807Z","relpermalink":"/publication/baur-2018-generating/","section":"publication","summary":"As many other machine learning driven medical image analysis tasks, skin image analysis suffers from a chronic lack of labeled data and skewed class distributions, which poses problems for the training of robust and well-generalizing models. The ability to synthesize realistic looking images of skin lesions could act as a reliever for the aforementioned problems. Generative Adversarial Networks (GANs) have been successfully used to synthesize realistically looking medical images, however limited to low resolution, whereas machine learning models for challenging tasks such as skin lesion segmentation or classification benefit from much higher resolution data. In this work, we successfully synthesize realistically looking images of skin lesions with GANs at such high resolution. Therefore, we utilize the concept of progressive growing, which we both quantitatively and qualitatively compare to other GAN architectures such as the DCGAN and the LAPGAN. Our results show that with the help of progressive growing, we can synthesize highly realistic dermoscopic images of skin lesions that even expert dermatologists find hard to distinguish from real ones.","tags":["Medical Imaging","Deep Learning","Radiology","Generative Adversarial Networks","Anomaly Detection"],"title":"Generating highly realistic images of skin lesions with GANs","type":"publication"},{"authors":["Katharina Breininger","Shadi Albarqouni","Tanja Kurzendorfer","Marcus Pfister","Markus Kowarschik","Andreas Maier"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"cd9fb8b3349e660fc2f3121d7cc8602e","permalink":"https://albarqouni.github.io/publication/breininger-2018-intraoperative/","publishdate":"2020-04-04T21:37:29.30414Z","relpermalink":"/publication/breininger-2018-intraoperative/","section":"publication","summary":"","tags":["Medical Imaging","Deep Learning","Radiology"],"title":"Intraoperative stent segmentation in X-ray fluoroscopy for endovascular aortic repair","type":"publication"},{"authors":["Danail Stoyanov","Zeike Taylor","Simone Balocco","Raphael Sznitman","Anne Martel","Lena Maier-Hein","Luc Duong","Guillaume Zahnd","Stefanie Demirci","Shadi Albarqouni"," others"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"17943f418440a8a71b3de3fbc1681687","permalink":"https://albarqouni.github.io/publication/stoyanov-2018-intravascular/","publishdate":"2020-04-04T21:37:29.599255Z","relpermalink":"/publication/stoyanov-2018-intravascular/","section":"publication","summary":"","tags":null,"title":"Intravascular Imaging and Computer Assisted Stenting and Large-scale Annotation of Biomedical Data and Expert Label Synthesis: 7th Joint International Workshop, CVII-STENT 2018 and Third International Workshop, LABELS 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Proceedings","type":"publication"},{"authors":["Raphael Sznitman","Veronika Cheplygina","Diana Mateus","Lena Maier-Hein","Eric Granger","Pierre Jannin","Emanuele Trucco"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"3790755e1dd75012b3ac274da6100b0e","permalink":"https://albarqouni.github.io/publication/sznitman-2018-labels/","publishdate":"2020-04-04T21:37:29.864882Z","relpermalink":"/publication/sznitman-2018-labels/","section":"publication","summary":"","tags":null,"title":"LABELS 2018 Preface","type":"publication"},{"authors":["Katharina Breininger","Tobias Würfl","Tanja Kurzendorfer","Shadi Albarqouni","Marcus Pfister","Markus Kowarschik","Nassir Navab","Andreas Maier"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"3423026e99a7fa126ed2c1572565b207","permalink":"https://albarqouni.github.io/publication/breininger-2018-multiple/","publishdate":"2020-04-04T21:37:29.445602Z","relpermalink":"/publication/breininger-2018-multiple/","section":"publication","summary":"","tags":["Medical Imaging","Deep Learning","Radiology"],"title":"Multiple device segmentation for fluoroscopic imaging using multi-task learning","type":"publication"},{"authors":["Mai Bui","Shadi Albarqouni","Slobodan Ilic","Nassir Navab"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"3f035057e9c922f0dc3d51563eda893a","permalink":"https://albarqouni.github.io/publication/bui-2018-scene/","publishdate":"2020-04-04T21:37:29.345711Z","relpermalink":"/publication/bui-2018-scene/","section":"publication","summary":"Scene coordinate regression has become an essential part of current camera re-localization methods. Different versions, such as regression forests and deep learning methods, have been successfully applied to estimate the corresponding camera pose given a single input image. In this work, we propose to regress the scene coordinates pixel-wise for a given RGB image by using deep learning. Compared to the recent methods, which usually employ RANSAC to obtain a robust pose estimate from the established point correspondences, we propose to regress confidences of these correspondences, which allows us to immediately discard erroneous predictions and improve the initial pose estimates. Finally, the resulting confidences can be used to score initial pose hypothesis and aid in pose refinement, offering a generalized solution to solve this task.","tags":["Deep Learning","Uncertainty","Computer Vision","Pose Estimation"],"title":"Scene coordinate and correspondence learning for image-based localization","type":"publication"},{"authors":["Amelia Jiménez-Sánchez","Anees Kazi","Shadi Albarqouni","Sonja Kirchhoff","Alexandra Sträter","Peter Biberthaler","Diana Mateus","Nassir Navab"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"97fd81661faed7cb9dfec503ca4fb7ee","permalink":"https://albarqouni.github.io/publication/jimenez-2018-weakly/","publishdate":"2020-04-04T21:37:29.409418Z","relpermalink":"/publication/jimenez-2018-weakly/","section":"publication","summary":"","tags":["Deep Learning","Medical Imaging","Radiology","Weakly-Supervised Learning"],"title":"Weakly-supervised localization and classification of proximal femur fractures","type":"publication"},{"authors":["Mai Bui","Sergey Zakharov","Shadi Albarqouni","Slobodan Ilic","Nassir Navab"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"a3cb3053fa671c0660fe7dbe44087a9a","permalink":"https://albarqouni.github.io/publication/bui-2018-regression/","publishdate":"2020-04-04T21:37:29.310194Z","relpermalink":"/publication/bui-2018-regression/","section":"publication","summary":"In this work, we propose a method for object recognition and pose estimation from depth images using convolutional neural networks. Previous methods addressing this problem rely on manifold learning to learn low dimensional viewpoint descriptors and employ them in a nearest neighbor search on an estimated descriptor space. In comparison we create an efficient multi-task learning framework combining manifold descriptor learning and pose regression. By combining the strengths of manifold learning using triplet loss and pose regression, we could either estimate the pose directly reducing the complexity compared to NN search, or use learned descriptor for the NN descriptor matching. By in depth experimental evaluation of the novel loss function we observed that the view descriptors learned by the network are much more discriminative resulting in almost 30% increase regarding relative pose accuracy compared to related works. On the other hand, regarding directly regressed poses we obtained important improvement compared to simple pose regression. By leveraging the advantages of both manifold learning and regression tasks, we are able to improve the current state-of-the-art for object recognition and pose retrieval that we demonstrate through in depth experimental evaluation.","tags":["Computer Vision","Deep Learning","Pose Estimation","Powerful Representation","Prior Knowledge"],"title":"When regression meets manifold learning for object recognition and pose estimation","type":"publication"},{"authors":["Stefanie Demirci","Shadi Albarqouni","Javad Fotouhi","Nassir Navab","Reza Ghotbi"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"4b70f02f21a1c50c9bb020d7533a0359","permalink":"https://albarqouni.github.io/publication/demirci-2018-x/","publishdate":"2020-04-04T21:37:29.452009Z","relpermalink":"/publication/demirci-2018-x/","section":"publication","summary":"","tags":null,"title":"X-ray Depthmaps: Revealing the Hidden Structures","type":"publication"},{"authors":["Anees Kazi","Shadi Albarqouni","Amelia Jimenez Sanchez","Sonja Kirchhoff","Peter Biberthaler","Nassir Navab","Diana Mateus"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"32bc9aaa85ff9cebe12b10078e7f8b33","permalink":"https://albarqouni.github.io/publication/kazi-2017-automatic/","publishdate":"2020-04-04T21:37:29.201741Z","relpermalink":"/publication/kazi-2017-automatic/","section":"publication","summary":"","tags":null,"title":"Automatic classification of proximal femur fractures based on attention models","type":"publication"},{"authors":["Babak Ehteshami Bejnordi","Mitko Veta","Paul Johannes Van Diest","Bram Van Ginneken","Nico Karssemeijer","Geert Litjens","Jeroen AWM Van Der Laak","Meyke Hermsen","Quirine F Manson","Maschenka Balkenhol"," others"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"98aae4834c0d8a194aa4a5a54b298821","permalink":"https://albarqouni.github.io/publication/bejnordi-2017-diagnostic/","publishdate":"2020-04-04T21:37:29.21771Z","relpermalink":"/publication/bejnordi-2017-diagnostic/","section":"publication","summary":"","tags":["Deep Learning","Medical Imaging","Histology"],"title":"Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer","type":"publication"},{"authors":["M Jorge Cardoso","Tal Arbel","SL Lee","V Cheplygina","S Balocco","D Mateus","G Zahnd","L Maier-Hein","S Demirci","E Granger"," others"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"16ef2909b2ae7311e47aa67ba3d390c4","permalink":"https://albarqouni.github.io/publication/cardoso-2017-intravascular/","publishdate":"2020-04-04T21:37:29.210348Z","relpermalink":"/publication/cardoso-2017-intravascular/","section":"publication","summary":"","tags":null,"title":"Intravascular Imaging and Computer Assisted Stenting, and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis","type":"publication"},{"authors":["Shadi Albarqouni"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"b94e7629ccfb9649104eeb2f13240e37","permalink":"https://albarqouni.github.io/publication/albarqouni-2017-machine/","publishdate":"2020-04-04T21:37:29.316185Z","relpermalink":"/publication/albarqouni-2017-machine/","section":"publication","summary":"","tags":null,"title":"Machine Learning for Biomedical Applications: From Crowdsourcing to Deep Learning","type":"publication"},{"authors":["Christoph Baur","Shadi Albarqouni","Nassir Navab"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"ad46151244f23e1dba7068e7d0bb29ec","permalink":"https://albarqouni.github.io/publication/baur-2017-semi/","publishdate":"2020-04-04T21:37:29.163621Z","relpermalink":"/publication/baur-2017-semi/","section":"publication","summary":"Deep learning usually requires large amounts of labeled training data, but annotating data is costly and tedious. The framework of semi-supervised learning provides the means to use both labeled data and arbitrary amounts of unlabeled data for training. Recently, semi-supervised deep learning has been intensively studied for standard CNN architectures. However, Fully Convolutional Networks (FCNs) set the state-of-the-art for many image segmentation tasks. To the best of our knowledge, there is no existing semi-supervised learning method for such FCNs yet. We lift the concept of auxiliary manifold embedding for semi-supervised learning to FCNs with the help of Random Feature Embedding. In our experiments on the challenging task of MS Lesion Segmentation, we leverage the proposed framework for the purpose of domain adaptation and report substantial improvements over the baseline model.","tags":["Radiology","Medical Imaging","Deep Learning","Semi-Supervised Learning","Domain Adaptation"],"title":"Semi-supervised deep learning for fully convolutional networks","type":"publication"},{"authors":["Shadi Albarqouni","Javad Fotouhi","Nassir Navab"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"31f4b55ea53e4d37ebd0505ab79b883b","permalink":"https://albarqouni.github.io/publication/albarqouni-2017-x/","publishdate":"2020-04-04T21:37:29.169729Z","relpermalink":"/publication/albarqouni-2017-x/","section":"publication","summary":"X-ray is the most readily available imaging modality and has a broad range of applications that spans from diagnosis to intra-operative guidance in cardiac, orthopedics, and trauma procedures. Proper interpretation of the hidden and obscured anatomy in X-ray images remains a challenge and often requires high radiation dose and imaging from several perspectives. In this work, we aim at decomposing the conventional X-ray image into d X-ray components of independent, non-overlapped, clipped sub-volume, that separate rigid structures into distinct layers, leaving all deformable organs in one layer, such that the sum resembles the original input. Our proposed model is validaed on 6 clinical datasets (∼7200 X-ray images) in addition to 615 real chest X-ray images. Despite the challenging aspects of modeling such a highly ill-posed problem, exciting and encouraging results are obtained paving the path for further contributions in this direction.","tags":["Radiology","Medical Imaging","Deep Learning"],"title":"X-ray in-depth decomposition: Revealing the latent structures","type":"publication"},{"authors":["Mai Bui","Shadi Albarqouni","Michael Schrapp","Nassir Navab","Slobodan Ilic"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"96433a86c37f544389b37898d9188c18","permalink":"https://albarqouni.github.io/publication/bui-2017-x/","publishdate":"2020-04-04T21:37:29.157697Z","relpermalink":"/publication/bui-2017-x/","section":"publication","summary":"Precise reconstruction of 3D volumes from X-ray projections requires precisely pre-calibrated systems where accurate knowledge of the systems geometric parameters is known ahead. However, when dealing with mobile X-ray devices such calibration parameters are unknown. Joint estimation of the systems calibration parameters and 3d reconstruction is a heavily unconstrained problem, especially when the projections are arbitrary. In industrial applications, that we target here, nominal CAD models of the object to be reconstructed are usually available. We rely on this prior information and employ Deep Learning to learn the mapping between simulated X-ray projections and its pose. Moreover, we introduce the reconstruction loss in addition to the pose loss to further improve the reconstruction quality. Finally, we demonstrate the generalization capabilities of our method in case where poses can be learned on instances of the objects belonging to the same class, allowing pose estimation of unseen objects from the same category, thus eliminating the need for the actual CAD model. We performed exhaustive evaluation demonstrating the quality of our results on both synthetic and real data.","tags":["Computer Vision","Radiology","Deep Learning","Pose Estimation"],"title":"X-Ray PoseNet: 6 DoF pose estimation for mobile X-Ray devices","type":"publication"},{"authors":["Shadi Albarqouni"],"categories":["Demo"],"content":"Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 widgets, themes, and language packs included!\n Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\n 👉 Get Started 📚 View the documentation 💬 Ask a question on the forum 👥 Chat with the community 🐦 Twitter: @source_themes @GeorgeCushen #MadeWithAcademic 💡 Request a feature or report a bug ⬆️ Updating? View the Update Guide and Release Notes ❤️ Support development of Academic:  ☕️ Donate a coffee 💵 Become a backer on Patreon 🖼️ Decorate your laptop or journal with an Academic sticker 👕 Wear the T-shirt 👩‍💻 Contribute      Academic is mobile first with a responsive design to ensure that your site looks stunning on every device.   Key features:\n Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 15+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Academic comes with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the sun/moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\n Choose a stunning theme and font for your site. Themes are fully customizable.\nEcosystem   Academic Admin: An admin tool to import publications from BibTeX or import assets for an offline site  Academic Scripts: Scripts to help migrate content to new versions of Academic  Install You can choose from one of the following four methods to install:\n  one-click install using your web browser (recommended)  install on your computer using Git with the Command Prompt/Terminal app  install on your computer by downloading the ZIP files  install on your computer with RStudio  Then personalize and deploy your new site.\nUpdating  View the Update Guide.\nFeel free to star the project on Github to help keep track of updates.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://albarqouni.github.io/post/getting-started/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Create a beautifully simple website in under 10 minutes.","tags":null,"title":"Academic: the website builder for Hugo","type":"post"},{"authors":["Shadi Albarqouni"],"categories":null,"content":"","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"630b0236cbe2f2c4d021dd025f4b03f8","permalink":"https://albarqouni.github.io/codes/anomaly-detection/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/codes/anomaly-detection/","section":"codes","summary":"Implemntation of our comparative study on anomaly detection","tags":["Anomaly Detection"],"title":"Anomaly Detection","type":"codes"},{"authors":["Shadi Albarqouni"],"categories":null,"content":"","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"788d1ff24d9839978c27e8e95f6c9e5c","permalink":"https://albarqouni.github.io/codes/capsule/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/codes/capsule/","section":"codes","summary":"MICCAI2018 Workshop: Capsule Networks against Medical Imaging Data Challenges","tags":["Capsule Networks"],"title":"Capsule Networks","type":"codes"},{"authors":["Shadi Albarqouni"],"categories":null,"content":"","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"b612b63604b046d5116a05b986faa1be","permalink":"https://albarqouni.github.io/codes/polyp-detection/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/codes/polyp-detection/","section":"codes","summary":"Our Paper: A learning without forgetting approach to incorporate artifact knowledge in polyp localization tasks","tags":["Endoscopy"],"title":"Polyp and Artifact Detector","type":"codes"},{"authors":["Shadi Albarqouni"],"categories":null,"content":"","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587921960,"objectID":"65ac80aebd59d2b3a212949e73373de6","permalink":"https://albarqouni.github.io/codes/roam/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/codes/roam/","section":"codes","summary":"Implemntation of our recent paper on Whole Brain Segmentation and COVID-19 CT Lung Segmentation using RandOm lAyer Mixup in Semi-Supervised Learning","tags":["Semi-Supervised Learning","Radiology"],"title":"ROAM","type":"codes"},{"authors":["Shadi Albarqouni"],"categories":null,"content":"StainGAN StainGAN implementation based on Cycle-Consistency Concept\nFor more information visit website.\nStructure  Stain-Transfer Model Pre-processing. Post-processing. Evaluation  Datasets The evaluation was done using the Camelyon16 challenge (https://camelyon16.grand-challenge.org/) consisting of 400 whole-slide images collected in two different labs in Radboud University Medical Center (lab 1) and University Medical Center Utrecht (lab 2). Otsu thresholding was used to remove the background, Afterwards, 40, 000 256 × 256 patches were generated on the x40 magnification level, 30, 000 were used for training and 10, 000 used for validation from lab 1 and 10, 000 patches were generated for testing from lab 2.\nPatches can be found here: https://campowncloud.in.tum.de/index.php/s/iGgQ9vdHiMZsFJB?path=%2FStainGAN_camelyon16\nAny use of the dataset or anypart of the code should be cited\nCitation If you use this code for your research, please cite our papers.\n@inproceedings{shaban2019staingan, author = {Shaban, M Tarek and Baur, Christoph and Navab, Nassir and Albarqouni, Shadi}, booktitle = {2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)}, organization = {IEEE}, pages = {953--956}, title = {Staingan: Stain style transfer for digital histological images}, year = {2019} }  Acknowledgments Code is inspired by pytorch-DCGAN and CycleGAN.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"2cd1849580c0d55965ffd14070aae73f","permalink":"https://albarqouni.github.io/codes/stain-normalization/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/codes/stain-normalization/","section":"codes","summary":"ISBI2019: StainGAN: Stain Style Transfer for Digital Histological Images","tags":["Stain Normalization"],"title":"Stain Normlization","type":"codes"},{"authors":["Shadi Albarqouni","Christoph Baur","Felix Achilles","Vasileios Belagiannis","Stefanie Demirci","Nassir Navab"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"c8ee1346ef099972c45e45c167a95036","permalink":"https://albarqouni.github.io/publication/albarqouni-2016-aggnet/","publishdate":"2020-12-24T16:17:38.965198Z","relpermalink":"/publication/albarqouni-2016-aggnet/","section":"publication","summary":"","tags":["Crowdsourcing","Histology","Medical Imaging"],"title":"Aggnet: deep learning from crowds for mitosis detection in breast cancer histology images","type":"publication"},{"authors":["Christoph Baur","Shadi Albarqouni","Stefanie Demirci","Nassir Navab","Pascal Fallavollita"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"05f341b7f8b139a89cc1a7c2031abfd8","permalink":"https://albarqouni.github.io/publication/baur-2016-cathnets/","publishdate":"2020-12-24T16:17:39.502042Z","relpermalink":"/publication/baur-2016-cathnets/","section":"publication","summary":"","tags":["Radiology","Medical Imaging","Deep Learning","Sparse Coding","Dictionary Learning"],"title":"Cathnets: detection and single-view depth prediction of catheter electrodes","type":"publication"},{"authors":["Gustavo Carneiro","Diana Mateus","Loı̈c Peter","Andrew Bradley","João Manuel RS Tavares","Vasileios Belagiannis","João Paulo Papa","Jacinto C Nascimento","Marco Loog","Zhi Lu"," others"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"3ce4d9fc03592b11c4b05a0a7ecfe122","permalink":"https://albarqouni.github.io/publication/carneiro-2016-deep/","publishdate":"2020-04-04T21:37:29.857376Z","relpermalink":"/publication/carneiro-2016-deep/","section":"publication","summary":"","tags":null,"title":"Deep Learning and Data Labeling for Medical Applications: First International Workshop, LABELS 2016, and Second International Workshop, DLMIA 2016, Held in Conjunction with MICCAI 2016, Athens, Greece, October 21, 2016, Proceedings","type":"publication"},{"authors":["Vasileios Belagiannis","Xinchao Wang","Horesh Beny Ben Shitrit","Kiyoshi Hashimoto","Ralf Stauder","Yoshimitsu Aoki","Michael Kranzfelder","Armin Schneider","Pascal Fua","Slobodan Ilic"," others"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"391adf70ec74058be6db240f055c4e45","permalink":"https://albarqouni.github.io/publication/belagiannis-2016-parsing/","publishdate":"2020-12-24T16:17:39.397048Z","relpermalink":"/publication/belagiannis-2016-parsing/","section":"publication","summary":"","tags":null,"title":"Parsing human skeletons in an operating room","type":"publication"},{"authors":["Shadi Albarqouni","Stefan Matl","Maximilian Baust","Nassir Navab","Stefanie Demirci"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"307fa42d2f1408661391c70d12a76dde","permalink":"https://albarqouni.github.io/publication/albarqouni-2016-playsourcing/","publishdate":"2020-12-24T16:17:40.071746Z","relpermalink":"/publication/albarqouni-2016-playsourcing/","section":"publication","summary":"","tags":["Crowdsourcing","Gamification"],"title":"Playsourcing: a novel concept for knowledge creation in biomedical research","type":"publication"},{"authors":["Shadi Albarqouni","Ulrich Konrad","Lichao Wang","Nassir Navab","Stefanie Demirci"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"e31eb8bfc8831f46cc7cdcd84596a935","permalink":"https://albarqouni.github.io/publication/albarqouni-2016-single/","publishdate":"2020-12-24T16:17:39.189639Z","relpermalink":"/publication/albarqouni-2016-single/","section":"publication","summary":"","tags":["Radiology","Sparse Coding","Dictionary Learning"],"title":"Single-view X-ray depth recovery: toward a novel concept for image-guided interventions","type":"publication"},{"authors":["Abhishek Vahadane","Tingying Peng","Amit Sethi","Shadi Albarqouni","Lichao Wang","Maximilian Baust","Katja Steiger","Anna Melissa Schlitter","Irene Esposito","Nassir Navab"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"22f7e1d861e9c8942c95fe5ef925b037","permalink":"https://albarqouni.github.io/publication/vahadane-2016-structure/","publishdate":"2020-12-24T16:17:39.947488Z","relpermalink":"/publication/vahadane-2016-structure/","section":"publication","summary":"","tags":["Histology","Medical Imaging"],"title":"Structure-preserving color normalization and sparse stain separation for histological images","type":"publication"},{"authors":["Shadi Albarqouni","Tobias Lasser","Weaam Alkhaldi","Ashraf Al-Amoudi","Nassir Navab"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"77eff784778dec6bb36892705c2ed283","permalink":"https://albarqouni.github.io/publication/albarqouni-2015-gradient/","publishdate":"2020-12-24T16:17:39.836086Z","relpermalink":"/publication/albarqouni-2015-gradient/","section":"publication","summary":"","tags":null,"title":"Gradient projection for regularized cryo-electron tomographic reconstruction","type":"publication"},{"authors":["S Albarqouni","M Baust","S Conjeti","A Al-Amoudi","N Navab"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"0b0b505694d0f3fac4d1e1a0e6c5d9ff","permalink":"https://albarqouni.github.io/publication/albarqouni-2015-multi/","publishdate":"2020-12-24T16:17:39.729751Z","relpermalink":"/publication/albarqouni-2015-multi/","section":"publication","summary":"","tags":["Prior Knowledge","Electron Microscopic Imaging"],"title":"Multi-scale Graph-based Guided Filter for De-noising Cryo-Electron Tomographic Data","type":"publication"},{"authors":["Abhishek Vahadane","Tingying Peng","Shadi Albarqouni","Maximilian Baust","Katja Steiger","Anna Melissa Schlitter","Amit Sethi","Irene Esposito","Nassir Navab"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"f92e2db4e477b6623d9de8904caf8616","permalink":"https://albarqouni.github.io/publication/vahadane-2015-structure/","publishdate":"2020-12-24T16:17:38.804117Z","relpermalink":"/publication/vahadane-2015-structure/","section":"publication","summary":"","tags":["Stain Normalization","Histology","Medical Imaging"],"title":"Structure-preserved color normalization for histological images","type":"publication"},{"authors":["Mohammed T Hussein","Shadi N Albarqouni"],"categories":[],"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608826659,"objectID":"d2ec842514751ef0c7fd12df789c17ad","permalink":"https://albarqouni.github.io/publication/hussein-2013-developing/","publishdate":"2020-12-24T16:17:39.615676Z","relpermalink":"/publication/hussein-2013-developing/","section":"publication","summary":"","tags":[],"title":"Developing MATLAB software for PV and battery sizing for lighting projects in Gaza Strip, Palestine","type":"publication"},{"authors":["Mohammed T Hussein","Shadi N Albarqouni"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"aae291970681aeeb0a74e8db18cd56c2","permalink":"https://albarqouni.github.io/publication/hussein-2012-developing/","publishdate":"2020-04-04T21:37:29.098911Z","relpermalink":"/publication/hussein-2012-developing/","section":"publication","summary":"","tags":null,"title":"Developing MATLAB software for PV and battery sizing for lighting projects in Gaza Strip, Palestine","type":"publication"},{"authors":["Mohammed Hussein","Shadi Albarqouni"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"2307734188255fad63754b28e6dde182","permalink":"https://albarqouni.github.io/publication/hussein-2010-developing/","publishdate":"2020-12-24T16:17:39.085146Z","relpermalink":"/publication/hussein-2010-developing/","section":"publication","summary":"","tags":null,"title":"Developing empirical models for estimating global solar radiation in Gaza Strip, Palestine","type":"publication"},{"authors":["Shadi N AlBarqouni"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"4bdcf69c5addf0c01be495e2dbc1f7b1","permalink":"https://albarqouni.github.io/publication/albarqouni-2010-re/","publishdate":"2020-12-24T16:17:40.240644Z","relpermalink":"/publication/albarqouni-2010-re/","section":"publication","summary":"","tags":null,"title":"Re-Evaluation and Re-Design Stand-Alone PV Solar Lighting Projects in Gaza","type":"publication"},{"authors":["Mohammed T Hussein","Shadi N Albarqouni"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"fcd5830d68378987ce8b08b8ffe95a50","permalink":"https://albarqouni.github.io/publication/hussein-2010-re/","publishdate":"2020-04-04T21:37:29.352401Z","relpermalink":"/publication/hussein-2010-re/","section":"publication","summary":"","tags":null,"title":"Re-evaluation and re-design stand-alone PV solar lighting projects in Gaza Strip, Palestine","type":"publication"},{"authors":["Mohammed Hussein","Shadi Albarqouni"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"31432095a29655ce628c571182f74818","permalink":"https://albarqouni.github.io/publication/hussein-2010-study/","publishdate":"2020-12-24T16:17:39.295363Z","relpermalink":"/publication/hussein-2010-study/","section":"publication","summary":"","tags":null,"title":"Study on the Optimum Tilt Angle and Orientation for Photovoltaic Panels and Feasibility Study of One axis-two positions tracking Solar PV in Palestine","type":"publication"},{"authors":["Shadi Albarqouni"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586645150,"objectID":"7552e787f48276386d802bccd35b0af8","permalink":"https://albarqouni.github.io/publication/albarqouni-2009-steps/","publishdate":"2020-04-04T21:37:29.766079Z","relpermalink":"/publication/albarqouni-2009-steps/","section":"publication","summary":"","tags":["Telemedicine","Medical Imaging","Histology","Endoscopy"],"title":"Steps towards Establishing Telemedicine Center in Palestine","type":"publication"}]