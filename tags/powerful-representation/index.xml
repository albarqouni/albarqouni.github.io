<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Powerful Representation | Shadi Albarqouni</title>
    <link>https://albarqouni.github.io/tags/powerful-representation/</link>
      <atom:link href="https://albarqouni.github.io/tags/powerful-representation/index.xml" rel="self" type="application/rss+xml" />
    <description>Powerful Representation</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â©Shadi Albarqouni 2022</copyright><lastBuildDate>Sat, 01 Aug 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://albarqouni.github.io/images/icon_hu681563d5489cbd6b4a3381f376416aae_52869_512x512_fill_lanczos_center_3.png</url>
      <title>Powerful Representation</title>
      <link>https://albarqouni.github.io/tags/powerful-representation/</link>
    </image>
    
    <item>
      <title>Fairness by Learning Orthogonal Disentangled Representations</title>
      <link>https://albarqouni.github.io/publication/sarhan-2020-fairness/</link>
      <pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/sarhan-2020-fairness/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learn to Adapt</title>
      <link>https://albarqouni.github.io/project/learn-to-adapt/</link>
      <pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/project/learn-to-adapt/</guid>
      <description>&lt;p&gt;To build domain-agnostic models that are generalizable to a different domain, i.e., scanners, we have investigated three directions; First, &lt;em&gt;Style Transfer&lt;/em&gt;, where the style/color of the source domain is transferred to match the target one.  Such style transfer is performed in the high-dimensional image space using adversarial learning, as shown in our papers on Histology Imaging (Lahiani &lt;em&gt;et al.&lt;/em&gt; 2019a, Lahiani &lt;em&gt;et al.&lt;/em&gt; 2019b, Shaban &lt;em&gt;et al.&lt;/em&gt; 2019). Second, &lt;em&gt;Domain Adaptation&lt;/em&gt;, where the distance between the features of the source and target domains are minimized. Such distance can be optimized in a supervised fashion, i.e., class aware, using angular cosine distance as shown in our paper on MS Lesion Segmentation in MR Imaging (Baur &lt;em&gt;et al.&lt;/em&gt; 2017), or in an unsupervised way, i.e., class agnostic, using adversarial learning as explained in our article on Left atrium Segmentation in Ultrasound Imaging (Degel &lt;em&gt;et al.&lt;/em&gt; 2018). Yet, another exciting direction that has been recently investigated in our paper (Lahiani &lt;em&gt;et al.&lt;/em&gt; 2019c) is to disentangle the feature that is responsible for the style and color from the one responsible for the semantics.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Baur_Degel_Shaban.jpeg&#34; alt=&#34;Baur et al. 2017, Degel et al. 2018, and Shaban et al. 2019&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;lahiani2019c.jpeg&#34; alt=&#34;Lahiani et al. 2019c&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;collaboration&#34;&gt;Collaboration:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Eldad Klaiman, Roche Diagnostics GmbH&lt;/li&gt;
&lt;li&gt;Georg Schummers and Matthias Friedrichs, TOMTEC Imaging Systems GmbH&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Learn to Learn</title>
      <link>https://albarqouni.github.io/project/learn-to-learn/</link>
      <pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/project/learn-to-learn/</guid>
      <description>&lt;p&gt;To build models that are transferable to different tasks or different data distributions, i.e., non i.i.d., we have investigated meta-learning approaches such as prototypical networks (PN) (Snell &lt;em&gt;et al.&lt;/em&gt; 2017). PN learns a class prototype from very few amounts of labeled data, e.g., 1-5 shots, and use the learned prototypes to perform the classification tasks. In the context of medical imaging, we were first to introduce Few-Shot Learning into the MIC community. We have shown in our recent ICML Workshop paper (Ayyad &lt;em&gt;et al.&lt;/em&gt; 2019) that our novel Semi-Supervised Few-Shot Learning achieves the state-of-the-art on benchmark datasets; Omniglot, miniImageNet, and TieredImageNet. Further, we have demonstrated in our recent paper (Parida &lt;em&gt;et al.&lt;/em&gt; 2019) that such concepts can be utilized in medical imaging segmentation with an extremely low budget of annotated data, e.g., bounding boxes, and better generalization capability, i.e., to new organs or anomalies, however, at the cost of less accurate segmentation. Yet, our proposed models have great potential in clinical practice where a novel application could come in, and only a very few annotations are required, to perform segmentation tasks. Further, such a learning paradigm has a great potential in Federated Learning, where the data acquired at different hospitals capture heterogeneous and non i.i.d data, i.e., various tasks, making proposed models suitable for such a problem.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Parida2019.jpeg&#34; alt=&#34;Parida et al. 2019&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;collaboration&#34;&gt;Collaboration:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Prof. 
&lt;a href=&#34;https://www.kaust.edu.sa/en/study/faculty/mohamed-elhoseiny&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mohamed Elhoseiny&lt;/a&gt;, 
&lt;a href=&#34;https://ai.facebook.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Facebook AI Research&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Learn to Reason and Explain</title>
      <link>https://albarqouni.github.io/project/learn-to-reason-and-explain/</link>
      <pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/project/learn-to-reason-and-explain/</guid>
      <description>&lt;p&gt;To build explainable AI models that are interpretable for our end-users, i.e., clinicians, we have investigated two research directions. First, we have utilized some visualization techniques to explain and interpret &amp;ldquo;black box&amp;rdquo; models by propagating back the gradient of the class of interest to the image space where you can see the relevant semantics, so-called Gradient Class Activation Maps (GradCAM). Sooner, we found out such techniques do not produce meaningful results. In other words, irrelevant semantics could be highly activated in GradCAM, yielding unreliable explanation tools. To overcome such a problem, we have introduced a robust optimization loss in our MICCAI paper (Khakzar &lt;em&gt;et al.&lt;/em&gt; 2019), which generated adversarial examples enforcing the network to only focus on relevant features and probably correlated with other examples belonging to the same class.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Khakzar2019.jpeg&#34; alt=&#34;Khakzar2019&#34;&gt;&lt;/p&gt;
&lt;p&gt;Second, we have investigated designing and building explainable models by i) uncertainty quantification and ii) disentangled feature representation. In the first category, we started understanding the uncertainty estimates generated by Monte-Carlo Dropout, the approximate of Bayesian Neural Networks, and other techniques, e.g. PointNet, in Camera Relocalization problem (Bui &lt;em&gt;et al.&lt;/em&gt; 2018), to shed light on the ambiguity present in the dataset. We took a step further, and use such uncertainty estimates to refine the segmentation in an unsupervised fashion (Soberanis-Mukul &lt;em&gt;et al.&lt;/em&gt; 2019, Bui &lt;em&gt;et al.&lt;/em&gt; 2019).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Sarhan2019.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Recently, we have investigated modeling the labels uncertainty, which is related to the inter-/intra-observer variability, and produced a metric to quantify such uncertainty. We have shown in our paper (Tomczack &lt;em&gt;et al.&lt;/em&gt; 2019) that such uncertainty can be rather disentangled from the model and data uncertainties, so-called, epistemic, and aleatoric uncertainties, respectively. We believe such uncertainty is of high importance to the referral systems. In the second category, we have studied the variational methods, and disentangled representations, where the assumption here that some generative factors, &lt;em&gt;e.g.&lt;/em&gt;, color, shape, and pathology, will be captured in the lower-dimensional latent space, and one can easily go through the manifold and generate tons of example by sampling from the posterior distribution. We were among the firsts who introduce such concepts in medical imaging by investigating the influence of residual blocks and adversarial learning on disentangled representation (Sarhan &lt;em&gt;et al.&lt;/em&gt; 2019). Our hypothesis that better reconstruction fidelity would force the network to model high resolution, which might have a positive influence on the disentangled representation, in particular, some pathologies.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Roger_Tomczack2019.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;collaboration&#34;&gt;Collaboration:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Dr. 
&lt;a href=&#34;https://scholar.google.de/citations?user=PmHOyT0AAAAJ&amp;amp;hl=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abouzar Eslami&lt;/a&gt;, Carl Zeiss Meditec AG&lt;/li&gt;
&lt;li&gt;PD. Dr. 
&lt;a href=&#34;https://scholar.google.de/citations?user=ELOVd8sAAAAJ&amp;amp;hl=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slobodan Ilic&lt;/a&gt;, Siemens AG&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;funding&#34;&gt;Funding:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Siemens AG&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Seamless Virtual Whole Slide Image Synthesis and Validation Using Perceptual Embedding Consistency</title>
      <link>https://albarqouni.github.io/publication/lahiani-2020-seamless/</link>
      <pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/lahiani-2020-seamless/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learn to Segment Organs with a Few Bounding Boxes</title>
      <link>https://albarqouni.github.io/publication/parida-2019-learn/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/parida-2019-learn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning interpretable disentangled representations using adversarial vaes</title>
      <link>https://albarqouni.github.io/publication/sarhan-2019-learning/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/sarhan-2019-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multi-scale Microaneurysms Segmentation Using Embedding Triplet Loss</title>
      <link>https://albarqouni.github.io/publication/sarhan-2019-multi/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/sarhan-2019-multi/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Perceptual Embedding Consistency for Seamless Reconstruction of Tilewise Style Transfer</title>
      <link>https://albarqouni.github.io/publication/lahiani-2019-perceptual/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/lahiani-2019-perceptual/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Semi-Supervised Few-Shot Learning with Prototypical Random Walks</title>
      <link>https://albarqouni.github.io/publication/ayyad-2019-semi/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/ayyad-2019-semi/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Virtualization of tissue staining in digital pathology using an unsupervised deep learning approach</title>
      <link>https://albarqouni.github.io/publication/lahiani-2019-virtualization/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/lahiani-2019-virtualization/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Capsule networks against medical imaging data challenges</title>
      <link>https://albarqouni.github.io/publication/jimenez-2018-capsule/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/jimenez-2018-capsule/</guid>
      <description></description>
    </item>
    
    <item>
      <title>When regression meets manifold learning for object recognition and pose estimation</title>
      <link>https://albarqouni.github.io/publication/bui-2018-regression/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/bui-2018-regression/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
