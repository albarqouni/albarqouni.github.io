<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Medical Imaging | Shadi Albarqouni</title>
    <link>https://albarqouni.github.io/tags/medical-imaging/</link>
      <atom:link href="https://albarqouni.github.io/tags/medical-imaging/index.xml" rel="self" type="application/rss+xml" />
    <description>Medical Imaging</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>©Shadi Albarqouni 2020</copyright><lastBuildDate>Mon, 06 Apr 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://albarqouni.github.io/images/icon_hu12b039ffaabbdafcf05d9d63bdcb294a_6057_512x512_fill_lanczos_center_2.png</url>
      <title>Medical Imaging</title>
      <link>https://albarqouni.github.io/tags/medical-imaging/</link>
    </image>
    
    <item>
      <title>Deep Federated Learning in Healthcare</title>
      <link>https://albarqouni.github.io/project/federated-learning/</link>
      <pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/project/federated-learning/</guid>
      <description>&lt;p&gt;Deep Learning (DL) has emerged as a leading technology for accomplishing many challenging tasks showing outstanding performance in a broad range of computer vision and medical applications. However, this success comes at the cost of collecting and processing a massive amount of data, which often are not accessible, in Healthcare, due to privacy issues. Federated Learning (FL) has been recently introduced to allow training DL models without sharing the data. Instead, DL models at local hubs, &lt;em&gt;i.e.&lt;/em&gt; hospitals, share only the trained parameters with a centralized DL model, which is, in return, responsible for updating the local DL models as well.&lt;/p&gt;
&lt;p&gt;Our golas in this project is to develop novel models and algorithms for a ground-breaking new generation of deep FL, which can distill the knowledge from local hubs, &lt;em&gt;i.e.&lt;/em&gt; hospitals, and edges, &lt;em&gt;i.e.&lt;/em&gt; wearable devices, to provide personalized healthcare services.&lt;/p&gt;
&lt;p&gt;The principal &lt;strong&gt;challenges&lt;/strong&gt;, to overcome, concern the nature of medical data, namely data heterogeneity; severe class-imbalance, few amounts of annotated data, inter-/intra-scanners variability (domain shift), inter-/intra-observer variability (noisy annotations), system heterogeneity, and privacy issues (see the example below).&lt;/p&gt;
&lt;h3 id=&#34;collaboration&#34;&gt;Collaboration:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;funding&#34;&gt;Funding:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Soon&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Learn from Crowds</title>
      <link>https://albarqouni.github.io/project/learn-from-crowds/</link>
      <pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/project/learn-from-crowds/</guid>
      <description>&lt;p&gt;Today&amp;rsquo;s clinical procedures often generate a large amount of digital images requiring close inspection. Manual examination by physicians is time-consuming and machine learning in computer vision and pattern recognition is playing an increasing role in medical applications. In contrast to pure machine learning methods, crowdsourcing can be used for processing big data sets, utilising the collective brainpower of huge crowds. Since individuals in the crowd are usually no medical experts, preparation of medical data as well as an appropriate visualization to the user becomes indispensable. The concept of gamification typically allows for embedding non-game elements in a serious game environment, providing an incentive for persistent engagement to the crowd. Medical image analysis empowered by the masses is still rare and only a few applications successfully use the crowd for solving medical problems. The goal of this project is to bring the gamification and crowdsourcing to the Medical Imaging community.&lt;/p&gt;
&lt;h3 id=&#34;collaboration&#34;&gt;Collaboration:&lt;/h3&gt;
&lt;h3 id=&#34;funding&#34;&gt;Funding:&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Learn from Prior Knowledge</title>
      <link>https://albarqouni.github.io/project/learn-from-graph/</link>
      <pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/project/learn-from-graph/</guid>
      <description>&lt;p&gt;Together with our clinical and industry partners, we realized that there is a need to incorporate domain-specific knowledge and let the model &lt;em&gt;Learn from a Prior Knowledge&lt;/em&gt;. We first investigated modeling general priors, i.e., manifold assumptions, to learn powerful representations. Such representations achieved state-of-the-art on benchmark datasets, such as e IDRiD for Diabetic Retinopathy Early Detection (Sarhan &lt;em&gt;et al.&lt;/em&gt; 2019), and 7 Scenes for Camera Relocalization (Bui &lt;em&gt;et al.&lt;/em&gt; 2017). Then, we started looking into the laplacian graph, where prior knowledge can be modeled as a soft constraint, i.e., regularization, to learn feature representation that follows such manifold defined by graphs. We have shown in our ISBI (Kazi &lt;em&gt;et al.&lt;/em&gt; 2019a), MICCAI (Kazi &lt;em&gt;et al.&lt;/em&gt; 2019b), and IPMI (Kazi &lt;em&gt;et al.&lt;/em&gt; 2019) papers that leveraging prior knowledge such as proximity of ages, gender, and a few lab results, are of high importance in Alzheimer classification.&lt;/p&gt;
&lt;h3 id=&#34;collaboration&#34;&gt;Collaboration:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;funding&#34;&gt;Funding:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Siemens AG&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Learn to Adapt</title>
      <link>https://albarqouni.github.io/project/learn-to-adapt/</link>
      <pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/project/learn-to-adapt/</guid>
      <description>&lt;p&gt;To build domain-agnostic models that are generalizable to a different domain, i.e., scanners, we have investigated three directions; First, &lt;em&gt;Style Transfer&lt;/em&gt;, where the style/color of the source domain is transferred to match the target one.  Such style transfer is performed in the high-dimensional image space using adversarial learning, as shown in our papers on Histology Imaging (Lahiani &lt;em&gt;et al.&lt;/em&gt; 2019a, Lahiani &lt;em&gt;et al.&lt;/em&gt; 2019b, Shaban &lt;em&gt;et al.&lt;/em&gt; 2019). Second, &lt;em&gt;Domain Adaptation&lt;/em&gt;, where the distance between the features of the source and target domains are minimized. Such distance can be optimized in a supervised fashion, i.e., class aware, using angular cosine distance as shown in our paper on MS Lesion Segmentation in MR Imaging (Baur &lt;em&gt;et al.&lt;/em&gt; 2017), or in an unsupervised way, i.e., class agnostic, using adversarial learning as explained in our article on Left atrium Segmentation in Ultrasound Imaging (Degel &lt;em&gt;et al.&lt;/em&gt; 2018). Yet, another exciting direction that has been recently investigated in our paper (Lahiani &lt;em&gt;et al.&lt;/em&gt; 2019c) is to disentangle the feature that is responsible for the style and color from the one responsible for the semantics.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Baur_Degel_Shaban.jpeg&#34; alt=&#34;Baur et al. 2017, Degel et al. 2018, and Shaban et al. 2019&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;lahiani2019c.jpeg&#34; alt=&#34;Lahiani et al. 2019c&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;collaboration&#34;&gt;Collaboration:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Eldad Klaiman, Roche Diagnostics GmbH&lt;/li&gt;
&lt;li&gt;Georg Schummers and Matthias Friedrichs, TOMTEC Imaging Systems GmbH&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Learn to Learn</title>
      <link>https://albarqouni.github.io/project/learn-to-learn/</link>
      <pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/project/learn-to-learn/</guid>
      <description>&lt;p&gt;To build models that are transferable to different tasks or different data distributions, i.e., non i.i.d., we have investigated meta-learning approaches such as prototypical networks (PN) (Snell &lt;em&gt;et al.&lt;/em&gt; 2017). PN learns a class prototype from very few amounts of labeled data, e.g., 1-5 shots, and use the learned prototypes to perform the classification tasks. In the context of medical imaging, we were first to introduce Few-Shot Learning into the MIC community. We have shown in our recent ICML Workshop paper (Ayyad &lt;em&gt;et al.&lt;/em&gt; 2019) that our novel Semi-Supervised Few-Shot Learning achieves the state-of-the-art on benchmark datasets; Omniglot, miniImageNet, and TieredImageNet. Further, we have demonstrated in our recent paper (Parida &lt;em&gt;et al.&lt;/em&gt; 2019) that such concepts can be utilized in medical imaging segmentation with an extremely low budget of annotated data, e.g., bounding boxes, and better generalization capability, i.e., to new organs or anomalies, however, at the cost of less accurate segmentation. Yet, our proposed models have great potential in clinical practice where a novel application could come in, and only a very few annotations are required, to perform segmentation tasks. Further, such a learning paradigm has a great potential in Federated Learning, where the data acquired at different hospitals capture heterogeneous and non i.i.d data, i.e., various tasks, making proposed models suitable for such a problem.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Parida2019.jpeg&#34; alt=&#34;Parida et al. 2019&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;collaboration&#34;&gt;Collaboration:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Prof. 
&lt;a href=&#34;https://www.kaust.edu.sa/en/study/faculty/mohamed-elhoseiny&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mohamed Elhoseiny&lt;/a&gt;, 
&lt;a href=&#34;https://ai.facebook.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Facebook AI Research&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Learn to Reason and Explain</title>
      <link>https://albarqouni.github.io/project/learn-to-reason-and-explain/</link>
      <pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/project/learn-to-reason-and-explain/</guid>
      <description>&lt;p&gt;To build explainable AI models that are interpretable for our end-users, i.e., clinicians, we have investigated two research directions. First, we have utilized some visualization techniques to explain and interpret &amp;ldquo;black box&amp;rdquo; models by propagating back the gradient of the class of interest to the image space where you can see the relevant semantics, so-called Gradient Class Activation Maps (GradCAM). Sooner, we found out such techniques do not produce meaningful results. In other words, irrelevant semantics could be highly activated in GradCAM, yielding unreliable explanation tools. To overcome such a problem, we have introduced a robust optimization loss in our MICCAI paper (Khakzar &lt;em&gt;et al.&lt;/em&gt; 2019), which generated adversarial examples enforcing the network to only focus on relevant features and probably correlated with other examples belonging to the same class.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Khakzar2019.jpeg&#34; alt=&#34;Khakzar2019&#34;&gt;&lt;/p&gt;
&lt;p&gt;Second, we have investigated designing and building explainable models by i) uncertainty quantification and ii) disentangled feature representation. In the first category, we started understanding the uncertainty estimates generated by Monte-Carlo Dropout, the approximate of Bayesian Neural Networks, and other techniques, e.g. PointNet, in Camera Relocalization problem (Bui &lt;em&gt;et al.&lt;/em&gt; 2018), to shed light on the ambiguity present in the dataset. We took a step further, and use such uncertainty estimates to refine the segmentation in an unsupervised fashion (Soberanis-Mukul &lt;em&gt;et al.&lt;/em&gt; 2019, Bui &lt;em&gt;et al.&lt;/em&gt; 2019).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Sarhan2019.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Recently, we have investigated modeling the labels uncertainty, which is related to the inter-/intra-observer variability, and produced a metric to quantify such uncertainty. We have shown in our paper (Tomczack &lt;em&gt;et al.&lt;/em&gt; 2019) that such uncertainty can be rather disentangled from the model and data uncertainties, so-called, epistemic, and aleatoric uncertainties, respectively. We believe such uncertainty is of high importance to the referral systems. In the second category, we have studied the variational methods, and disentangled representations, where the assumption here that some generative factors, &lt;em&gt;e.g.&lt;/em&gt;, color, shape, and pathology, will be captured in the lower-dimensional latent space, and one can easily go through the manifold and generate tons of example by sampling from the posterior distribution. We were among the firsts who introduce such concepts in medical imaging by investigating the influence of residual blocks and adversarial learning on disentangled representation (Sarhan &lt;em&gt;et al.&lt;/em&gt; 2019). Our hypothesis that better reconstruction fidelity would force the network to model high resolution, which might have a positive influence on the disentangled representation, in particular, some pathologies.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Roger_Tomczack2019.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;collaboration&#34;&gt;Collaboration:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Dr. 
&lt;a href=&#34;https://scholar.google.de/citations?user=PmHOyT0AAAAJ&amp;amp;hl=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abouzar Eslami&lt;/a&gt;, Carl Zeiss Meditec AG&lt;/li&gt;
&lt;li&gt;PD. Dr. 
&lt;a href=&#34;https://scholar.google.de/citations?user=ELOVd8sAAAAJ&amp;amp;hl=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slobodan Ilic&lt;/a&gt;, Siemens AG&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;funding&#34;&gt;Funding:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Siemens AG&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Learn to Recognize</title>
      <link>https://albarqouni.github.io/project/learn-to-recognize/</link>
      <pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/project/learn-to-recognize/</guid>
      <description>&lt;p&gt;We started investigating Convolutional Neural Networks for Object Recognition in a supervised fashion, for example, mitotic figure detection in histology imaging (Albarqouni &lt;em&gt;et al.&lt;/em&gt; 2016), Catheter electrodes detection and depth estimation in Interventional Imaging (Baur &lt;em&gt;et al.&lt;/em&gt; 2016), femur fracture detection in radiology (Kazi &lt;em&gt;et al.&lt;/em&gt; 2017), in-depth layer X-ray synthesis (Albarqouni &lt;em&gt;et al.&lt;/em&gt; 2017), and pose estimation of mobile X-rays (Bui &lt;em&gt;et al.&lt;/em&gt; 2017). One of the first work which has been highly recognized and featured in the media is AggNet (Albarqouni &lt;em&gt;et al.&lt;/em&gt; 2016) for Mitotic figure detection in Histology Images. Although the network architecture was shallow, it was trained using millions of multi-scale RGB patches of histology images, achieving outstanding performance (ranked 3rd among 15 participants in AMIDA13 challenge).&lt;/p&gt;
&lt;p&gt;During our work, we found out such data-driven models demand a massive amount of annotated data, which might not be available in medical imaging and can not be mitigated by simple data augmentation. Besides, we found out such models are so sensitive to domain shift, i.e., different scanner, and methods such as domain adaptation is required. Therefore, we have focused our research directions to develop fully-automated, high accurate solutions that save export labor and efforts, and mitigate the challenges in medical imaging. For example,  i) the availability of a few annotated data, ii) low inter-/intra-observers agreement, iii) high-class imbalance, iv) inter-/intra-scanners variability and v) domain shift.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Shadi_Web_Images.016.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;To mitigate the problem of limited annotated data, we developed models that &lt;em&gt;Learn from a Few Examples&lt;/em&gt; by i) leveraging the massive amount of unlabeled data via semi-supervised techniques (Baur and Albarqouni &lt;em&gt;et al.&lt;/em&gt; 2017), ii) utilizing weakly labeled data, which is way cheaper than densely one (Kazi &lt;em&gt;et al.&lt;/em&gt; 2017), iii) generating more examples through modeling the data distribution (Baur &lt;em&gt;et al.&lt;/em&gt; 2018), and finally by iv) investigating unsupervised approaches (Baur &lt;em&gt;et al.&lt;/em&gt; 2018, Baur &lt;em&gt;et al.&lt;/em&gt; 2019).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Shadi_Web_Images.017.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;collaboration&#34;&gt;Collaboration:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Prof. 
&lt;a href=&#34;https://www.med.upenn.edu/apps/faculty/index.php/g275/p9161623&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Peter Nöel&lt;/a&gt;, Department of Radiology, 
&lt;a href=&#34;https://www.med.upenn.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Pennsylvania&lt;/a&gt;, USA&lt;/li&gt;
&lt;li&gt;Prof. 
&lt;a href=&#34;https://www.med.physik.uni-muenchen.de/personen/guests/dr_guillaume_landry/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Guillaume Landry&lt;/a&gt;, Department of Radiation Oncology, Medical Center of the University of Munich, Germany&lt;/li&gt;
&lt;li&gt;Dr. 
&lt;a href=&#34;https://www.neurokopfzentrum.med.tum.de/neuroradiologie/forschung_projekt_computational_imaging.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Benedikt Wiestler&lt;/a&gt;, TUM Neuroradiologie, 
&lt;a href=&#34;https://www.mri.tum.de/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Klinikum rechts der Isar&lt;/a&gt;, Germany&lt;/li&gt;
&lt;li&gt;Prof. Dr. med. 
&lt;a href=&#34;https://www.kernspin-maximilianstrasse.de/prof-dr-med-sonja-kirchhoff/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sonja Kirchhoff&lt;/a&gt;, 
&lt;a href=&#34;https://www.mri.tum.de/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Klinikum rechts der Isar&lt;/a&gt;, Germany&lt;/li&gt;
&lt;li&gt;Prof. 
&lt;a href=&#34;[https://www.ls2n.fr/annuaire/Diana%20MATEUS/&#34;&gt;Diana Mateus&lt;/a&gt;, 
&lt;a href=&#34;https://www.ec-nantes.fr/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ecole Centrale Nantes&lt;/a&gt;, France&lt;/li&gt;
&lt;li&gt;Prof. 
&lt;a href=&#34;https://www5.cs.fau.de/en/our-team/maier-andreas/projects/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Andreas Maier&lt;/a&gt;, 
&lt;a href=&#34;https://www.fau.de/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Friedrich-Alexander-Universität Erlangen-Nürnberg&lt;/a&gt;, Germany&lt;/li&gt;
&lt;li&gt;Prof. 
&lt;a href=&#34;https://health.uottawa.ca/people/fallavollita-pascal&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pascal Fallavollita&lt;/a&gt;, 
&lt;a href=&#34;https://www.uottawa.ca/en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ottawa University&lt;/a&gt;, Canada&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;funding&#34;&gt;Funding:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Siemens Healthineers&lt;/li&gt;
&lt;li&gt;Siemens AG&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Modelling Uncertainty in Deep Learning for Medical Applications</title>
      <link>https://albarqouni.github.io/project/uncertainty/</link>
      <pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/project/uncertainty/</guid>
      <description>&lt;p&gt;Deep Learning has emerged as a leading technology for accomplishing many challenging tasks showing outstanding performance in a broad range of applications in computer vision and medical applications. Despite its success and merit in recent state-of-the-art methods, DL tools still lack in robustness hindering its adoption in medical applications. Modeling uncertainty, through Bayesian Inference and Monte-Carlo dropout, has been successfully introduced to computer vision for better understanding the underlying deep learning models. In this proposal, we investigate modeling the uncertainty for medical applications given the well-known challenges in medical image analysis, namely severe class-imbalance, few amounts of labeled data, domain shift, and noisy annotations.&lt;/p&gt;
&lt;h3 id=&#34;collaboration&#34;&gt;Collaboration:&lt;/h3&gt;
&lt;p&gt;Prof. 
&lt;a href=&#34;http://people.ee.ethz.ch/~kender/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ender Konukoglu&lt;/a&gt;, 
&lt;a href=&#34;https://ee.ethz.ch/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Department of Information Technology and Electrical Engineerng&lt;/a&gt;, 
&lt;a href=&#34;https://ethz.ch/en.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ETH Zurich&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Prof. 
&lt;a href=&#34;http://wp.doc.ic.ac.uk/dr/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Daniel Rueckert&lt;/a&gt;, 
&lt;a href=&#34;http://www.imperial.ac.uk/computing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Department of Computing&lt;/a&gt;, 
&lt;a href=&#34;http://www.imperial.ac.uk/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imperial College London&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Prof. 
&lt;a href=&#34;http://campar.in.tum.de/Main/NassirNavab&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nassir Navab&lt;/a&gt;, 
&lt;a href=&#34;http://campar.in.tum.de/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Faculty of Informatics&lt;/a&gt;, 
&lt;a href=&#34;www.tum.de&#34;&gt;Technical University of Munich&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;funding&#34;&gt;Funding:&lt;/h3&gt;
&lt;p&gt;This project is supported by the 
&lt;a href=&#34;https://www.daad.de/de/studieren-und-forschen-in-deutschland/stipendien-finden/prime/prime-fellows-201819/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PRIME programme&lt;/a&gt; of the 
&lt;a href=&#34;www.daad.de&#34;&gt;German Academic Exchange Service (DAAD)&lt;/a&gt; with funds from the 
&lt;a href=&#34;www.bmbf.de&#34;&gt;German Federal Ministry of Education and Research (BMBF)&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Telemedicine in Palestine</title>
      <link>https://albarqouni.github.io/project/telemedicine/</link>
      <pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/project/telemedicine/</guid>
      <description>&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/BXqwyYh8hPU9Ub&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt; &lt;div style=&#34;margin-bottom:5px&#34;&gt; &lt;strong&gt; &lt;a href=&#34;//www.slideshare.net/sbaraqouni/telemedicine-in-palestine&#34; title=&#34;Telemedicine in Palestine&#34; target=&#34;_blank&#34;&gt;Telemedicine in Palestine&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href=&#34;https://www.slideshare.net/sbaraqouni&#34; target=&#34;_blank&#34;&gt;Shadi Nabil Albarqouni&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;
&lt;h3 id=&#34;collaboration&#34;&gt;Collaboration:&lt;/h3&gt;
&lt;h3 id=&#34;funding&#34;&gt;Funding:&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Uncertainty-based graph convolutional networks for organ segmentation refinement</title>
      <link>https://albarqouni.github.io/publication/soberanis-2019-uncertainty/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/soberanis-2019-uncertainty/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A learning without forgetting approach to incorporate artifact knowledge in polyp localization tasks</title>
      <link>https://albarqouni.github.io/publication/soberanis-2020-learning/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/soberanis-2020-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An objective comparison of detection and segmentation algorithms for artefacts in clinical endoscopy</title>
      <link>https://albarqouni.github.io/publication/ali-2020-objective/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/ali-2020-objective/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Autoencoders for Unsupervised Anomaly Segmentation in Brain MR Images: A Comparative Study</title>
      <link>https://albarqouni.github.io/publication/baur-2020-autoencoders/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/baur-2020-autoencoders/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Benefit of dual energy CT for lesion localization and classification with convolutional neural networks</title>
      <link>https://albarqouni.github.io/publication/shapira-2020-benefit/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/shapira-2020-benefit/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Image-to-Images Translation for Multi-Task Organ Segmentation and Bone Suppression in Chest X-Ray Radiography</title>
      <link>https://albarqouni.github.io/publication/eslami-2020-image/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/eslami-2020-image/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Liver lesion localisation and classification with convolutional neural networks: a comparison between conventional and spectral computed tomography</title>
      <link>https://albarqouni.github.io/publication/shapira-2020-liver/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/shapira-2020-liver/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ROAM: Random Layer Mixup for Semi-Supervised Learning in Medical Imaging</title>
      <link>https://albarqouni.github.io/publication/bdair-2020-roam/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/bdair-2020-roam/</guid>
      <description></description>
    </item>
    
    <item>
      <title>AI meets COVID-19</title>
      <link>https://albarqouni.github.io/slides/_example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/slides/_example/</guid>
      <description>&lt;h1 id=&#34;brief-progress-of&#34;&gt;Brief Progress of&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic&lt;/a&gt; | 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;pathology-quantification&#34;&gt;Pathology Quantification:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;To be able to quantify the pathologies in thorax CT scans, one needs to segment the pathologies, and probably classify them into common ones characterizing the COVID-19, &lt;em&gt;e.g.&lt;/em&gt;,
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;fragment &#34; &gt;
Ground Glass Opacity (GGO)
&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;fragment &#34; &gt;
Consolidations
&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;fragment &#34; &gt;
Scarr
&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;fragment &#34; &gt;
Pleueral Effusion
&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
Three
&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://spectrum.chat/academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AI meets COVID-19</title>
      <link>https://albarqouni.github.io/slides/covid19/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/slides/covid19/</guid>
      <description>&lt;h1 id=&#34;brief-progress-of&#34;&gt;Brief Progress of&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic&lt;/a&gt; | 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;* Reporting on current results (CAMP, IBBM):  (1) data availability, (2) segmentation, (3) classification, (4) algorithmic integration.
* Next steps and modus operandi for the next weeks or months: (1) data from Radiologie and elsewhere, (2) data products and interface to Radiologie infrastructure, (3) integration of CAMP &amp;amp; IBBM algorithms.
* Transition to &amp;ldquo;funded research&amp;rdquo;. (At IBBM the means for continuing this in an unfunded mode are essentially gone. Continuation will require a coordinated program.)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
Three
&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://spectrum.chat/academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Preliminary Meeting for the seminar on Federated Learning in Healthcare</title>
      <link>https://albarqouni.github.io/slides/federated/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/slides/federated/</guid>
      <description>&lt;p&gt;Seminar on&lt;/p&gt;
&lt;h2 id=&#34;span-stylecolororangefederated-learningspan-in-span-stylecolorgreenhealthcarespan&#34;&gt;&lt;span style=&#34;color:orange&#34;&gt;Federated Learning&lt;/span&gt; in &lt;span style=&#34;color:green&#34;&gt;Healthcare&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://albarqouni.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Shadi Albarqouni&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Visiting Scientist at 
&lt;a href=&#34;https://people.ee.ethz.ch/~salbarqouni/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ETH Zurich&lt;/a&gt; | Senior Research Scientist and Team Lead at 
&lt;a href=&#34;http://campar.in.tum.de/Main/ShadiAlbarqouni&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TU Munich&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;content&#34;&gt;Content&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Brief introduction about Federated Learning&lt;/li&gt;
&lt;li&gt;It is the right thing at the right time!&lt;/li&gt;
&lt;li&gt;Course structure&lt;/li&gt;
&lt;li&gt;Registration&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;
&lt;a href=&#34;https://federated.withgoogle.com/#about&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;FL_overview.png&#34; alt=&#34;https://federated.withgoogle.com/#about&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;span style=&#34;color:orange&#34;&gt;&lt;strong&gt;Deep Learning&lt;/strong&gt; &lt;/span&gt; success comes at the cost of collecting and processing a &lt;span style=&#34;color:orange&#34;&gt;&lt;strong&gt;massive amount of data&lt;/strong&gt; &lt;/span&gt; , which often are not accessible due to &lt;span style=&#34;color:orange&#34;&gt;&lt;strong&gt;privacy issues&lt;/strong&gt; &lt;/span&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;span style=&#34;color:orange&#34;&gt;&lt;strong&gt;Federated Learning&lt;/strong&gt; &lt;/span&gt; has been recently introduced to allow training DL models &lt;span style=&#34;color:orange&#34;&gt;&lt;strong&gt;without sharing the data&lt;/strong&gt; &lt;/span&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&#34;FL.png&#34; alt=&#34;https://federated.withgoogle.com/#about&#34;&gt;
&lt;sub&gt;Taken from Rieke et al. &amp;ldquo;The future of digital health with federated learning.&amp;rdquo; arXiv preprint arXiv:2003.08119 (2020).&lt;/sub&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;The principal &lt;span style=&#34;color:orange&#34;&gt;&lt;strong&gt;challenges&lt;/strong&gt; &lt;/span&gt;, to overcome, concern the nature of medical data, namely&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;fragment &#34; &gt;
&lt;span style=&#34;color:orange&#34;&gt;&lt;strong&gt;Data heterogeneity&lt;/strong&gt;&lt;/span&gt;; severe class-imbalance, few amounts of annotated data, inter-/intra-scanners variability (domain shift), and inter-/intra-observer variability (noisy annotations)
&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;fragment &#34; &gt;
&lt;span style=&#34;color:orange&#34;&gt;&lt;strong&gt;System Heterogeneity&lt;/strong&gt; &lt;/span&gt;
&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;fragment &#34; &gt;
&lt;span style=&#34;color:orange&#34;&gt;&lt;strong&gt;Privacy-Issues&lt;/strong&gt; &lt;/span&gt;
&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;right-thing-at-the-right-time&#34;&gt;Right thing at the right time&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;Google.png&#34; alt=&#34;https://federated.withgoogle.com/#about&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&#34;SE1.png&#34; alt=&#34;https://federated.withgoogle.com/#about&#34;&gt;
&lt;img src=&#34;SE3.png&#34; alt=&#34;https://federated.withgoogle.com/#about&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&#34;FL_Paper.png&#34; alt=&#34;https://federated.withgoogle.com/#about&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;
&lt;a href=&#34;http://federated-learning.org/fl-neurips-2019/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;NeurIPS.png&#34; alt=&#34;https://federated.withgoogle.com/#about&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;
&lt;a href=&#34;http://federated-learning.org/fl-icml-2020/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;ICML.png&#34; alt=&#34;https://federated.withgoogle.com/#about&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;
&lt;a href=&#34;https://dcl-workshop.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;DCL_MICCAI.png&#34; alt=&#34;https://federated.withgoogle.com/#about&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;course-structurehttpcamparintumdeviewchairteachingws20flhrequirements&#34;&gt;
&lt;a href=&#34;http://campar.in.tum.de/view/Chair/TeachingWs20FLH#Requirements&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Course Structure&lt;/a&gt;&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Basic Info.&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Type:&lt;/strong&gt;  Master Seminar (IN2107)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Language:&lt;/strong&gt; English&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SWS:&lt;/strong&gt; 2&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ECTS:&lt;/strong&gt; 5&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Webpage:&lt;/strong&gt; &lt;a href=&#34;http://campar.in.tum.de/Chair/TeachingWs20FLH&#34;&gt;http://campar.in.tum.de/Chair/TeachingWs20FLH&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Time:&lt;/strong&gt; Mondays (bi-weekly), 10 - 12&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Location:&lt;/strong&gt; TBA (Zoom OR MI 03.13.010)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Requirements:&lt;/strong&gt; Solid Background in Machine/Deep Learning&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Tutors&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;MedIA_CAMP_Albarqouni.jpeg&#34; alt=&#34;Team&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Objectives:&lt;/strong&gt;
Learn through &lt;span style=&#34;color:orange&#34;&gt;&lt;em&gt;read&lt;/em&gt;, &lt;em&gt;understand&lt;/em&gt;, &lt;em&gt;present&lt;/em&gt;, and &lt;em&gt;discuss&lt;/em&gt;&lt;/span&gt; many scientific papers&lt;sup&gt;1&lt;/sup&gt; tackling the challenges present in Federated Learning.&lt;/p&gt;
&lt;p&gt;&lt;sub&gt;&lt;sup&gt;1&lt;/sup&gt;Our pool of papers includes the ones published in NeurIPS, ICML, ICLR, IEEE TMI, MedIA, MICCAI, MIDL, and ISBI.&lt;/sub&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;schedule&#34;&gt;Schedule:&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;em&gt;Date&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Session&lt;/em&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;09.11.2020&lt;/td&gt;
&lt;td&gt;Federated Learning; Challenges, Methods, and Future&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;23.11.2020&lt;/td&gt;
&lt;td&gt;Data Heterogeneity I&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;07.12.2020&lt;/td&gt;
&lt;td&gt;Data Heterogeneity II&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;21.12.2020&lt;/td&gt;
&lt;td&gt;System Heterogeneity&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;18.01.2021&lt;/td&gt;
&lt;td&gt;Privacy-Issues&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;01.02.2021&lt;/td&gt;
&lt;td&gt;Explainability and Accountability&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;fragment &#34; &gt;
&lt;span style=&#34;color:orange&#34;&gt;&lt;strong&gt;Presentation (60%)&lt;/strong&gt;:&lt;/span&gt; The selected paper is presented to the other participants (30 minutes presentation plus 10 minutes Q&amp;amp;A)
&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;fragment &#34; &gt;
&lt;span style=&#34;color:orange&#34;&gt;&lt;strong&gt;Blog | Poster (30%)&lt;/strong&gt;:&lt;/span&gt; A blog post of 1000-1500 words excluding references should be submitted before the deadline
&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;fragment &#34; &gt;
&lt;span style=&#34;color:orange&#34;&gt;&lt;strong&gt;Attendance (10%)&lt;/strong&gt;:&lt;/span&gt; Students are expected  to participate actively in all seminar sessions
&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;If you are still interested in this seminsr course, please write a brief motivation paragraph (few lines) showing your interest and your background in Machine/Deep Learning and send it with a subject “FLH_Motivation”, to Shadi Albarqouni (&lt;a href=&#34;mailto:shadi.albarqouni@tum.de&#34;&gt;shadi.albarqouni@tum.de&lt;/a&gt;). Deadline is 21.07.2020.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
&lt;span style=&#34;color:orange&#34;&gt;Don’t forget to register at TUM matching system 16.07 to 21.07.2020: register via 
&lt;a href=&#34;matching.in.tum.de&#34;&gt;matching.in.tum.de&lt;/a&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>Preliminary Meeting for the seminar on Federated Learning in Healthcare</title>
      <link>https://albarqouni.github.io/slides/pres/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/slides/pres/</guid>
      <description>&lt;h2 id=&#34;span-stylecolororangepresentationspan-and-span-stylecolorgreenblog-post-spanguidelines&#34;&gt;&lt;span style=&#34;color:orange&#34;&gt;Presentation&lt;/span&gt; and &lt;span style=&#34;color:green&#34;&gt;Blog Post &lt;/span&gt;Guidelines&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://albarqouni.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Shadi Albarqouni&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;AI Young Investigator Group Leader at 
&lt;a href=&#34;https://www.helmholtz.ai/themenmenue/our-research/research-groups/albarqounis-group/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Helmholtz AI&lt;/a&gt; | TUM Junior Fellow at 
&lt;a href=&#34;http://campar.in.tum.de/Main/ShadiAlbarqouni&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TU Munich&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;05.11.2020&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;span-stylecolororangepresentationspan-guidelines&#34;&gt;&lt;span style=&#34;color:orange&#34;&gt;Presentation&lt;/span&gt; Guidelines&lt;/h2&gt;
&lt;hr&gt;
&lt;ol start=&#34;0&#34;&gt;
&lt;li&gt;&lt;span class=&#34;fragment &#34; &gt;
&lt;span style=&#34;color:orange&#34;&gt;&lt;strong&gt;First Slide&lt;/strong&gt;:&lt;/span&gt; Please have the title of the paper, authors (&amp;lt;3), conference proceedings, or journal, and the name of the presenter.
&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;Example: 

The Future of Digital Health with Federated Learning

Rieke et al., Nature Digital Medicine, 2020

Presenter: Firstname Surname  
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li&gt;&lt;span class=&#34;fragment &#34; &gt;
&lt;span style=&#34;color:orange&#34;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;:&lt;/span&gt; In the first few slides, you need to introduce the subject to the audience. A brief &lt;strong&gt;background&lt;/strong&gt; (big picture) and a few &lt;strong&gt;related works&lt;/strong&gt; (more concise) would help you to position your paper in the big picture. &lt;em&gt;It is quite important to talk about the key conclusions at the very beginning&lt;/em&gt;.  The &lt;strong&gt;rationale&lt;/strong&gt; for the paper, i.e. &lt;em&gt;why you did the work?&lt;/em&gt;, has to be addressed by the end of the Intro. slides.
&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;span class=&#34;fragment &#34; &gt;
&lt;span style=&#34;color:orange&#34;&gt;&lt;strong&gt;Methodology&lt;/strong&gt;:&lt;/span&gt; You need to explain the method in details, if possible. Start with an overview of the &lt;strong&gt;framework&lt;/strong&gt; (e.g. flow chart); input, output, and core components, before you dive deeper into the &lt;strong&gt;key contributions&lt;/strong&gt;; e.g. design architecture, objective functions, &amp;hellip;etc. Details that might distract the audience can be moved to the backup slides. In short, explain &lt;em&gt;how did you do it?&lt;/em&gt;
&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;span class=&#34;fragment &#34; &gt;
&lt;span style=&#34;color:orange&#34;&gt;&lt;strong&gt;Experiments and Results&lt;/strong&gt;:&lt;/span&gt; You need to explain the &lt;strong&gt;experimental designs&lt;/strong&gt;; datasets, evaluation metrics, and training setup, and the &lt;strong&gt;rationale behind them&lt;/strong&gt;, before you show the &lt;strong&gt;key results&lt;/strong&gt;.  The figures should be clearly labeled, e.g. explain the figures axes before you describe the results addressing the question &lt;em&gt;what did you find?&lt;/em&gt;
&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;span class=&#34;fragment &#34; &gt;
&lt;span style=&#34;color:orange&#34;&gt;&lt;strong&gt;Conclusion &amp;amp; Future Work&lt;/strong&gt;:&lt;/span&gt; Discuss the results (your interpretation), before you list the concluding reamrks, learned lessons, and future research directions.
&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;&lt;span class=&#34;fragment &#34; &gt;
&lt;span style=&#34;color:orange&#34;&gt;&lt;strong&gt;Group Discussion&lt;/strong&gt;:&lt;/span&gt; This is the most important part where you need to list a few major things that you need to discuss with the group, for example:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;How the paper could be improved?&lt;/em&gt; e.g. critique on the proposed method, design choices, missing experiments, or inappropriate evaluation metrics.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;How the paper could be applied in medical domain?&lt;/em&gt; e.g. challenges in healthcare.&lt;/li&gt;
&lt;li&gt;Have a look at the reviewers feedback, if available, e.g. 
&lt;a href=&#34;https://openreview.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;openreview&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/span&gt;
&lt;hr&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;&lt;span class=&#34;fragment &#34; &gt;
&lt;span style=&#34;color:orange&#34;&gt;&lt;strong&gt;Needless to Say&lt;/strong&gt;:&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;Read the paper carefully, and look for complementary materials; blog posts, videos, or code repo. to better understand the paper.&lt;/li&gt;
&lt;li&gt;Be mindful of time.  You have 30 mins for points 1-4, and 15 mins for point 5. As a rule of thumb, # slides &amp;lt; given time slot in mins.&lt;/li&gt;
&lt;li&gt;Build a compelling story and try to engage your audience.&lt;/li&gt;
&lt;li&gt;List the References in the footer of the corresponding slide&lt;/li&gt;
&lt;li&gt;&lt;span style=&#34;color:orange&#34;&gt;Practice, practice, practice&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/span&gt;
&lt;hr&gt;
&lt;h2 id=&#34;span-stylecolorgreenblog-post-span-guidelines&#34;&gt;&lt;span style=&#34;color:green&#34;&gt;Blog Post &lt;/span&gt; Guidelines&lt;/h2&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;The secret to getting into the deep learning community is high quality blogging. Read 5 different blog posts about the same subject and then try to synthesize your own view. Don’t just write something ok, either — take 3 or 4 full days on a post and try to make it as short and simple (yet complete) as possible.
&amp;ndash; 
&lt;a href=&#34;https://hackernoon.com/interview-with-deep-learning-researcher-and-leader-of-openmined-andrew-trask-77cd33570a8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Andrew Trask&lt;/a&gt;, DeepMind&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;This is a free-style blog post! One can hardly enforce guidelines. However, I personally liked the &lt;em&gt;Dos and Don&amp;rsquo;ts&lt;/em&gt; appeared in this 
&lt;a href=&#34;https://www.fast.ai/2019/05/13/blogging-advice/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;.  Here some examples&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://medium.com/@ODSC/what-is-federated-learning-99c7fc9bc4f5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;What is Federated Learning?
&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.inovex.de/blog/federated-learning-collaborative-training-part-1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Federated Learning: A Guide to Collaborative Training with Decentralized Sensitive Data – Part 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://towardsdatascience.com/do-we-need-deep-graph-neural-networks-be62d3ec5c59&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Do we need deep graph neural networks?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://blog.ml.cmu.edu/2020/10/12/fact-diagnostic-how-to-better-understand-trade-offs-involving-group-fairness/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FACT Diagnostic: How to Better Understand Trade-offs Involving Group Fairness&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Other blog sites: 
&lt;a href=&#34;https://blog.ml.cmu.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CMU ML Blog&lt;/a&gt;, 
&lt;a href=&#34;https://bair.berkeley.edu/blog/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BAIR Blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;helpful-resourses&#34;&gt;Helpful resourses&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://web.stanford.edu/class/ee384m/Handouts/HowtoReadPaper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How to read a paper?&lt;/a&gt; &amp;ndash;Three-pass method&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.eecs.harvard.edu/~michaelm/postscripts/ReadPaper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How to read a research paper?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://medium.com/@drewdennis/how-to-read-scientific-papers-quickly-efficiently-e7030c4018fa&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How to Read Scientific Papers Quickly &amp;amp; Efficiently&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://medium.com/advice-and-help-in-authoring-a-phd-or-non-fiction/how-to-write-a-blogpost-from-your-journal-article-6511a3837caa&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How to write a blog post from your journal article?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.fast.ai/2019/05/13/blogging-advice/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Advice for Better Blog Posts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;Should you have any questions, please drop me an email at &lt;a href=&#34;mailto:shadi.albarqouni@tum.de&#34;&gt;shadi.albarqouni@tum.de&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://twitter.com/ShadiAlbarqouni&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@ShadiAlbarqouni&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adaptive image-feature learning for disease classification using inductive graph networks</title>
      <link>https://albarqouni.github.io/publication/burwinkel-2019-adaptive/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/burwinkel-2019-adaptive/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Fusing unsupervised and supervised deep learning for white matter lesion segmentation</title>
      <link>https://albarqouni.github.io/publication/baur-2019-fusing/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/baur-2019-fusing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Graph Convolution Based Attention Model for Personalized Disease Prediction</title>
      <link>https://albarqouni.github.io/publication/kazi-2019-graph/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/kazi-2019-graph/</guid>
      <description></description>
    </item>
    
    <item>
      <title>InceptionGCN: receptive field aware graph convolutional network for disease prediction</title>
      <link>https://albarqouni.github.io/publication/kazi-2019-inceptiongcn/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/kazi-2019-inceptiongcn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learn to Segment Organs with a Few Bounding Boxes</title>
      <link>https://albarqouni.github.io/publication/parida-2019-learn/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/parida-2019-learn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning interpretable disentangled representations using adversarial vaes</title>
      <link>https://albarqouni.github.io/publication/sarhan-2019-learning/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/sarhan-2019-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning Interpretable Features via Adversarially Robust Optimization</title>
      <link>https://albarqouni.github.io/publication/khakzar-2019-learning/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/khakzar-2019-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multi-scale Microaneurysms Segmentation Using Embedding Triplet Loss</title>
      <link>https://albarqouni.github.io/publication/sarhan-2019-multi/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/sarhan-2019-multi/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Self-attention equipped graph convolutions for disease prediction</title>
      <link>https://albarqouni.github.io/publication/kazi-2019-self/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/kazi-2019-self/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Staingan: Stain style transfer for digital histological images</title>
      <link>https://albarqouni.github.io/xtarx.github.io/staingan/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/xtarx.github.io/staingan/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Towards an Interactive and Interpretable CAD System to Support Proximal Femur Fracture Classification</title>
      <link>https://albarqouni.github.io/publication/jimenez-2019-towards/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/jimenez-2019-towards/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Capsule networks against medical imaging data challenges</title>
      <link>https://albarqouni.github.io/publication/jimenez-2018-capsule/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/jimenez-2018-capsule/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Deep autoencoding models for unsupervised anomaly segmentation in brain MR images</title>
      <link>https://albarqouni.github.io/publication/baur-2018-deep/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/baur-2018-deep/</guid>
      <description></description>
    </item>
    
    <item>
      <title>GANs for medical image analysis</title>
      <link>https://albarqouni.github.io/publication/kazeminia-2018-gans/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/kazeminia-2018-gans/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Generalizing multistain immunohistochemistry tissue segmentation using one-shot color deconvolution deep neural networks</title>
      <link>https://albarqouni.github.io/publication/lahiani-2018-generalizing/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/lahiani-2018-generalizing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Generating highly realistic images of skin lesions with GANs</title>
      <link>https://albarqouni.github.io/publication/baur-2018-generating/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/baur-2018-generating/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Intraoperative stent segmentation in X-ray fluoroscopy for endovascular aortic repair</title>
      <link>https://albarqouni.github.io/publication/breininger-2018-intraoperative/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/breininger-2018-intraoperative/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multiple device segmentation for fluoroscopic imaging using multi-task learning</title>
      <link>https://albarqouni.github.io/publication/breininger-2018-multiple/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/breininger-2018-multiple/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Weakly-supervised localization and classification of proximal femur fractures</title>
      <link>https://albarqouni.github.io/publication/jimenez-2018-weakly/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/jimenez-2018-weakly/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer</title>
      <link>https://albarqouni.github.io/publication/bejnordi-2017-diagnostic/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/bejnordi-2017-diagnostic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Semi-supervised deep learning for fully convolutional networks</title>
      <link>https://albarqouni.github.io/publication/baur-2017-semi/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/baur-2017-semi/</guid>
      <description></description>
    </item>
    
    <item>
      <title>X-ray in-depth decomposition: Revealing the latent structures</title>
      <link>https://albarqouni.github.io/publication/albarqouni-2017-x/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/albarqouni-2017-x/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Aggnet: deep learning from crowds for mitosis detection in breast cancer histology images</title>
      <link>https://albarqouni.github.io/publication/albarqouni-2016-aggnet/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/albarqouni-2016-aggnet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CathNets: detection and single-view depth prediction of catheter electrodes</title>
      <link>https://albarqouni.github.io/publication/baur-2016-cathnets/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/baur-2016-cathnets/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Structure-preserving color normalization and sparse stain separation for histological images</title>
      <link>https://albarqouni.github.io/publication/vahadane-2016-structure/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/vahadane-2016-structure/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Structure-preserved color normalization for histological images</title>
      <link>https://albarqouni.github.io/publication/vahadane-2015-structure/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/vahadane-2015-structure/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Steps towards Establishing Telemedicine Center in Palestine</title>
      <link>https://albarqouni.github.io/publication/albarqouni-2009-steps/</link>
      <pubDate>Thu, 01 Jan 2009 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/albarqouni-2009-steps/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
