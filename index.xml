<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Albarqouni Lab</title>
    <link>https://albarqouni.github.io/</link>
      <atom:link href="https://albarqouni.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Albarqouni Lab</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>©Shadi Albarqouni 2022</copyright><lastBuildDate>Thu, 22 Dec 2022 23:15:58 +0000</lastBuildDate>
    <image>
      <url>https://albarqouni.github.io/images/icon_hu681563d5489cbd6b4a3381f376416aae_52869_512x512_fill_lanczos_center_3.png</url>
      <title>Albarqouni Lab</title>
      <link>https://albarqouni.github.io/</link>
    </image>
    
    <item>
      <title>Experience</title>
      <link>https://albarqouni.github.io/resume/experience/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/resume/experience/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Federated disentangled representation learning for unsupervised brain anomaly detection</title>
      <link>https://albarqouni.github.io/publication/cite-key/</link>
      <pubDate>Thu, 22 Dec 2022 23:15:58 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/cite-key/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CIR/01/2023: Ph.D. position in Computational Medical Imaging – Affordable Federated Learning (m/f/d)</title>
      <link>https://albarqouni.github.io/positions/phd_cir.01.2023/</link>
      <pubDate>Thu, 15 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/positions/phd_cir.01.2023/</guid>
      <description>&lt;p&gt;We combine excellence in research, teaching, and patient care. The University Hospital Bonn (UKB) is a maximum care hospital with more than 1,300 beds. With around 38 clinics and 31 institutes as well as more than 8,000 employees (over 5,000 full-time staff), the UKB is one of the largest employers in Bonn. Every year, the UKB treats around 50,000 inpatients and around 35,000 emergencies, as well as provides over 350,000 outpatient treatments.&lt;/p&gt;
&lt;p&gt;The following &lt;strong&gt;full-time (38.5 hrs./week) Ph.D. position&lt;/strong&gt; is available at the &lt;strong&gt;Computational Imaging Research (CIR) Lab&lt;/strong&gt;, headed by Prof. Dr. Shadi Albarqouni, in the &lt;strong&gt;Clinic for Diagnostic and Interventional Radiology&lt;/strong&gt; of the University Hospital Bonn, University of Bonn:&lt;/p&gt;
&lt;h2 id=&#34;phd-position-in-computational-medical-imaging--affordable-federated-learning-mfd&#34;&gt;&lt;strong&gt;Ph.D. position in Computational Medical Imaging – Affordable Federated Learning (m/f/d)&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;starting January 2023 or as agreed upon. The position is initially limited to three years, with the possibility of extension.&lt;/p&gt;
&lt;p&gt;The Ph.D. position will be based in the newly founded research lab for Computational Imaging Research (CIR), which aims to develop i) fully automated, highly accurate innovative computational methods that save expert labor and efforts, and mitigate the challenges in medical imaging; namely the availability of a few annotated data, low inter-/intra-observers agreement, inter-/intra-scanners variability and domain shift, ii) innovative deep Federated Learning algorithms that can fairly distill and share the knowledge among AI agents in a robust and privacy-preserved way, and iii) affordable AI algorithms suitable for low-quality data generated by low-resource settings and point-of-care devices. As AI technology becomes the de facto knowledge discovery approach in many industries including Healthcare, federated learning (FL) has emerged as a key factor to be considered for the future of 
&lt;a href=&#34;https://www.nature.com/articles/s41746-020-00323-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;digital health&lt;/a&gt;. With FL, collaborative learning without a centralized data lake is enabled and has already been adapted to digital health applications. In this project, we aim to broaden and strengthen our knowledge in the area of &lt;strong&gt;affordable FL&lt;/strong&gt; where models are trained and deployed in low-resource settings. The Ph.D. candidate will have the chance to test the developed affordable AI algorithms in low-resource settings and closely work with our clinicians to identify the clinical use cases.  The Ph.D. candidate will be enrolled in 
&lt;a href=&#34;https://www.mnf.uni-bonn.de/promotion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the Faculty of Mathematics and Natural Science&lt;/a&gt; at the University of Bonn. If you have experience with computational methods in medical imaging, this is a great opportunity to be part of our team and contribute to AI in Medicine.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Your responsibilities:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Build and create a few clinical use cases for benchmarking existing state-of-the-art (SOTA) FL algorithms. This includes running baselines and pre-/post-processing pipelines&lt;/li&gt;
&lt;li&gt;Develop innovative computational algorithms for affordable, scalable, and efficient FL models via knowledge distillation, pruning, and/or quantization methods&lt;/li&gt;
&lt;li&gt;Potentially, test and deploy the developed algorithms in low-resource settings&lt;/li&gt;
&lt;li&gt;Publish and present scientific outcomes at Intl. conferences and high-impact journals&lt;/li&gt;
&lt;li&gt;Maintain close collaboration with the team members and clinical partners&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Your qualifications:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;M.Sc. in Computer Science or equivalent with an interest in Medical Imaging&lt;/li&gt;
&lt;li&gt;Strong knowledge in Machine/Deep Learning with experience in annotation-efficient deep models, e.g., unsupervised and semi-/self-supervised learning&lt;/li&gt;
&lt;li&gt;Excellent programming skills in Python and PyTorch including fundamental software engineering principles and machine learning design patterns&lt;/li&gt;
&lt;li&gt;Excellent analytical, technical, and problem-solving skills&lt;/li&gt;
&lt;li&gt;Be highly motivated and a team player with excellent communication and presentation skills, including experience in communicating across discipline boundaries&lt;/li&gt;
&lt;li&gt;Fluent command of the English language&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Desirable qualifications:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Track record of publications at top-tier conferences and high-impact journals in the field&lt;/li&gt;
&lt;li&gt;Hands-on experience with Federated Learning frameworks&lt;/li&gt;
&lt;li&gt;Hands-on experience with the 
&lt;a href=&#34;https://monai.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MONAI framework&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Working in a Linux environment, with experience in shell and cluster (SLURM) scripting&lt;/li&gt;
&lt;li&gt;Fluency in spoken and written German&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;What we offer you:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;A secure future:&lt;/strong&gt; remuneration according to the German salary scale TV-L (E13 - 75%)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flexible for families:&lt;/strong&gt; flexible working time, home office, onsite nursery, and parental care.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Provisions for later:&lt;/strong&gt; company pension scheme&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Discounted public transport ticket:&lt;/strong&gt; discounted ticket for public transport (VRS)
on-site health management service: Numerous health promotion offers&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Employer benefits:&lt;/strong&gt; Discounted offers for employees&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Subsidized continuing education and training&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The University of Bonn is committed to diversity and equal opportunity and is certified as a family-friendly university. It aims to increase the proportion of women in areas where women are under-represented and to promote their careers in particular. Therefore, we strongly encourage applications from qualified women. Applications will be handled in accordance with the State Equality Act (Landesgleichstellungsgesetz). Applications from individuals with a certified severe disability and from those of equal status are particularly welcome.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contact:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you meet the requirements and you are looking for a challenging job? Do not hesitate and send your application including a cover letter (highlighting your qualifications), a detailed CV (with links to previous projects and code), scanned academic degrees, and the contact details of two referees (preferably by e-mail in a single PDF file up to 5 MB in size) by &lt;strong&gt;15th January 2023&lt;/strong&gt;, quoting the job advertisement no. CIR/01/2023 in your email’s subject to 
&lt;a href=&#34;mailto:shadi.albarqouni@ukbonn.de&#34;&gt;Prof. Dr. Shadi Albarqouni&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CIR/02/2023: Postdoctoral position in Computational Medical Imaging - Robustness and Uncertainty in MRI-guided Radiotherapy (m/f/d)</title>
      <link>https://albarqouni.github.io/positions/postdoc_cir.02.2023/</link>
      <pubDate>Thu, 15 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/positions/postdoc_cir.02.2023/</guid>
      <description>&lt;p&gt;We combine excellence in research, teaching, and patient care. The University Hospital Bonn (UKB) is a maximum care hospital with more than 1,300 beds. With around 38 clinics and 31 institutes as well as more than 8,000 employees (over 5,000 full-time staff), the UKB is one of the largest employers in Bonn. Every year, the UKB treats around 50,000 inpatients and around 35,000 emergencies, as well as provides over 350,000 outpatient treatments.&lt;/p&gt;
&lt;p&gt;In a collaborative &lt;strong&gt;DFG-funded research project&lt;/strong&gt; between &lt;strong&gt;TU Munich (Prof. Nassir Navab)&lt;/strong&gt;, &lt;strong&gt;LMU Munich (Prof. Guillaume Landry and PD Dr. Christopher Kurz)&lt;/strong&gt;, and &lt;strong&gt;the University of Bonn (Prof. Shadi Albarqouni)&lt;/strong&gt;, a &lt;strong&gt;full-time (38.5 hrs./week)&lt;/strong&gt; Postdoctoral position is available at the &lt;strong&gt;Computational Imaging Research (CIR) Lab&lt;/strong&gt;, headed by Prof. Dr. Shadi Albarqouni, in the &lt;strong&gt;Clinic for Diagnostic and Interventional Radiology&lt;/strong&gt; of the University Hospital Bonn, University of Bonn:&lt;/p&gt;
&lt;h2 id=&#34;postdoctoral-position-in-computational-medical-imaging---robustness-and-uncertainty-in-mri-guided-radiotherapy-mfd&#34;&gt;&lt;strong&gt;Postdoctoral position in Computational Medical Imaging - Robustness and Uncertainty in MRI-guided Radiotherapy (m/f/d)&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;starting January 2023 or as agreed upon. The position is initially limited to two years.&lt;/p&gt;
&lt;p&gt;The Postdoc position will be based in the newly founded research lab for Computational Imaging Research (CIR), which aims to develop i) fully automated, highly accurate innovative computational methods that save expert labor and efforts, and mitigate the challenges in medical imaging; namely the availability of a few annotated data, low inter-/intra-observers agreement, inter-/intra-scanners variability and domain shift, ii) innovative deep Federated Learning algorithms that can fairly distill and share the knowledge among AI agents in a robust and privacy-preserved way, and iii) affordable AI algorithms suitable for low-quality data generated by low-resource settings and point-of-care.  The Postdoctoral researcher will investigate the robustness and uncertainty in both discriminative (Spatio-temporal image segmentation) and generative (synthetic CT generation) deep learning-based models in MR-guided radiotherapy. The project will make use of data acquired with an MR-Linac (ViewRay MRIdian) that has recently begun clinical operation at the Department of Radiation Oncology of LMU Munich. The research project will offer a broad spectrum of topics in the scope of MR-guided radiotherapy, including deep learning-based image processing, as well as dose calculation and optimization. The postdoc will have the chance to closely work with clinicians and two Ph.D. students at both TU Munich and LMU Munich. If you have experience with computational methods in medical imaging, this is a great opportunity to be part of our team.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Your responsibilities:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Develop innovative computational algorithms for Robustness and Uncertainty in both discriminative (Spatio-temporal image segmentation) and generative (synthetic CT generation) deep learning-based models in MR-guided radiotherapy.&lt;/li&gt;
&lt;li&gt;Publish and present scientific outcomes at Intl. conferences and high-impact journals&lt;/li&gt;
&lt;li&gt;Maintain close collaboration with the team members and clinical partners&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Your qualifications:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ph.D. in Computer Science, Machine Learning, or equivalent with an interest in Medical Imaging&lt;/li&gt;
&lt;li&gt;Strong knowledge in Machine/Deep Learning with experience in discriminative and generative models, robustness, and uncertainty quantification.&lt;/li&gt;
&lt;li&gt;Track record of publications at top-tier conferences in the field (e.g., MICCAI, IPMI, and MIDL) and high-impact journals in the field (e.g., Nat. Mach. Intell., MedIA, and IEEE TMI)&lt;/li&gt;
&lt;li&gt;Excellent programming skills in Python, PyTorch, and MONAI including fundamental software engineering principles and machine learning design patterns&lt;/li&gt;
&lt;li&gt;Working in a Linux environment, with experience in shell and cluster (SLURM) scripting&lt;/li&gt;
&lt;li&gt;Excellent analytical, technical, and problem-solving skills&lt;/li&gt;
&lt;li&gt;Be highly motivated and a team player with excellent communication and presentation skills, including experience in communicating across discipline boundaries&lt;/li&gt;
&lt;li&gt;Fluent command of the English language&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;What we offer you:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;A secure future:&lt;/strong&gt; remuneration according to the German salary scale TV-L (E13 - 100%)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flexible for families:&lt;/strong&gt; flexible working time, home office, onsite nursery, and parental care.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Provisions for later:&lt;/strong&gt; company pension scheme&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Discounted public transport ticket:&lt;/strong&gt; discounted ticket for public transport (VRS)
on-site health management service: Numerous health promotion offers&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Employer benefits:&lt;/strong&gt; Discounted offers for employees&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Subsidized continuing education and training&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The University of Bonn is committed to diversity and equal opportunity and is certified as a family-friendly university. It aims to increase the proportion of women in areas where women are under-represented and to promote their careers in particular. Therefore, we strongly encourage applications from qualified women. Applications will be handled in accordance with the State Equality Act (Landesgleichstellungsgesetz). Applications from individuals with a certified severe disability and from those of equal status are particularly welcome.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contact:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you meet the requirements and you are looking for a challenging job? Do not hesitate and send your application including a cover letter (highlighting your qualifications), a detailed CV (with links to previous projects and code), scanned academic degrees, and the contact details of two referees (preferably by e-mail in a single PDF file up to 5 MB in size) by &lt;strong&gt;15th January 2023&lt;/strong&gt;, quoting the job advertisement no. CIR/02/2023 in your email’s subject to 
&lt;a href=&#34;mailto:shadi.albarqouni@ukbonn.de&#34;&gt;Prof. Dr. Shadi Albarqouni&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CIR/03/2023: Medical Research Assistant (m/f/d)</title>
      <link>https://albarqouni.github.io/positions/mra_cir.03.2023/</link>
      <pubDate>Thu, 15 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/positions/mra_cir.03.2023/</guid>
      <description>&lt;p&gt;We combine excellence in research, teaching, and patient care. The University Hospital Bonn (UKB) is a maximum care hospital with more than 1,300 beds. With around 38 clinics and 31 institutes as well as more than 8,000 employees (over 5,000 full-time staff), the UKB is one of the largest employers in Bonn. Every year, the UKB treats around 50,000 inpatients and around 35,000 emergencies, as well as provides over 350,000 outpatient treatments.&lt;/p&gt;
&lt;p&gt;The following &lt;strong&gt;part-time (8 hrs./week)&lt;/strong&gt; Medical Research Assistant is available at the &lt;strong&gt;Computational Imaging Research (CIR) Lab&lt;/strong&gt;, led by Prof. Dr. Shadi Albarqouni, in the &lt;strong&gt;Clinic for Diagnostic and Interventional Radiology&lt;/strong&gt; of the University Hospital Bonn, University of Bonn:&lt;/p&gt;
&lt;h2 id=&#34;medical-research-assistant-mfd&#34;&gt;&lt;strong&gt;Medical Research Assistant (m/f/d)&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;starting January 2023 or as agreed upon. The position is initially limited to two years, with the possibility of extension.&lt;/p&gt;
&lt;p&gt;The medical research assistant position will be based in the newly founded research lab for Computational Imaging Research (CIR), which aims to develop i) fully automated, highly accurate innovative computational methods that save expert labor and efforts, and mitigate the challenges in medical imaging; namely the availability of a few annotated data, low inter-/intra-observers agreement, inter-/intra-scanners variability and domain shift, ii) innovative deep Federated Learning algorithms that can fairly distill and share the knowledge among AI agents in a robust and privacy-preserved way, and iii) affordable AI algorithms suitable for low-quality data generated by low-resource settings and point-of-care devices. The medical research assistant is the key component in developing the next generation of AI in Medicine by providing high-quality annotations for medical imaging and radiology/pathology reports.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Your responsibilities:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Assisting the team members in the data collection process&lt;/li&gt;
&lt;li&gt;Annotating the collected imaging data for various projects using readily available tools&lt;/li&gt;
&lt;li&gt;Performing quality control of annotated images collected by different partners&lt;/li&gt;
&lt;li&gt;Collaboration with team members and clinical partners must be maintained&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Your qualifications:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Medical Doctorate (MD) degree&lt;/li&gt;
&lt;li&gt;Passionate about Machine/Deep Learning in Medical Imaging&lt;/li&gt;
&lt;li&gt;Be highly motivated and a team player with excellent communication and presentation skills, including experience in communicating across discipline boundaries&lt;/li&gt;
&lt;li&gt;Fluent command of the English language&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;What we offer you:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;A secure future:&lt;/strong&gt; remuneration according to the German salary scale TV-Ä (Ä1/Ä2 &amp;ndash; 20%)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flexible for families:&lt;/strong&gt; flexible working time, home office, onsite nursery, and parental care.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Provisions for later:&lt;/strong&gt; company pension scheme&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Discounted public transport ticket:&lt;/strong&gt; discounted ticket for public transport (VRS)
on-site health management service: Numerous health promotion offers&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Employer benefits:&lt;/strong&gt; Discounted offers for employees&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Subsidized continuing education and training&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The University of Bonn is committed to diversity and equal opportunity and is certified as a family-friendly university. It aims to increase the proportion of women in areas where women are under-represented and to promote their careers in particular. Therefore, we strongly encourage applications from qualified women. Applications will be handled in accordance with the State Equality Act (Landesgleichstellungsgesetz). Applications from individuals with a certified severe disability and from those of equal status are particularly welcome.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contact:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you meet the requirements and you are looking for a challenging job? Do not hesitate and send your application including a cover letter (highlighting your qualifications), a detailed CV (with links to previous projects and code), scanned academic degrees, and the contact details of two referees (preferably by e-mail in a single PDF file up to 5 MB in size) by &lt;strong&gt;15th January 2023&lt;/strong&gt;, quoting the job advertisement no. CIR/03/2023 in your email’s subject to 
&lt;a href=&#34;mailto:shadi.albarqouni@ukbonn.de&#34;&gt;Prof. Dr. Shadi Albarqouni&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MA Thesis: Deep Learning based model for detection and grading of prostate cancer using mpMRI and MR-Fingerprinting</title>
      <link>https://albarqouni.github.io/students/theses/ma_thesis_automatic_detection_and_grading_of_prostate_cancer/</link>
      <pubDate>Thu, 15 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/students/theses/ma_thesis_automatic_detection_and_grading_of_prostate_cancer/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract.&lt;/strong&gt; Prostate cancer (PCa) is the most common cancer in men and the second leading cause of cancer death in Germany  [4,14]. Both digital rectal examination (DRE) along with the prostate-specific antigen (PSA) level in blood samples are typically used in PCa screening. Altogether, about 40% of males in Western industrialized countries have the risk of developing PCa during their lifetime, of whom only about 10% become symptomatic and 3% die [15,16]. To determine the clinical significance of PCa, prostate biopsies are assessed histologically. Prostate lesions with a pathology/histology of Gleason score ≥ 7, and/or volume ≥ 0.5 cc, and/or extraprostatic extension (EPE) are commonly considered clinically significant Prostate Cancer (csPCa).  Readers are referred to  [17,18] for further details about the acquisition and the interpretation.&lt;/p&gt;
&lt;p&gt;Treatment options of active surveillance, surgery, and/or radiotherapy are determined, accordingly. Multiparametric magnetic resonance imaging (mpMRI) has been increasingly utilized for the detection and staging of csPCa. The PI-RADS 2.1 scoring system [1][2] was introduced to standardize the image acquisition and interpretation (scoring) of csPCa. Recently, Alice et al. [5], Lo et al. [6], and Panda et al. [13], among others, have shown that quantitative characterization of prostate lesions can be successfully performed using diffusion MRI and Magnetic Resonance Fingerprinting (MRF) [3]. MRF represents an MRI sequence with a novel data acquisition, post-processing, and visualization approach. A pseudorandomized acquisition pattern with the variation of flip angle, repetition time and echo time within a scan allows the measurement of specific signal patterns, so-called &amp;ldquo;fingerprints&amp;rdquo;, which, via matching with a database (&amp;ldquo;dictionary&amp;rdquo;), enable the simultaneous generation of co-registered, multiparametric quantitative maps, based on T1-, T2- and T2-relaxation times. In this project, we will investigate developing a data-driven deep learning-based model to automatically detect and stage the csPCa cases using the MRF-based relaxometry, and potentially combined it with the mpMRI examination, including high b-value imaging and apparent diffusion coefficient mapping (ADC).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Objectives:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Given a database of morphological high-resolution T2w images (in axial and sagittal orientation), diffusion MRI images including a high b-value image of 1500 and ADC mapping as well as MRF sequences and their corresponding pixel-wise delineation of the Prostate lesions, the PI-RADS v2.1 scoring (1-5), and histopathological grading - if available - we aim to develop a deep learning (DL) model to:
&lt;ul&gt;
&lt;li&gt;segment the prostate lesions&lt;/li&gt;
&lt;li&gt;automatically detect and stage the prostate lesions in csPCa, and&lt;/li&gt;
&lt;li&gt;eventually, predict the histopathological grading (Gleason score) – if time allows&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Run a comparative analysis between the DL models trained on T2w, high b-value DWI, and ADC, and the one trained on MRF sequences - potentially combined with the high b-value DWI and ADC image.&lt;/li&gt;
&lt;li&gt;Report relevant evaluation metrics such as the Dice Coefficient, and Area Under the Precision-Recall Curve (AUPRC).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Dataset:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cohort: Up to date, 171 patients with elevated PSA levels have been included in this ongoing study.&lt;/li&gt;
&lt;li&gt;MRI: Patients received an MRI examination on a 3T MRI scanner (Philips Ingenia) including the following sequences:
&lt;ul&gt;
&lt;li&gt;High-resolution T2-weighted sequences in axial and sagittal orientation&lt;/li&gt;
&lt;li&gt;DWI, including b-values of 100, 400, and 800 with the calculation of an ADC map, and a high b-value of 1500&lt;/li&gt;
&lt;li&gt;Magnetic Resonance Fingerprinting (MRF)&lt;/li&gt;
&lt;li&gt;Perfusion imaging&lt;/li&gt;
&lt;li&gt;Pre- and post-contrast T1-weighted axial images of the pelvis&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Data annotation: Axial high-resolution T2-weighted images are labeled by a radiology resident with 4 years of experience to include the following labels: i) Peripheral Zone, ii) Transitional Zone, iii) Lesion&lt;/li&gt;
&lt;li&gt;Histopathological evaluation: Patients that were graded with a PI-RADS score of 3 or higher will receive a systematic/targeted biopsy that allows histopathological correlation with lesions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Roadmap:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Familiarize yourself with the current literature on&lt;/li&gt;
&lt;li&gt;Radiomics model for prostate [10] and its repeatability with MRF [12]&lt;/li&gt;
&lt;li&gt;Deep Learning with MRF in Parameter Estimation [7-9], Correlation with Histopathology [11], and lesions catheterization [13]&lt;/li&gt;
&lt;li&gt;Develop the baseline and proposed method&lt;/li&gt;
&lt;li&gt;Run extensive experiments and analysis&lt;/li&gt;
&lt;li&gt;Write up your thesis&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Solid background in Machine/Deep Learning&lt;/li&gt;
&lt;li&gt;Familiar with discriminative deep learning models and SOTA architectures&lt;/li&gt;
&lt;li&gt;Sufficient knowledge of Python programming language and libraries (Scikit-learn)&lt;/li&gt;
&lt;li&gt;Experience with a mainstream deep learning framework such as PyTorch.&lt;/li&gt;
&lt;li&gt;Machine/Deep learning hands-on experience&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;
&lt;font size = &#34;2&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Steiger, P. and Thoeny, H.C., 2016. Prostate MRI based on PI-RADS version 2: how we review and report. Cancer Imaging, 16(1), pp.1-9.&lt;/li&gt;
&lt;li&gt;Turkbey, B., Rosenkrantz, A.B., Haider, M.A., Padhani, A.R., Villeirs, G., Macura, K.J., Tempany, C.M., Choyke, P.L., Cornud, F., Margolis, D.J. and Thoeny, H.C., 2019. Prostate imaging reporting and data system version 2.1: 2019 update of prostate imaging reporting and data system version 2. European urology, 76(3), pp.340-351.&lt;/li&gt;
&lt;li&gt;Ma, D., Gulani, V., Seiberlich, N., Liu, K., Sunshine, J.L., Duerk, J.L. and Griswold, M.A., 2013. Magnetic resonance fingerprinting. Nature, 495(7440), pp.187-192.&lt;/li&gt;
&lt;li&gt;Ferlay J, Ervik M, Lam F, Colombet M, Mery L, Piñeros M, Znaor A, Soerjomataram I, Bray F (2020). Global Cancer Observatory: Cancer Today. Lyon, France: International Agency for Research on Cancer. Available from: 
&lt;a href=&#34;https://gco.iarc.fr/today&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gco.iarc.fr/today&lt;/a&gt;, accessed [01 July 2022]&lt;/li&gt;
&lt;li&gt;Alice, C.Y., Badve, C., Ponsky, L.E., Pahwa, S., Dastmalchian, S., Rogers, M., Jiang, Y., Margevicius, S., Schluchter, M., Tabayoyong, W. and Abouassaly, R., 2017. Development of a combined MR fingerprinting and diffusion examination for prostate cancer. Radiology, 283(3), p.729.&lt;/li&gt;
&lt;li&gt;Lo, W.C., Panda, A., Jiang, Y., Ahad, J., Gulani, V. and Seiberlich, N., 2022. MR fingerprinting of the prostate. Magnetic Resonance Materials in Physics, Biology and Medicine, pp.1-15.&lt;/li&gt;
&lt;li&gt;Hoppe, E., Körzdörfer, G., Würfl, T., Wetzl, J., Lugauer, F., Pfeuffer, J. and Maier, A.K., 2017. Deep Learning for Magnetic Resonance Fingerprinting: A New Approach for Predicting Quantitative Parameter Values from Time Series. GMDS, 243, pp.202-206.&lt;/li&gt;
&lt;li&gt;Girardeau, S., Oksuz, I., Cruz, G., Vasquez, C.P., King, A. and Clough, J., 2019, April. Deep Learning for Magnetic Resonance Fingerprinting. In International Conference on Medical Imaging with Deep Learning&amp;ndash;Extended Abstract Track.&lt;/li&gt;
&lt;li&gt;Golbabaee, M., Chen, D., Gómez, P.A., Menzel, M.I. and Davies, M.E., 2019, May. Geometry of deep learning for magnetic resonance fingerprinting. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 7825-7829). IEEE.&lt;/li&gt;
&lt;li&gt;Abdollahi, H., Mofid, B., Shiri, I., Razzaghdoust, A., Saadipoor, A., Mahdavi, A., Galandooz, H.M. and Mahdavi, S.R., 2019. Machine learning-based radiomic models to predict intensity-modulated radiation therapy response, Gleason score and stage in prostate cancer. La radiologia medica, 124(6), pp.555-567.&lt;/li&gt;
&lt;li&gt;Shiradkar, R., Panda, A., Leo, P., Janowczyk, A., Farre, X., Janaki, N., Li, L., Pahwa, S., Mahran, A., Buzzy, C. and Fu, P., 2021. T1 and T2 MR fingerprinting measurements of prostate cancer and prostatitis correlate with deep learning–derived estimates of epithelium, lumen, and stromal composition on corresponding whole mount histopathology. European radiology, 31(3), pp.1336-1346.&lt;/li&gt;
&lt;li&gt;Fujita, S., Hagiwara, A., Yasaka, K., Akai, H., Kunimatsu, A., Kiryu, S., Fukunaga, I., Kato, S., Akashi, T., Kamagata, K. and Wada, A., 2022. Radiomics with 3-dimensional magnetic resonance fingerprinting: influence of dictionary design on repeatability and reproducibility of radiomic features. European Radiology, pp.1-10.&lt;/li&gt;
&lt;li&gt;Panda, A., Obmann, V.C., Lo, W.C., Margevicius, S., Jiang, Y., Schluchter, M., Patel, I.J., Nakamoto, D., Badve, C., Griswold, M.A. and Jaeger, I., 2019. MR fingerprinting and ADC mapping for characterization of lesions in the transition zone of the prostate gland. Radiology, 292(3), p.685.&lt;/li&gt;
&lt;li&gt;Krebs in Deutschland für 2017/2018. 13. Ausgabe. Robert Koch-Institut (Hrsg) und die Gesellschaft der epidemiologischen Krebsregister in Deutschland e.V. (Hrsg). Berlin, 2021&lt;/li&gt;
&lt;li&gt;Bott SR, Birtle AJ, Taylor CJ et al. Prostate cancer management: (1) an update on localised disease. Postgrad Med J 2003; 79: 575-580. doi:10.1136/pmj.79.936.575&lt;/li&gt;
&lt;li&gt;Leitlinienprogramm Onkologie (Deutsche Krebsgesellschaft, Deutsche Krebshilfe, AWMF). Interdisziplinäre Leitlinie der Qualität S3 zur Früherkennung, Diagnose und Therapie der verschiedenen Stadien des Prostatakarzinoms, Langversion 5.1, AWMF Registernummer: 043/022OL. 
&lt;a href=&#34;http://www.leitlinienprogramm-onkologie.de/leitlinien/prostatakarzinom/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.leitlinienprogramm-onkologie.de/leitlinien/prostatakarzinom/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Engels, R.R., Israël, B., Padhani, A.R. and Barentsz, J.O., 2020. Multiparametric magnetic resonance imaging for the detection of clinically significant prostate cancer: what urologists need to know. Part 1: acquisition. European urology, 77(4), pp.457-468.&lt;/li&gt;
&lt;li&gt;Israel, B., van der Leest, M., Sedelaar, M., Padhani, A.R., Zamecnik, P. and Barentsz, J.O., 2020. Multiparametric magnetic resonance imaging for the detection of clinically significant prostate cancer: what urologists need to know. Part 2: interpretation. European urology, 77(4), pp.469-480.&lt;/li&gt;
&lt;/ol&gt;
&lt;/font&gt;
&lt;p&gt;Interested, please contact 
&lt;a href=&#34;mailto:shadi.albarqouni@ukbonn.de&#34;&gt;Prof. Dr. Shadi Albarqouni&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MA Thesis: Deep Learning-based method for virtual ECV in cardiac magnetic resonance imaging</title>
      <link>https://albarqouni.github.io/students/theses/ma_thesis_deep_learning-based_method_for_virtual_ecv_in_cardiac_magnetic_resonance_imaging/</link>
      <pubDate>Thu, 15 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/students/theses/ma_thesis_deep_learning-based_method_for_virtual_ecv_in_cardiac_magnetic_resonance_imaging/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract.&lt;/strong&gt; Diseases of the cardiovascular system are among the most common diseases worldwide and are the leading cause of death. The World Health Organization (WHO) estimates that about 17.9 million people die of cardiovascular diseases each year worldwide. In particular, diseases of the heart muscle, for example in the context of a heart attack or myocarditis, are of great relevance here. In order to be able to examine such diseases in a targeted manner and as gently as possible, the MRI examination of the heart has become increasingly established and further developed over the last decades. It captures the entire heart non-invasively and provides an important basis for further more invasive examinations, such as cardiac catheterization or myocardial biopsy. However, in order to make reliable diagnoses, the administration of contrast media is inevitable. After the administration of the contrast medium, it is possible to distinguish exactly which areas of the heart malfunctioned (See the right figure – taken from [1]). To reduce the risks of such contrast administration, we would like to develop a DL-based algorithm in this study that can automatically generate extracellular volume maps (ECV) without the need for post-contrast images [8].&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Objectives:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Given the pre-contrast native T1-mapping, their corresponding post-contrast T1-mapping, and the segmentation of the myocardium, perform the registration between pre-contrast and post-contrast imaging and calculate the extracellular volume (ECV) map, accordingly. For cases where HtK values are missing or outdated, HtK could be estimated by the native T1 relaxation time of blood by applying a regression model [7].&lt;/li&gt;
&lt;li&gt;Given a database of the pre-contrast native T1-mapping, and their corresponding post-contrast T1-mapping and the computed/calculated extracellular volume (ECV) images of the patients’ cohort, we aim, with the power of deep learning, to model and generate virtual extracellular volume (ECV) maps. Different diseases such as acute myocarditis (Fig. 1a), Takutsubo cardiomyopathy (Fig. 1b), acute myocardial infarction, and chronic myocardial infarction show a distinctive pattern (Fig. 1c-e).&lt;/li&gt;
&lt;li&gt;To avoid the requirements of one-to-one correspondences, the proposed algorithm should be trained on unpaired data. CycleGAN approach [2,3] or enhanced version with the perceptual embedding consistency [4] could be investigated in this context.&lt;/li&gt;
&lt;li&gt;The virtual extracellular volume (ECV) should be evaluated against the ground-truth ECV images via common evaluation metrics such as Mean Square Error (MSE), Peak-Signal-to-Noise-Ratio (PSNR), Mean Absolute Error (MAE), Structural Similarity Index (SSIM), and relevant clinical measures such as the post-contrast T1-relaxation-time and the ECV.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Data cohort:&lt;/strong&gt;
Pre- and post-contrast cardiac T1 relaxation time maps from a total of 1086 subjects are available for this project. For each subject, 3 short-axis images were acquired at a field strength of 1.5 Tesla at the University Hospital Bonn. All of these maps have already been assessed for image quality by a radiology resident, and the total number of slices with good or moderate image quality is 2472, from 922 different patients. In 92 of these patients, a Htk value was obtained with a time difference of less than 48h to the examination, in 511 patients within 30 days.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Roadmap:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Familiarize yourself with the current literature on:&lt;/li&gt;
&lt;li&gt;Cardiac Imaging and Extracellular Volume Mapping [1,8]&lt;/li&gt;
&lt;li&gt;Image-to-Image Translation [2,6]&lt;/li&gt;
&lt;li&gt;Image-to-Image Translation in Medical Domain [3,4,5]&lt;/li&gt;
&lt;li&gt;Develop the baseline and proposed method&lt;/li&gt;
&lt;li&gt;Run extensive experiments and analysis&lt;/li&gt;
&lt;li&gt;Write up your thesis&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Solid background in Machine/Deep Learning&lt;/li&gt;
&lt;li&gt;Familiar with discriminative deep learning models and SOTA architectures&lt;/li&gt;
&lt;li&gt;Sufficient knowledge of Python programming language and libraries (Scikit-learn)&lt;/li&gt;
&lt;li&gt;Experience with a mainstream deep learning framework such as PyTorch.&lt;/li&gt;
&lt;li&gt;Machine/Deep learning hands-on experience&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;
&lt;font size = &#34;2&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[1] Haaf, P., Garg, P., Messroghli, D.R., Broadbent, D.A., Greenwood, J.P. and Plein, S., 2017. Cardiac T1 mapping and extracellular volume (ECV) in clinical practice: a comprehensive review. Journal of Cardiovascular Magnetic Resonance, 18(1), pp.1-12.&lt;/li&gt;
&lt;li&gt;[2] Zhu, J.Y., Park, T., Isola, P. and Efros, A.A., 2017. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision (pp. 2223-2232).&lt;/li&gt;
&lt;li&gt;[3] Shaban, M.T., Baur, C., Navab, N. and Albarqouni, S., 2019, April. Staingan: Stain style transfer for digital histological images. In 2019 Ieee 16th international symposium on biomedical imaging (Isbi 2019) (pp. 953-956). IEEE.&lt;/li&gt;
&lt;li&gt;[4] Lahiani, A., Navab, N., Albarqouni, S. and Klaiman, E., 2019, October. Perceptual embedding consistency for seamless reconstruction of tilewise style transfer. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 568-576). Springer, Cham.&lt;/li&gt;
&lt;li&gt;[5] Zhang, Q., Burrage, M.K., Lukaschuk, E., Shanmuganathan, M., Popescu, I.A., Nikolaidou, C., Mills, R., Werys, K., Hann, E., Barutcu, A. and Polat, S.D., 2021. Toward replacing late gadolinium enhancement with artificial intelligence virtual native enhancement for gadolinium-free cardiovascular magnetic resonance tissue characterization in hypertrophic cardiomyopathy. Circulation, 144(8), pp.589-599.&lt;/li&gt;
&lt;li&gt;[6] Xu, Y., Xie, S., Wu, W., Zhang, K., Gong, M. and Batmanghelich, K., 2022. Maximum Spatial Perturbation Consistency for Unpaired Image-to-Image Translation. CVPR 2022. arXiv preprint arXiv:2203.12707. 
&lt;a href=&#34;https://github.com/batmanlab/MSPC&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/batmanlab/MSPC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[7] Mesropyan, N., Kupczyk, P., Isaak, A., Endler, C., Faron, A., Dold, L., Sprinkart, A.M., Pieper, C.C., Kuetting, D., Attenberger, U. and Luetkens, J.A., 2021. Synthetic extracellular volume fraction without hematocrit sampling for hepatic applications. Abdominal Radiology, 46(10), pp.4637-4646.&lt;/li&gt;
&lt;li&gt;[8] Chen, W., Doeblin, P., Al-Tabatabaee, S., Klingel, K., Tanacli, R., Jakob Weiß, K., Stehning, C., Patel, A.R., Pieske, B., Zou, J. and Kelle, S., 2022. Synthetic Extracellular Volume in Cardiac Magnetic Resonance Without Blood Sampling: a Reliable Tool to Replace Conventional Extracellular Volume. Circulation: Cardiovascular Imaging, 15(4), p.e013745.&lt;/li&gt;
&lt;/ul&gt;
&lt;/font&gt;
&lt;p&gt;Interested, please contact 
&lt;a href=&#34;mailto:shadi.albarqouni@ukbonn.de&#34;&gt;Prof. Dr. Shadi Albarqouni&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>FLamby: Datasets and Benchmarks for Cross-Silo Federated Learning in Realistic Healthcare Settings</title>
      <link>https://albarqouni.github.io/publication/terrail-2022-flamby/</link>
      <pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/terrail-2022-flamby/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Course: Introduction to Machine Learning</title>
      <link>https://albarqouni.github.io/students/courses/iml2022/</link>
      <pubDate>Sat, 15 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/students/courses/iml2022/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Lecturer:&lt;/strong&gt; 
&lt;a href=&#34;../../#about&#34;&gt;Prof. Dr. Shadi Albarqouni&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Class timing:&lt;/strong&gt; The 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Academic_quarter_%28class_timing%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;academic quarter&lt;/a&gt; is applied here! This means the lecture will start 15 minutes after the said time.&lt;/p&gt;
&lt;h3 id=&#34;course-description&#34;&gt;Course Description&lt;/h3&gt;
&lt;p&gt;Machine Learning has gained a lot of momentum within development organizations that are actively looking for innovative solutions to leverage their data to identify new levels of understanding their operations and processes. Machine learning is a subfield of Artificial Intelligence where the machine learns from data rather than from explicit programming. Machine Learning applications are likely to be part of your daily life without you being aware of it! Take 
&lt;a href=&#34;https://support.google.com/recaptcha&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reCAPTCHA&lt;/a&gt; as an example that you might face on a daily basis! This course will walk you through the basics of machine learning and explain how to formulate machine learning problems that are relevant to a wide range of applications. The class will be self-contained; a brief review on probability and information theory will be given before we dive into the main topics; linear and logistic regression, neural networks, non-parametric models, and unsupervised learning.&lt;/p&gt;
&lt;h3 id=&#34;course-objectives&#34;&gt;Course Objectives&lt;/h3&gt;
&lt;p&gt;Successful students should&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;gain a basic understanding of machine learning theory&lt;/li&gt;
&lt;li&gt;identify the strengths and weaknesses of various machine learning algorithms&lt;/li&gt;
&lt;li&gt;be able to formulate machine learning problems that are relevant to a wide range of applications&lt;/li&gt;
&lt;li&gt;be able to solve moderately complex problems using machine learning algorithms&lt;/li&gt;
&lt;li&gt;be able to apply, adapt and optimize machine learning algorithms to real-world problems&lt;/li&gt;
&lt;li&gt;be able to report various performance measures and evaluation metrics&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;required-materials&#34;&gt;Required Materials&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Murphy, K.P., 2022. 
&lt;a href=&#34;https://github.com/probml/pml-book/releases/latest/download/book1.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Probabilistic machine learning: an introduction&lt;/a&gt;. MIT press.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Linear Algebra&lt;/li&gt;
&lt;li&gt;Probability Theory&lt;/li&gt;
&lt;li&gt;Programming in Python&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;course-structure-and-grading-policy-tentative&#34;&gt;Course Structure and grading policy (tentative):&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Assignments/Quizzes (10%)&lt;/strong&gt;: You will be occasionally given assigments or quizzes which contribute to 10% of your grade&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lab. (20%)&lt;/strong&gt;: You will be given a few lab assignments to have hands-on experience with programming in python&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Class Project/Midterm Exam (20%)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Final Exam (50%)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;schedule&#34;&gt;Schedule&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;em&gt;Date&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Topic&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Material&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Self-reading&lt;/em&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;29.09.2022&lt;/td&gt;
&lt;td&gt;Introduction (Ch1)&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;Lect01.pdf&#34;&gt;Lecture01&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;06.10.2022&lt;/td&gt;
&lt;td&gt;Foundations: Probability (Ch2)&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;Lect02.pdf&#34;&gt;Lecture02&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Statistics (Ch4) + Decicion Theory (Ch5)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;13.10.2022&lt;/td&gt;
&lt;td&gt;Foundations: Probability (Ch3)&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;Assignment01.pdf&#34;&gt;Assignment01&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Information Theory (Ch6) + Linear Algebra (Ch7) + Optimization (Ch8)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20.10.2022&lt;/td&gt;
&lt;td&gt;Linear Models: Logistic Regression (Ch10)&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;Lect03.pdf&#34;&gt;Lecture03&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;27.10.2022&lt;/td&gt;
&lt;td&gt;Linear Models: Logistic Regression (Ch10) &amp;ndash; Cont.&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;Assignment02.pdf&#34;&gt;Assignment02&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Linear Discriminative Analysis (Ch9)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;03.11.2022&lt;/td&gt;
&lt;td&gt;Linear Models: Linear Regression (Ch11)&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;Lect04.pdf&#34;&gt;Lecture04&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Generalized Linear Models (Ch12)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10.11.2022&lt;/td&gt;
&lt;td&gt;Linear Models: Linear Regression (Ch11) &amp;ndash; Cont.&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;Lab01.pdf&#34;&gt;Lab01&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Generalized Linear Models (Ch12)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;17.11.2022&lt;/td&gt;
&lt;td&gt;Mid-term Evaluation&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;01.12.2022&lt;/td&gt;
&lt;td&gt;Neural Networks: NNs for Tabular Data (Ch13)&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;Lect05.pdf&#34;&gt;Lecture05&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;08.12.2022&lt;/td&gt;
&lt;td&gt;Neural Networks: NNs for Tabular Data (Ch13) &amp;ndash; Cont. + Lab&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;Lab02.pdf&#34;&gt;Lab02&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;15.12.2022&lt;/td&gt;
&lt;td&gt;Neural Networks: NNs for Imaging Data (Ch14)&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;Lect06.pdf&#34;&gt;Lecture06&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;22.12.2022&lt;/td&gt;
&lt;td&gt;Beyond Supervised Learning: Dimensionality Reduction (Ch20) + Clustering (Ch21)&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;Lect07.pdf&#34;&gt;Lecture07&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Recommender Systems (Ch22)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;29.12.2022&lt;/td&gt;
&lt;td&gt;Hands-on Experience on the projects&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;05.01.2023&lt;/td&gt;
&lt;td&gt;Course Recap.&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;Lect08.pdf&#34;&gt;Lecture08&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;19.01.2023&lt;/td&gt;
&lt;td&gt;Final Project Evaluation&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Bishop, C.M. and Nasrabadi, N.M., 2006. 
&lt;a href=&#34;https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pattern recognition and machine learning&lt;/a&gt; (Vol. 4, No. 4, p. 738). New York: springer.&lt;/li&gt;
&lt;li&gt;James, G., Witten, D., Hastie, T. and Tibshirani, R., 2013. 
&lt;a href=&#34;https://link.springer.com/content/pdf/10.1007/978-1-0716-1418-1.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An introduction to statistical learning&lt;/a&gt; (Vol. 112, p. 18). New York: springer.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;projects&#34;&gt;Projects&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/Statlog&amp;#43;%28German&amp;#43;Credit&amp;#43;Data%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;German Credit Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/Heart&amp;#43;Disease&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Heart Disease Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/Divorce&amp;#43;Predictors&amp;#43;data&amp;#43;set&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Divorce Predictors Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/Water&amp;#43;Quality&amp;#43;Prediction&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Water Quality Prediction Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/Early&amp;#43;stage&amp;#43;diabetes&amp;#43;risk&amp;#43;prediction&amp;#43;dataset.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Early stage diabetes risk prediction Dataset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;interested-in-the-medical-imaging-community&#34;&gt;Interested in the Medical Imaging community!&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Listen to our recent talks about Federated Learning in Healthcare (
&lt;a href=&#34;https://www.youtube.com/watch?v=ZQiyH0tlnwM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;English&lt;/a&gt; /
&lt;a href=&#34;https://www.youtube.com/watch?v=mDU16ZMPTXw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Arabic&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Join and contribute to the 
&lt;a href=&#34;../../slides/MONAI_Slides_Master_v2.pdf&#34;&gt;Medical Open Network for AI (MONAI)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contact-the-organizers&#34;&gt;Contact the organizers&lt;/h3&gt;
&lt;p&gt;If you have any questions regarding the course, please do not hesitate to contact 
&lt;a href=&#34;../../#about&#34;&gt;Prof. Dr. Shadi Albarqouni&lt;/a&gt;!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Joint Self-Supervised Image-Volume Representation Learning with Intra-Inter Contrastive Clustering</title>
      <link>https://albarqouni.github.io/publication/nguyen-2022-joint/</link>
      <pubDate>Mon, 10 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/nguyen-2022-joint/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Invited Talk at the Workshop on Collaborative Learning: From Theory to Practice</title>
      <link>https://albarqouni.github.io/talk/mbzuai/</link>
      <pubDate>Sat, 08 Oct 2022 09:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/talk/mbzuai/</guid>
      <description>&lt;p&gt;I had the pleasure to give an invited talk at the #Collaborative Learning workshop at MBZUAI (Mohamed bin Zayed University of Artificial Intelligence)! It was a wonderful weekend full of amazing talks and fruitful discussions! I had the pleasure to meet a few familiar faces in our community along with other great speakers from UC Berkeley, Harvard, MIT, KAUST, ETH Zurich, Nvidia, and EPFL, among others. I would like to thank Michael I. Jordan and the organizing team behind the workshop for the invitation and the excellent hospitality! For those who are interested in the talks, they will be made publicly available soon!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Federated disentangled representation learning for unsupervised brain anomaly detection</title>
      <link>https://albarqouni.github.io/publication/bercea-2022-federated/</link>
      <pubDate>Sat, 01 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/bercea-2022-federated/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Invited Talk at AI 4 Imaging</title>
      <link>https://albarqouni.github.io/talk/ai4i2022/</link>
      <pubDate>Wed, 29 Jun 2022 09:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/talk/ai4i2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Attending the general assembly of the Arab German Young Academy</title>
      <link>https://albarqouni.github.io/talk/agya2022/</link>
      <pubDate>Thu, 23 Jun 2022 09:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/talk/agya2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Organizing the 2nd version of the MICCAI Workshop on aFfordable AI and healthcare</title>
      <link>https://albarqouni.github.io/talk/fair2022/</link>
      <pubDate>Thu, 23 Jun 2022 09:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/talk/fair2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Federated Learning</title>
      <link>https://albarqouni.github.io/codes/federated-learning/</link>
      <pubDate>Thu, 23 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/codes/federated-learning/</guid>
      <description>&lt;p&gt;
&lt;a href=&#34;https://github.com/owkin/FLamby&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Flamby: Datasets and Benchmarks for Cross-Silo Federated Learning in Realistic Healthcare Settings&lt;/a&gt;
Accepted paper at NeurIPS&#39;22&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/tbdair/FedPerlV1.0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FedPerl: Semi-Supervised Peer Learning for Skin Lesion Classification&lt;/a&gt;
Accepted paper at MICCAI&#39;21 and extended to MELBA&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/albarqounilab/FedDis-NMI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FedDis: Disentangled Federated Learning for Unsupervised Brain Pathology Segmentation&lt;/a&gt;
Accepted paper at MICCAI&#39;21 DCL and extended to a journal&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/albarqounilab/FedNorm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FedNorm: Modality-Based Normalization in Federated Learning for Multi-Modal Liver Segmentation&lt;/a&gt;
Under Review&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Co-Organizing the 10. DFG-#Nachwuchsakademie</title>
      <link>https://albarqouni.github.io/talk/aim2022/</link>
      <pubDate>Sun, 23 Jan 2022 09:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/talk/aim2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Anomaly-aware multiple instance learning for rare anemia disorder classification</title>
      <link>https://albarqouni.github.io/publication/kazeminia-2022-anomaly/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/kazeminia-2022-anomaly/</guid>
      <description></description>
    </item>
    
    <item>
      <title>FedNorm: Modality-Based Normalization in Federated Learning for Multi-Modal Liver Segmentation</title>
      <link>https://albarqouni.github.io/publication/bernecker-2022-fednorm/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/bernecker-2022-fednorm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ROAM: Random layer mixup for semi-supervised learning in medical images</title>
      <link>https://albarqouni.github.io/publication/bdair-2022-roam/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/bdair-2022-roam/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Semi-Supervised Federated Peer Learning for Skin Lesion Classification</title>
      <link>https://albarqouni.github.io/publication/melba-2022-011-bdair/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/melba-2022-011-bdair/</guid>
      <description></description>
    </item>
    
    <item>
      <title>TriMix: Virtual embeddings and self-consistency for self-supervised learning</title>
      <link>https://albarqouni.github.io/publication/bdair-2022-trimix/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/bdair-2022-trimix/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Unsupervised Cross-Domain Feature Extraction for Single Blood Cell Image Classification</title>
      <link>https://albarqouni.github.io/publication/salehi-2022-unsupervised/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/salehi-2022-unsupervised/</guid>
      <description></description>
    </item>
    
    <item>
      <title>What can we learn about a generated image corrupting its latent representation?</title>
      <link>https://albarqouni.github.io/publication/tomczak-2022-can/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/tomczak-2022-can/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Organizing a workshop on the Next Generation of AI in Medicine</title>
      <link>https://albarqouni.github.io/talk/hida2021/</link>
      <pubDate>Tue, 30 Nov 2021 09:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/talk/hida2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Co-Organizing the workshop and training on Improving Job Market Skills of Graduates</title>
      <link>https://albarqouni.github.io/talk/agya2021/</link>
      <pubDate>Sun, 19 Sep 2021 09:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/talk/agya2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Affordable AI and Healthcare</title>
      <link>https://albarqouni.github.io/project/affordable-ai/</link>
      <pubDate>Wed, 15 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/project/affordable-ai/</guid>
      <description>&lt;p&gt;We are also interested in developing affordable AI solutions suitable for poor-quality data generated by low infrastructure and point-of-care diagnosis.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Organizing the 1st MICCAI Workshop on aFfordable AI and healthcare</title>
      <link>https://albarqouni.github.io/talk/fair2021/</link>
      <pubDate>Wed, 23 Jun 2021 09:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/talk/fair2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Seminar: Federated Learning in Healthcare (SoSe2021)</title>
      <link>https://albarqouni.github.io/students/courses/flhsose2021/</link>
      <pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/students/courses/flhsose2021/</guid>
      <description>&lt;p&gt;Organizers: 
&lt;a href=&#34;../../#about&#34;&gt;Dr. Shadi Albarqouni&lt;/a&gt;, 
&lt;a href=&#34;https://www.helmholtz.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Helmholtz AI&lt;/a&gt; and 
&lt;a href=&#34;https://www.in.tum.de/en/cover-page/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TU Munich&lt;/a&gt;, 
&lt;a href=&#34;http://campar.in.tum.de/Main/NassirNavab&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. Nassir Navab&lt;/a&gt;, Chair for 
&lt;a href=&#34;http://campar.in.tum.de/WebHome&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Computer Aided Medical Procedures&lt;/a&gt;, and 
&lt;a href=&#34;http://aim-lab.io/author/daniel-ruckert/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. Daniel Rueckert&lt;/a&gt;, Chair for 
&lt;a href=&#34;http://aim-lab.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AI in Medicine&lt;/a&gt;,  
&lt;a href=&#34;https://www.in.tum.de/en/cover-page/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TU Munich&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Time: Fridays, 10:00 - 12:00&lt;/p&gt;
&lt;h4 id=&#34;announcements&#34;&gt;Announcements&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;14-07-2021: The deadline of the blog post is extended to Thursday, 22.07.2021.&lt;/li&gt;
&lt;li&gt;14-04-2021: Papers are assigned. Please have a look at the table below.&lt;/li&gt;
&lt;li&gt;08-04-2021: Please send 3-4 preferences from different topics (Data Heterogeneity, Robustness, &amp;hellip;etc.) to 
&lt;a href=&#34;../../authors/cosmin-bercea/&#34;&gt;Cosmin Bercea&lt;/a&gt; by 13th April 2021.&lt;/li&gt;
&lt;li&gt;29-01-2021: Preliminary meeting is moved to &lt;font style=&#34;color: red; font-weight: bold&#34;&gt;Friday, 05.02.2021  (11:00-11:30)&lt;/font&gt;. Please register in advance for this meeting through this 
&lt;a href=&#34;https://tum-conf.zoom.us/meeting/register/u5ApcuihqjIoGNe7lbY6r6O-vGtHHagGK2nw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt;. After the registration, you will receive a confirmation email with the dial-up information.&lt;/li&gt;
&lt;li&gt;19-01-2021: Preliminary meeting:  &lt;s&gt;&lt;font style=&#34;color: red; font-weight: bold&#34;&gt;Monday, 01.02.2021  (11:00-11:30)&lt;/font&gt;&lt;/s&gt; via Zoom&lt;/li&gt;
&lt;li&gt;19-01-2021: Contact information: if you have any questions about this seminar, please feel free to contact 
&lt;a href=&#34;mailto:shadi.albarqouni@tum.de&#34;&gt;Dr. Shadi Albarqouni&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;19-01-2021: The website is up!&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Following the great success of our on-going seminar on Deep Learning for Medical Applications, we would like to discuss advanced topics that are quite relevant to Federated Learning which becomes an interesting and hot research direction in the community. In simple words, Federated Learning enables training models at the client-side while preserving their privacy, and aggregates the knowledge from the nodes to learn a global model. The interesting part here that the data are kept private and not transmitted to any other nodes. Instead, the characteristics (e.g. parameters) of the global model are shared with the clients, and once the training is done locally, the characteristics are sent back to the global one for aggregation. This learning paradigm has been received quite nicely in the community, in particular, for sensitive domains, e.g. Healthcare. To push this momentum, we proposed, together with our academia and industry partners, a workshop on 
&lt;a href=&#34;https://dcl-workshop.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Federated, Collaborative, and Distributed Learning&lt;/a&gt; in the 
&lt;a href=&#34;https://miccai2020.org/en/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;International Conference on Medical Image Computing and Computer-Aided Intervention (MICCAI)&lt;/a&gt; to attract significant contributions attacking the challenges in Medical Imaging and Healthcare. In this seminar, we will be discussing the relevant papers on Federated Learning with an emphasis on the papers tackling the common challenges in Medical Imaging, e.g. data heterogeneity, domain shift, and non-iid distributed data.&lt;/p&gt;
&lt;h3 id=&#34;registration&#34;&gt;Registration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Interested students should attend the preliminary meeting to enlist in the course.&lt;/li&gt;
&lt;li&gt;Students can only register through 
&lt;a href=&#34;https://matching.in.tum.de/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TUM Matching Platform&lt;/a&gt; themselves if the maximum number of participants hasn&amp;rsquo;t been reached (please pay attention to the 
&lt;a href=&#34;http://docmatching.in.tum.de/index.php/schedule&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deadlines&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;A maximum number of participants: 12.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;requirements&#34;&gt;Requirements&lt;/h3&gt;
&lt;p&gt;In this Master Seminar, each student is asked to send three preferences from the list, then he will be assigned one paper. In order to successfully complete the seminar, participants have to fulfill these requirements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Presentation&lt;/strong&gt;: The selected paper is presented to the other participants.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Blog Post&lt;/strong&gt;: A blog post of 1000-1500 words excluding references should be submitted before the deadline.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Attendance&lt;/strong&gt;: Participants have to participate actively in all seminar sessions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The students are required to attend each seminar presentation which will be held during this course. Each presentation is followed by a discussion and everyone is encouraged to actively participate. The blog post must include all references used and must be written &lt;em&gt;completely in your own words&lt;/em&gt;. Copy and paste will not be tolerated. Both the blog post and presentation have to be written in &lt;em&gt;English&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Submission Deadline&lt;/strong&gt;:  You have to submit the presentation &lt;em&gt;two weeks&lt;/em&gt; right after your presentation session. Please push your slides to the respective folder, i.e., SS2021, at this 
&lt;a href=&#34;https://gitlab.lrz.de/Albarqouni_BMC/flh2020.git&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;repository&lt;/a&gt;, and create your blog at the Wiki section under Blogs. The deadline of the blog post is extended to Thursday, 22.07.2021.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Guidelines:&lt;/strong&gt;
I could not find better than this 
&lt;a href=&#34;https://ocw.mit.edu/courses/brain-and-cognitive-sciences/9-916-the-neural-basis-of-visual-object-recognition-in-monkeys-and-humans-spring-2005/assignments/how_to_pres_pap.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;guidelines&lt;/a&gt; to prepare for your presentation. The only difference is that you need to plan for 30 minutes for 1-4, and 10 minutes  for 5). Nevertheless, I have prepared a few slides acting as a 
&lt;a href=&#34;../../slides/pres&#34;&gt;guidelines&lt;/a&gt; for your presentation and blog posts.&lt;/p&gt;
&lt;h3 id=&#34;schedule&#34;&gt;Schedule&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;em&gt;Date (tentative)&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Session: Topic&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Speakers / Presenters&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;05.02.2021 (11:00 AM)&lt;/td&gt;
&lt;td&gt;Preliminary Meeting&lt;/td&gt;
&lt;td&gt;Shadi&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://albarqouni.github.io/slides/federated/#/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Online&lt;/td&gt;
&lt;td&gt;Paper Assignment&lt;/td&gt;
&lt;td&gt;Shadi/Cosmin&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;16.04.2021&lt;/td&gt;
&lt;td&gt;Guidelines&lt;/td&gt;
&lt;td&gt;Shadi&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;30.04.2021&lt;/td&gt;
&lt;td&gt;Data Heterogeneity I&lt;/td&gt;
&lt;td&gt;Buess, Yirik&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;14.05.2021&lt;/td&gt;
&lt;td&gt;Data Heterogeneity II&lt;/td&gt;
&lt;td&gt;Buchberger, Flecken&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;28.05.2021&lt;/td&gt;
&lt;td&gt;&amp;ndash;&lt;/td&gt;
&lt;td&gt;&amp;ndash;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;11.06.2021&lt;/td&gt;
&lt;td&gt;Robustness&lt;/td&gt;
&lt;td&gt;Stampfl, Mostafa&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;25.06.2021&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Federated Learning in Healthcare: from Theory to Practice&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Dr. 
&lt;a href=&#34;https://marcolorenzi.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marco Lorenzi&lt;/a&gt;, Inria, France&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;09.07.2021&lt;/td&gt;
&lt;td&gt;Uncertainty and Interpretability&lt;/td&gt;
&lt;td&gt;Yu, Feil&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;list-of-topics-and-papers&#34;&gt;List of Topics and Papers&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;em&gt;Topic&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;No&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Title&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Conference/Journal&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Tutor&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Student (Last name)&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Link&lt;/em&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Data Heterogeneity&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;FedDG: Federated Domain Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency Space&lt;/td&gt;
&lt;td&gt;CVPR 2021&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2103.06030.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;Personalized Federated Learning using Hypernetworks&lt;/td&gt;
&lt;td&gt;arxiv 2021&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Buess&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2103.04628.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;Adaptive Federated Optimization&lt;/td&gt;
&lt;td&gt;ICLR 2021&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2003.00295.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;FedMix: Approximation of Mixup Under Mean Augmented Federated Learning&lt;/td&gt;
&lt;td&gt;ICLR 2021&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://openreview.net/pdf?id=Ogga20D2HO-&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;HeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients&lt;/td&gt;
&lt;td&gt;ICLR 2021&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Yirik&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://openreview.net/pdf?id=TNkPBBYFkXg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;Personalized Federated Learning with First Order Model Optimization&lt;/td&gt;
&lt;td&gt;ICLR 2021&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Buchgberger&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2012.08565v3.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;Federated Learning Based on Dynamic Regulariation&lt;/td&gt;
&lt;td&gt;ICLR 2021&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://openreview.net/pdf?id=B7v4QMR6Z9w&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;Tackling the Objective Inconsistency Problem in Heterogenous Federated Optimization&lt;/td&gt;
&lt;td&gt;NeurIPS 2020&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2007.07481.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;Ensemble Distillation for Robust Model Fusion in Federated Learning&lt;/td&gt;
&lt;td&gt;NeurIPS 2020&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2006.07242.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;Salvaging Federated Learning by Local Adaptation&lt;/td&gt;
&lt;td&gt;arxiv  2020&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Flecken&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2002.04758.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;FedAwS: Federated Learning with Only Positive Labels&lt;/td&gt;
&lt;td&gt;ICML 2020&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;http://www.sanjivk.com/FedPositive_ICML20.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Robustness&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;Provable Defense against Privacy Leakage in Federated Learning from Representation Perspective&lt;/td&gt;
&lt;td&gt;CVPR 2021&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2012.06043.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;Untargeted Poisoning Attack Detection in Federated Learning via Behaviour Attestation&lt;/td&gt;
&lt;td&gt;arxiv 2021&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2101.10904.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;Privacy and Robustness in Federated Learning: Attacks and Defenses&lt;/td&gt;
&lt;td&gt;arxiv 2020&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Stampfl&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2012.06337.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;How to backdoor federated learning&lt;/td&gt;
&lt;td&gt;PMLR 2020&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Mostafa&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1807.00459.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;DBA: Distributed Backdoor Attacks against Federated Learning&lt;/td&gt;
&lt;td&gt;ICLR 2020&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://openreview.net/pdf/61dc789b9f12be96506a23ddb7670ac132a51d6d.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Privacy and Security&lt;/td&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;Federated f-Differential Privacy&lt;/td&gt;
&lt;td&gt;PMLR 2021&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;http://proceedings.mlr.press/v130/zheng21a/zheng21a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;td&gt;BatchCrypt: Efficient Homomorphic Encryption for Cross-Silo Federated Learning&lt;/td&gt;
&lt;td&gt;USENIX 2020&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://www.usenix.org/system/files/atc20-zhang-chengliang.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;19&lt;/td&gt;
&lt;td&gt;Secure Weighted Aggregation in Federated Learning&lt;/td&gt;
&lt;td&gt;arxiv 2020&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2010.08730.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;Differentially Private Meta-Learning&lt;/td&gt;
&lt;td&gt;ICLR 2020&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://openreview.net/pdf?id=rJgqMRVYvr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;21&lt;/td&gt;
&lt;td&gt;Secure, privacy-preserving and federated machine learning in medical imaging&lt;/td&gt;
&lt;td&gt;Nature 2020&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://www.nature.com/articles/s42256-020-0186-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Uncertainty  and Interpretability&lt;/td&gt;
&lt;td&gt;22&lt;/td&gt;
&lt;td&gt;Federated Uncertainty-Aware Learning for Distributed Hospital EHR Data&lt;/td&gt;
&lt;td&gt;Journal of Healthcare Informatics Research 2021&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1910.12191.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;23&lt;/td&gt;
&lt;td&gt;FedBE: Making Bayesian Model Ensemble Applicable to Federated Learning&lt;/td&gt;
&lt;td&gt;ICLR 2021&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Yu&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://openreview.net/pdf?id=dgtpE6gKjHn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;24&lt;/td&gt;
&lt;td&gt;Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms&lt;/td&gt;
&lt;td&gt;ICLR 2021&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Feil&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://openreview.net/pdf?id=GFsU8a0sGB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;25&lt;/td&gt;
&lt;td&gt;Probabilistic predictions with federated learning&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=&amp;amp;ved=2ahUKEwi4ydzbjdjvAhWRgP0HHf62AVAQFjABegQIBBAD&amp;amp;url=https%3A%2F%2Fwww.mdpi.com%2F1099-4300%2F23%2F1%2F41%2Fpdf&amp;amp;usg=AOvVaw0RI0T7gLKLjjDs5oUfhEY4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;26&lt;/td&gt;
&lt;td&gt;Interpret Federated Learning with Shapley Values&lt;/td&gt;
&lt;td&gt;arxiv 2019&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1905.04519.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;interested&#34;&gt;Interested?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Join our Federated Learning Working Group at 
&lt;a href=&#34;../../slides/MONAI_Slides_Master_v2.pdf&#34;&gt;Medical Open Network for AI (MONAI)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contact-the-organizers&#34;&gt;Contact the organizers&lt;/h3&gt;
&lt;p&gt;If you have any questions regarding the course, please do not hesitate to contact 
&lt;a href=&#34;mailto:shadi.albarqouni@tum.de&#34;&gt;Dr. Shadi Albarqouni&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Organizers.png&#34; alt=&#34;Organizers&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>BigPicture Project</title>
      <link>https://albarqouni.github.io/project/bigpicture/</link>
      <pubDate>Sat, 30 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/project/bigpicture/</guid>
      <description>&lt;h2 id=&#34;a-new-consortium-of-the-eu-innovative-medicines-initiative-imi-will-establish-the-biggest-database-of-pathology-images-to-accelerate-the-development-of-artificial-intelligence-in-medicine&#34;&gt;A new consortium of the EU Innovative Medicines Initiative (IMI) will establish the biggest database of pathology images to accelerate the development of artificial intelligence in medicine.&lt;/h2&gt;
&lt;p&gt;To take AI development in pathology to the next level, a European consortium combining leading European research centres, hospitals as well as major pharmaceutical industries, is going to develop a repository for the sharing of pathology data. The 6-year, €70 million project called 
&lt;a href=&#34;https://www.bigpicture.eu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BIGPICTURE&lt;/a&gt;, will herald a new era in pathology.&lt;/p&gt;
&lt;h3 id=&#34;background&#34;&gt;Background&lt;/h3&gt;
&lt;p&gt;Pathology is the cornerstone of the workup of many diseases such as cancer, autoimmune diseases, of the follow up after transplantation and is also critical for the evaluation of the safety of drugs. It’s based on the examination of tissue samples (slides) under the microscope. However, despite its pivotal role, it still relies heavily on the qualitative interpretation by a qualified pathologist.&lt;/p&gt;
&lt;p&gt;While the microscope symbolizes the profession, the digitalisation of slides in recent years ignited a revolution: not only images can now be shared and accessed from distant locations, they can also be processed by computers. This opens the door for artificial intelligence (AI) applications to assist the pathologist and help study diseases, find better treatments and contribute to the 3Rs (replace, reduce, and refine animal use in research). However, the development of robust AI applications requires large amounts of data, which in the case of pathology means a huge collection of digital slides and the medical data necessary for their interpretation. Sharing these has so far remained challenging due to the data storage capacity required to host a sufficiently large collection and to concerns regarding the confidential character of the medical information.&lt;/p&gt;
&lt;p&gt;To allow the fast development of AI in pathology, the 
&lt;a href=&#34;https://www.bigpicture.eu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BIGPICTURE&lt;/a&gt; project aims to create the first European, ethical and GDPR-compliant (General Data Protection Regulation), quality-controlled platform, in which both large-scale data and AI algorithms will coexist. The 
&lt;a href=&#34;https://www.bigpicture.eu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BIGPICTURE&lt;/a&gt; platform will be developed in a sustainable and inclusive way by connecting communities of pathologists, researchers, AI developers, patients, and industry parties.&lt;/p&gt;
&lt;h3 id=&#34;tu-munich&#34;&gt;TU Munich&lt;/h3&gt;
&lt;p&gt;In this project, 
&lt;a href=&#34;http://campar.in.tum.de/Main/NassirNavab&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. Nassir Navab&lt;/a&gt;, and 
&lt;a href=&#34;../../#about&#34;&gt;Dr. Shadi Albarqouni&lt;/a&gt; from 
&lt;a href=&#34;www.tum.de&#34;&gt;TU Munich&lt;/a&gt;, together with 
&lt;a href=&#34;https://owkin.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Owkin&lt;/a&gt; will be leading and contributing to the development of Federated Deep Learning algorithms leveraging massive amounts of data, distributed in multiple sources, in a privacy-preserved fashion. This will enable deep learning models to be trained using sensitive data that cannot be made publicly available due to GPDR or sensitivity, e.g. rare diseases. Please visit the website of 
&lt;a href=&#34;https://www.bigpicture.eu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BIGPICTURE&lt;/a&gt; for further details.&lt;/p&gt;
&lt;h3 id=&#34;intended-results&#34;&gt;Intended results&lt;/h3&gt;
&lt;p&gt;The project is divided into four main aspects that concern the large-scale collection of data. First, an infrastructure (hardware and software) must be created to store, share and process millions of images that can be gigabytes each. Second, legal and ethical constraints must be put in place to ensure adequate usage of data while fully respecting patient’s privacy and data confidentiality. Then, an initial set of 3 million digital slides from humans and laboratory animals will be collected and stored into the repository to provide data for the development of pathology AI tools. Finally, functionalities that aid the use of the database as well as the processing of images for diagnostic and research purposes will be developed.&lt;/p&gt;
&lt;h3 id=&#34;consortium&#34;&gt;Consortium&lt;/h3&gt;
&lt;p&gt;BIGPICTURE is a public-private partnership funded by IMI, with representation from academic institutions, small- and medium-sized enterprises (SMEs), public organisations and pharmaceutical companies, together with a large network slide contributing partners. The consortium partners involved in the project are:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Academic institutions:&lt;/strong&gt; Radboud University Medical Center (NL), Linköping University (SE), Leeds Teaching Hospitals NHS Trust (UK), University Medical Centre Utrecht (NL), Uppsala University (SE, ELIXIR node), Haute Ecole Spécialisé de Suisse Occidentale (CH), Technical University Eindhoven (NL), University of Warwick (UK), [Technical University of Munich (DE), Medical University Graz (AT), Institut Pasteur (FR), University of Liege (BE), University of Semmelweis (HU), National Cancer Institute (NL), Region Östergötland (SE), Medical University Vienna (AT), University of Marburg (DE), Helsingin ja Uudenmaan sairaanhoitopiirin kuntayhtymä (FI),&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pharmaceutical companies:&lt;/strong&gt; Novartis Pharma AG (CH), Janssen Pharmaceutica NV (BE), Bayer AG (DE), Boehringer Ingelheim International GmbH (DE), Novo Nordisk A/S (DK), Pfizer (US), Genentech – Roche (US), Sanofi Aventis recherche et Développement (FR), Institut de Recherches Internationales Servier (FR), and UCB Biopharma SRL (BE).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Other public &amp;amp; private organisations:&lt;/strong&gt; CSC – IT Center for Science Finland (FI, ELIXIR node), Biobanks and biomolecular resources research infrastructure (AT), Azienda Ospedaliera Per L’Emergenza Cannizzaro (IT), Fraunhofer-Gesellschaft zur Förderung der angewandten Forschung e.V.(DE), Deutsches Institut für Normung E.V. (DE), European Institute for Innovation through Health Data (BE), European Society of Pathology (BE), Digital pathology association (US), GBG Forschungs Gmbh (DE), ttopstart (NL), Sectra AB (SE), Cytomine SCRLFS (BE), Stichting Lygature (NL), Owkin (FR), Deciphex (IE), MedicalPhit (NL), Timelex (BE),&lt;/p&gt;
&lt;p&gt;BIGPICTURE starts on 1st February 2021 and will run for 6 years. However, the platform is meant to last, and the consortium will elaborate sustainability plans to maintain and continue to develop the platform beyond this term.&lt;/p&gt;
&lt;h4 id=&#34;acknowledgment-of-support-and-disclaimer&#34;&gt;Acknowledgment of support and disclaimer&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;This project has received funding from the Innovative Medicines Initiative 2 Joint Undertaking under grant agreement No 945358. This Joint Undertaking receives support from the European Union’s Horizon 2020 research and innovation program and EFPIA.&lt;/em&gt; 
&lt;a href=&#34;http://www.imi.europe.eu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;www.imi.europe.eu&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This communication reflects the consortium’s view. Neither IMI nor the European Union or EFPIA are responsible for any use that may be made of the information contained therein.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;EU.png&#34; alt=&#34;EU&#34;&gt; &lt;img src=&#34;EFPIA.png&#34; alt=&#34;EFPIA&#34;&gt; &lt;img src=&#34;IMI.png&#34; alt=&#34;IMI&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;BigPicture.png&#34; alt=&#34;BigPicture&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Autoencoders for Unsupervised Anomaly Segmentation in Brain MR Images: A Comparative Study</title>
      <link>https://albarqouni.github.io/publication/baur-2020-autoencoders/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/baur-2020-autoencoders/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Autoencoders for Unsupervised Anomaly Segmentation in Brain MR Images: A Comparative Study</title>
      <link>https://albarqouni.github.io/publication/baur-2021-autoencoders/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/baur-2021-autoencoders/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Butterfly-Net: Spatial-Temporal Architecture For Medical Image Segmentation</title>
      <link>https://albarqouni.github.io/publication/klymenko-2021-butterfly/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/klymenko-2021-butterfly/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Eine computergestützte automatische Polypencharakterisierung von Hyperplasten, Adenomen und Serratierten Adenomen im Kolorektum-Ergebnisse der CASSANDRA Studie</title>
      <link>https://albarqouni.github.io/publication/zvereva-2021-computergestutzte/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/zvereva-2021-computergestutzte/</guid>
      <description></description>
    </item>
    
    <item>
      <title>FedDis: Disentangled Federated Learning for Unsupervised Brain Pathology Segmentation</title>
      <link>https://albarqouni.github.io/publication/bercea-2021-feddis/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/bercea-2021-feddis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Federated Disentangled Representation Learning for Unsupervised Brain Anomaly Detection</title>
      <link>https://albarqouni.github.io/publication/bercea-2021-federated/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/bercea-2021-federated/</guid>
      <description></description>
    </item>
    
    <item>
      <title>FedPerl: Semi-Supervised Peer Learning for Skin Lesion Classification</title>
      <link>https://albarqouni.github.io/publication/bdair-2021-fedperl/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/bdair-2021-fedperl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Fourier Transform of Percoll Gradients Boosts CNN Classification of Hereditary Hemolytic Anemias</title>
      <link>https://albarqouni.github.io/publication/sadafi-2021-fourier/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/sadafi-2021-fourier/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Microaneurysms segmentation and diabetic retinopathy detection by learning discriminative representations</title>
      <link>https://albarqouni.github.io/publication/sarhan-2021-microaneurysms/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/sarhan-2021-microaneurysms/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Modeling Healthy Anatomy with Artificial Intelligence for Unsupervised Anomaly Detection in Brain MRI</title>
      <link>https://albarqouni.github.io/publication/baur-2021-modeling/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/baur-2021-modeling/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Semi-Supervised Few-Shot Learning with Prototypical Random Walks</title>
      <link>https://albarqouni.github.io/publication/ayyad-2021-semi/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/ayyad-2021-semi/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sickle Cell Disease Severity Prediction from Percoll Gradient Images using Graph Convolutional Networks</title>
      <link>https://albarqouni.github.io/publication/sadafi-2021-sickle/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/sadafi-2021-sickle/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Federated Tumor Segmentation (FeTS) Challenge</title>
      <link>https://albarqouni.github.io/publication/pati-2021-federated/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/pati-2021-federated/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The OOD Blind Spot of Unsupervised Anomaly Detection</title>
      <link>https://albarqouni.github.io/publication/heer-2021-ood/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/heer-2021-ood/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multi-task multi-domain learning for digital staining and classification of leukocytes</title>
      <link>https://albarqouni.github.io/publication/tomczak-2020-multi/</link>
      <pubDate>Tue, 22 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/tomczak-2020-multi/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Uncertainty-Driven GCN Refinement Strategy for Organ Segmentation</title>
      <link>https://albarqouni.github.io/publication/soberanis-2020-uncertainty/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/soberanis-2020-uncertainty/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Ascertaining the Pose of an X-Ray Unit Relative to an Object on the Basis of a Digital Model of the Object</title>
      <link>https://albarqouni.github.io/publication/albarqouni-2020-ascertaining/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/albarqouni-2020-ascertaining/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Determining a Pose of an Object in the Surroundings of the Object by Means of Multi-Task Learning</title>
      <link>https://albarqouni.github.io/publication/zakharov-2020-determining/</link>
      <pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/zakharov-2020-determining/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Seminar: Federated Learning in Healthcare (WiSe2020)</title>
      <link>https://albarqouni.github.io/students/courses/flh/</link>
      <pubDate>Thu, 15 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/students/courses/flh/</guid>
      <description>&lt;p&gt;Organizers: 
&lt;a href=&#34;../#about&#34;&gt;Dr. Shadi Albarqouni&lt;/a&gt;, 
&lt;a href=&#34;https://www.helmholtz.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Helmholtz AI&lt;/a&gt; and  
&lt;a href=&#34;http://campar.in.tum.de/Main/ShadiAlbarqouni&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TU Munich&lt;/a&gt;, and 
&lt;a href=&#34;http://campar.in.tum.de/Main/NassirNavab&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. Nassir Navab&lt;/a&gt;, TU Munich.&lt;/p&gt;
&lt;h4 id=&#34;announcements&#34;&gt;Announcements&lt;/h4&gt;
&lt;details&gt;&lt;summary&gt;
- 17-12-2020: The deadline to submit the blog post is moved to 1st. Feb. 2021.   &lt;/br&gt;  &lt;/summary&gt;
- 05-11-2020: Our seminar will be held online via &lt;a href=&#34;https://tum-conf.zoom.us/j/91926157752&#34;&gt;Zoom.&lt;/a&gt;  Password is communicated to the participants through TUMonline. 
- 30-09-2020: Your name has been assigned to one of our sessions. Please pin the date on your calendar.&lt;/br&gt;
- 30-09-2020: Two invited talks were just arranged. Thanks to the invited speakers! &lt;/br&gt;
- 21-09-2020: We are still working on the schedule. &lt;/br&gt;
- 21-09-2020: We extended the time slots of the presentations from 20 to 30 mins (See the requirements) &lt;/br&gt;
- 21-09-2020: Papers are assigned to the students according to their preferences (See the list of Topics and Material Table). &lt;/br&gt;
- 22-07-2020: Registration is done via the matching system. You need to send a motivation letter with the subject &#34;FLH_Motivatoon&#34; to Dr. Shadi Albarqouni in order to get a higher ranking in the matching system from our side&lt;/br&gt;
- 14-07-2020: Register in advance for this meeting &lt;a href=&#34;https://tum-conf.zoom.us/meeting/register/tJMqc-uqqTwsG9fC0e88gznwxvrEFpNIvPQ6&#34;&gt;here&lt;/a&gt;. After registering, you will receive a confirmation email containing information about joining the meeting.&lt;/br&gt;
- 08-07-2020: Preliminary meeting:  &lt;s&gt;&lt;font style=&#34;color: red; font-weight: bold&#34;&gt;Thursday, 16.07.2020  (10:00-11:00) &lt;/font&gt;&lt;/s&gt; &lt;font style=&#34;color: red; font-weight: bold&#34;&gt;Friday, 17.07.2020  (11:00-12:00) &lt;/font&gt;  in virtual meeting room (zoom). &lt;/br&gt;
- 08-07-2020: Contact information-If you have any question about this seminar, please feel free to contact Dr. Shadi Albarqouni&lt;/br&gt;
- 08-07-2020: Website is up!&lt;/br&gt;
&lt;/details&gt;
&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Following the great success of our on-going seminar on Deep Learning for Medical Applications, we would like to discuss advanced topics that are quite relevant to Federated Learning which becomes an interesting and hot research direction in the community. In simple words, Federated Learning enables training models at the client-side while preserving their privacy, and aggregates the knowledge from the nodes to learn a global model. The interesting part here that the data are kept private and not transmitted to any other nodes. Instead, the characteristics (e.g. parameters) of the global model are shared with the clients, and once the training is done locally, the characteristics are sent back to the global one for aggregation. This learning paradigm has been received quite nicely in the community, in particular, for sensitive domains, e.g. Healthcare. To push this momentum, we proposed, together with our academia and industry partners, a workshop on 
&lt;a href=&#34;https://dcl-workshop.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Federated, Collaborative, and Distributed Learning&lt;/a&gt; in the 
&lt;a href=&#34;https://miccai2020.org/en/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;International Conference on Medical Image Computing and Computer-Aided Intervention (MICCAI)&lt;/a&gt; to attract significant contributions attacking the challenges in Medical Imaging and Healthcare. In this seminar, we will be discussing the relevant papers on Federated Learning with an emphasis on the papers tackling the common challenges in Medical Imaging, e.g. data heterogeneity, domain shift, and non-iid distributed data.&lt;/p&gt;
&lt;h3 id=&#34;registration&#34;&gt;Registration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Interested students should attend the preliminary meeting to enlist in the course.&lt;/li&gt;
&lt;li&gt;Students can only register through 
&lt;a href=&#34;https://matching.in.tum.de/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TUM Matching Platform&lt;/a&gt; themselves if the maximum number of participants hasn&amp;rsquo;t been reached (please pay attention to the 
&lt;a href=&#34;http://docmatching.in.tum.de/index.php/schedule&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deadlines&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;A maximum number of participants: 12.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;requirements&#34;&gt;Requirements&lt;/h3&gt;
&lt;p&gt;In this Master Seminar, each student is asked to send three preferences from the list, then he will be assigned one paper. In order to successfully complete the seminar, participants have to fulfill these requirements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Presentation&lt;/strong&gt;: The selected paper is presented to the other participants.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Blog Post&lt;/strong&gt;: A blog post of 1000-1500 words excluding references should be submitted before the deadline.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Attendance&lt;/strong&gt;: Participants have to participate actively in all seminar sessions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The students are required to attend each seminar presentation which will be held during this course. Each presentation is followed by a discussion and everyone is encouraged to actively participate. The blog post must include all references used and must be written &lt;em&gt;completely in your own words&lt;/em&gt;. Copy and paste will not be tolerated. Both the blog post and presentation have to be written in &lt;em&gt;English&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Submission Deadline&lt;/strong&gt;: You have to submit both the presentation &lt;em&gt;two weeks&lt;/em&gt; right after your presentation session. The deadline of the blog post is moved to 1st Feb. 2021.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Guidelines:&lt;/strong&gt;
I could not find better than this 
&lt;a href=&#34;https://ocw.mit.edu/courses/brain-and-cognitive-sciences/9-916-the-neural-basis-of-visual-object-recognition-in-monkeys-and-humans-spring-2005/assignments/how_to_pres_pap.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;guidelines&lt;/a&gt; to prepare for your presentation. The only difference is that you need to plan for 30 minutes for 1-4, and 10 minutes  for 5). Nevertheless, I have prepared a few slides acting as a 
&lt;a href=&#34;../../slides/pres&#34;&gt;guidelines&lt;/a&gt; for your presentation and blog posts.&lt;/p&gt;
&lt;h3 id=&#34;schedule&#34;&gt;Schedule&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;em&gt;Date&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Session: Topic&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Speakers / Presenters&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;17.07.2020 (11:00 AM)&lt;/td&gt;
&lt;td&gt;Preliminary Meeting&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://albarqouni.github.io/slides/federated/#/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Online&lt;/td&gt;
&lt;td&gt;Paper Assignment&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;../../slides/pres&#34;&gt;Guidelines&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;09.11.2020 @ 11:00 AM&lt;/td&gt;
&lt;td&gt;Federated Learning; Challenges, Methods, and Future I&lt;/td&gt;
&lt;td&gt;Invited Talk: 
&lt;a href=&#34;https://www.nature.com/articles/s41746-020-00323-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Federated Learning: Collaborative AI without Exposing Patient Data&lt;/a&gt; &amp;ndash;&lt;em&gt;
&lt;a href=&#34;../../authors/nicola-rieke&#34;&gt;Nicola Rieke&lt;/a&gt; from 
&lt;a href=&#34;https://www.nvidia.com/en-us/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NVIDIA&lt;/a&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;16.11.2020&lt;/td&gt;
&lt;td&gt;Federated Learning; Challenges, Methods, and Future II&lt;/td&gt;
&lt;td&gt;Ünay, Sánchez Clemente&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;23.11.2020&lt;/td&gt;
&lt;td&gt;Data Heterogeneity I&lt;/td&gt;
&lt;td&gt;Lin, Raether&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;30.11.2020&lt;/td&gt;
&lt;td&gt;Data Heterogeneity II&lt;/td&gt;
&lt;td&gt;Stoican, Schwarz&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;07.12.2020&lt;/td&gt;
&lt;td&gt;System Heterogeneity and Privacy Issues I&lt;/td&gt;
&lt;td&gt;Invited Talk: 
&lt;a href=&#34;https://www.nature.com/articles/s42256-020-0186-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Secure, privacy-preserving and federated machine learning in medical imaging&lt;/a&gt; &amp;ndash;&lt;em&gt;
&lt;a href=&#34;https://g-k.ai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;George Kaissis&lt;/a&gt; from 
&lt;a href=&#34;https://www.mri.tum.de/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Klinikum rechts der Isar&lt;/a&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;14.12.2020&lt;/td&gt;
&lt;td&gt;System Heterogeneity and Privacy Issues II&lt;/td&gt;
&lt;td&gt;Qian, Heidmann&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;21.12.2020&lt;/td&gt;
&lt;td&gt;Data Heterogeneity III&lt;/td&gt;
&lt;td&gt;Spannagl, Arfaoui&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;11.01.2021&lt;/td&gt;
&lt;td&gt;Federated Learning with Medical Imaging I&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;../../authors/zehra-hayirci/&#34;&gt;Zehra&lt;/a&gt; (
&lt;a href=&#34;https://proceedings.neurips.cc/paper/2020/file/e32cc80bf07915058ce90722ee17bb71-Paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeurIPS&#39;20 Paper&lt;/a&gt;), 
&lt;a href=&#34;../../authors/cosmin-bercea/&#34;&gt;Cosmin&lt;/a&gt; (
&lt;a href=&#34;https://arxiv.org/abs/2008.07665&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MICCAIW&#39;20 Paper&lt;/a&gt;)&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;18.01.2021&lt;/td&gt;
&lt;td&gt;Federated Learning with Medical Imaging II&lt;/td&gt;
&lt;td&gt;Boysen&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;25.01.2021&lt;/td&gt;
&lt;td&gt;Federated Learning with Medical Imaging III&lt;/td&gt;
&lt;td&gt;Invited Talk:  Federated Learning with Heterogeneous data in Healthcare &amp;ndash;&lt;em&gt;Jean Ogier du Terrail&lt;/em&gt; from 
&lt;a href=&#34;https://owkin.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Owkin&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;list-of-topics-and-papers&#34;&gt;List of Topics and Papers&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;em&gt;Topic&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;No&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Title&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Conference/Journal&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Tutor&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Student (Last name)&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Link&lt;/em&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Intro. to FL&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;FedAvg: Communication-Efficient Learning of Deep Networks from Decentralized Data&lt;/td&gt;
&lt;td&gt;AISTATS, 2016&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Ünay&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://arxiv.org/abs/1602.05629&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;The Future of Digital Health with Federated Learning&lt;/td&gt;
&lt;td&gt;arXiv, 2020&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Invited Speaker&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2003.08119.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Challenges&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;Federated Learning: Challenges, Methods, and Future Directions&lt;/td&gt;
&lt;td&gt;IEEE Signal Processing Magazine, 2020&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Sánchez Clemente&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1908.07873.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;On the Convergence of FedAvg on Non-IID Data&lt;/td&gt;
&lt;td&gt;ICLR 2020&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://openreview.net/pdf?id=HJxNAnVtDS&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Data Heterogeneity&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;FedMA: Federated Learning with Matched Averaging&lt;/td&gt;
&lt;td&gt;ICLR 2020&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Lin&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://openreview.net/pdf?id=BkluqlSFDS&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;Federated Adversarial Domain Adaptation&lt;/td&gt;
&lt;td&gt;ICLR 2020&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Spannagl&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://openreview.net/pdf?id=HJezF3VYPB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;Federated optimization in heterogeneous networks&lt;/td&gt;
&lt;td&gt;MLSys 2020&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Raether&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1812.06127.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;FedAwS: Federated Learning with Only Positive Labels&lt;/td&gt;
&lt;td&gt;ICML 2020&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://proceedings.icml.cc/static/paper_files/icml/2020/5034-Paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;SCAFFOLD: Stochastic Controlled Averaging for Federated Learning&lt;/td&gt;
&lt;td&gt;ICML 2020&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Stoican&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://proceedings.icml.cc/static/paper_files/icml/2020/788-Paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;Federated Visual Classification with Real-World Data Distribution&lt;/td&gt;
&lt;td&gt;CVPR 2020&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Schwarz&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2003.08082.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;System Heterogeneity&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;Federated Multi-Task Learning&lt;/td&gt;
&lt;td&gt;NeurIPS 2017&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Qian&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://papers.nips.cc/paper/7029-federated-multi-task-learning.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;Variational Federated Multi-Task Learning&lt;/td&gt;
&lt;td&gt;arXiv 2019&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Heidmann&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1906.06268.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Privacy-Issues&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;Secure, privacy-preserving and federated machine learning in medical imaging&lt;/td&gt;
&lt;td&gt;Nature MI&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Invited Speaker&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://www.nature.com/articles/s42256-020-0186-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HTML&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;Differentially Private Meta-Learning&lt;/td&gt;
&lt;td&gt;ICLR 2020&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://iclr.cc/virtual/poster_rJgqMRVYvr.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Explainability and Robustness&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;The Non-IID Data Quagmire of Decentralized Machine Learning&lt;/td&gt;
&lt;td&gt;ICML 2020&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Arfaoui&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://proceedings.icml.cc/static/paper_files/icml/2020/3152-Paper.pdf]&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;DBA: Distributed Backdoor Attacks against Federated Learning&lt;/td&gt;
&lt;td&gt;ICLR 2020&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;http://www.openreview.net/pdf?id=rkgyS0VFvr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Open Problems in FL&lt;/td&gt;
&lt;td&gt;&amp;ndash;&lt;/td&gt;
&lt;td&gt;Advances and Open Problems in Federated Learning&lt;/td&gt;
&lt;td&gt;arXiv&lt;/td&gt;
&lt;td&gt;&amp;ndash;&lt;/td&gt;
&lt;td&gt;&amp;ndash;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1912.04977.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Federated Learning with Medical Imaging&lt;/td&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;Privacy-preserving Federated Brain Tumour Segmentation&lt;/td&gt;
&lt;td&gt;MICCAIW 2019&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Boysen&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-32692-0_16&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HTML&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Multi-institutional Deep Learning Modeling Without Sharing Patient Data: A Feasibility Study on Brain Tumor Segmentation&lt;/td&gt;
&lt;td&gt;MICCAIW 2019&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Boysen&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-11723-8_9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HTML&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;td&gt;Federated Learning in Distributed Medical Databases: Meta-Analysis of Large-Scale Subcortical Brain Data&lt;/td&gt;
&lt;td&gt;ISBI 2019&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Hofmann&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/8759317?casa_token=n3-x19MurqwAAAAA:Eyz2sIgH5MPRzVgtV9ADzDrl_A97A7M6xUYqi3iReri0d-SisH0CYfPYEh8aYjbSwEGHP45n&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HTML&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Inverse Distance Aggregation for Federated Learning with Non-IID Data&lt;/td&gt;
&lt;td&gt;MICCAIW 2020&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Hofmann&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://arxiv.org/abs/2008.07665&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;interested&#34;&gt;Interested?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Join our Federated Learning Working Group at 
&lt;a href=&#34;../../slides/MONAI_Slides_Master_v2.pdf&#34;&gt;Medical Open Network for AI (MONAI)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contact-us&#34;&gt;Contact us&lt;/h3&gt;
&lt;p&gt;If you have any questions regarding the course, please do not hesitate to contact us at 
&lt;a href=&#34;mailto:shadi.albarqouni@tum.de&#34;&gt;shadi.albarqouni@tum.de&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Future of Digital Health with Federated Learning</title>
      <link>https://albarqouni.github.io/publication/rieke-2020-future/</link>
      <pubDate>Mon, 14 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/rieke-2020-future/</guid>
      <description></description>
    </item>
    
    <item>
      <title>6D Camera Relocalization in Ambiguous Scenes via Continuous Multimodal Inference</title>
      <link>https://albarqouni.github.io/publication/bui-20206-d/</link>
      <pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/bui-20206-d/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Fairness by Learning Orthogonal Disentangled Representations</title>
      <link>https://albarqouni.github.io/publication/sarhan-2020-fairness/</link>
      <pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/sarhan-2020-fairness/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Medium Blog: Journey through COVID-19 RSNA Papers</title>
      <link>https://albarqouni.github.io/talk/covid19/</link>
      <pubDate>Mon, 20 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/talk/covid19/</guid>
      <description>&lt;p&gt;&lt;em&gt;Disclaimer: I am neither a radiologist nor a clinician. I am a computer scientist who have been working on medical image computing for a while. I tried to summairze the key findings reported in almost 15 papers published in the Radiology Society in North America (RSNA) in the last two months.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://www.euro.who.int/en/health-topics/health-emergencies/coronavirus-covid-19/novel-coronavirus-2019-ncov&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Intro about COVID-19&lt;/strong&gt; &lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;ct-imaging-features&#34;&gt;&lt;strong&gt;CT Imaging features&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Key CT findings have been studied and investigated by Guan et al. [10] in a large cohort of 1099 patients with confirmed COVID-19, and Chung et al. [1] in a group of 21 patients infected with COVID-19 in China. Their key results that majority of RT-PCR confirmed patients (some are asymptomatic) show typical CT findings such as the presence of bilateral ground-glass opacities (GGO) and/or consolidation, with a rounded morphology and a peripheral lung distribution (cf. Fig.1). In another cohort of 104 patients, from the cruise ship “Diamond Princess”, Inui et al. [4] have reported similar findings of lung opacities and airway abnormalities in both asymptomatic and symptomatic cases. In addition to the key characteristics of peripheral GGO, Caruso et al. [7] have also observed an association with sub-segmental vessel enlargement (&amp;gt; 3 mm) in his cohort of 158 participants from Italy (cf Fig.2).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/1440/1*j4iktyLv3cqyIh5LEcwf0A.png&#34; alt=&#34;img&#34;&gt;Fig.1: Image adopted from Chung et al. [1]&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Of 21 patients with the 2019 novel coronavirus, 15 (71%) had involvement of more than two lobes at chest CT, 12 (57%) had ground-glass opacities, seven (33%) had opacities with a rounded morphology, seven (33%) had a peripheral distribution of disease, six (29%) had consolidation with ground-glass opacities, and four (19%) had crazy-paving pattern. [1]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/1440/1*fnIOZsThw7AR_XrVaiIyXA.png&#34; alt=&#34;img&#34;&gt;Fig.2: Image adopted from Caruso et al. [7]&lt;/p&gt;
&lt;p&gt;Surprisingly, 14% of the patients (3 out of 21), studied by Chang et al. [1], show negative CT findings in their initial chest CT scan. Follow-up scans, however, show rounded peripheral ground-glass opacity (cf. Fig.3). Xie et al. [3] and Caruso et al. [7] have also reported similar percentages of 4% (7 out of 167), and 3% (2 out of 62), respectively, of their cohorts who show no findings in their CT scans.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/1440/1*fq3JovfT6QlwCdCg9iaDhA.png&#34; alt=&#34;img&#34;&gt;Fig.3: Image adopted from Chung et al. [1]&lt;/p&gt;
&lt;p&gt;Contrary, Xie et al. [3] found out that 3% of their cohort (5 out of 167), who had initially negative RT-PCR, show positive Chest CT with similar findings of viral pneumonia reported by Chung et al. [1] (cf Fig.4). A few days later, and after repeated swap tests, the RT-PCR had become positive. This has been. also confirmed in another cohort, reported by Fang et al. [8], where the percentage was around 29% (15 out of 51).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/1440/1*TUkEdff20eeO6JBAmY7lfA.png&#34; alt=&#34;img&#34;&gt;Fig.4: Adopted from Xie et al. [3]&lt;/p&gt;
&lt;h3 id=&#34;chest-ct-vs-rt-pcr&#34;&gt;&lt;strong&gt;Chest CT vs. RT-PCR&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Given the aforementioned key characteristics of COVID-19, the low sensitivity of the RT-PCR test (42–71%) [6], and the long mean interval time between the initial negative to positive RT-PCR (5.1 +/- 1.5 days), clinicians and researchers have investigated &lt;strong&gt;whether diagnostic imaging features could be used an alternative to RT-PCR in screening&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In patients at high risk for 2019-nCoV infection, chest CT evidence of viral pneumonia may precede positive negative RT-PCR test results. [3]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A few recent studies (Ai et al. [6], Caruso et al. [7], Fang et al. [8]) have investigated the correlation of Chest CT findings and RT-PCR test reporting high sensitivity of 97–98% for Chest CT in diagnosing COVID-19. Detailed evaluation metrics against the RT-PCR are reported below. Interestingly, 98% of the patients (56 out of 57), reported by Ai et al. [6], who had initially positive CT findings show positive RT-PCR within 6 days (cf Fig.5). &lt;strong&gt;Such interesting results suggest chest CT could be considered for screening.&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In a series of 51 patients with chest CT and RT-PCR assay performed within 3 days, the sensitivity of CT for COVID-19 infection was 98% compared to RT-PCR sensitivity of 71% (p&amp;lt;.001) [2]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Papers&lt;/th&gt;
&lt;th&gt;Sample Size&lt;/th&gt;
&lt;th&gt;Country&lt;/th&gt;
&lt;th&gt;Sensitivity&lt;/th&gt;
&lt;th&gt;Specificity&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Caruso et al. [7]&lt;/td&gt;
&lt;td&gt;58&lt;/td&gt;
&lt;td&gt;Rome, Italy&lt;/td&gt;
&lt;td&gt;97%&lt;/td&gt;
&lt;td&gt;56%&lt;/td&gt;
&lt;td&gt;72%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ai et al. [6]&lt;/td&gt;
&lt;td&gt;1014&lt;/td&gt;
&lt;td&gt;Wuhan, China&lt;/td&gt;
&lt;td&gt;97%&lt;/td&gt;
&lt;td&gt;25%&lt;/td&gt;
&lt;td&gt;68%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Fang et al. [8]&lt;/td&gt;
&lt;td&gt;51&lt;/td&gt;
&lt;td&gt;Shanghai, China&lt;/td&gt;
&lt;td&gt;98%&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/1440/1*c9DxSwH8gO2Wey-Ml6HfSQ.png&#34; alt=&#34;img&#34;&gt;Fig.5: Adopted from Ai et al. [6]&lt;/p&gt;
&lt;p&gt;Discrepancy between CT findings and RT-PCR motivated clinicians and researchers to analyze the serial CT findings over time (Wang and Dong et al. [9], Pan et al. [12]) and study the relationship to duration of infection (Bernheim et al. [11]). As reported in their analysis (Fig. 6, 7,8), the appearance of GGO and Consolidations varies over time explaining the discrepancy in the sensitivity. &lt;strong&gt;Both studies suggest, however, pathology quantification might help in the prognosis.&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The extent of CT abnormalities progressed rapidly after the onset of symptoms, peaked around 6–11 days, and followed by persistence of high levels in lung abnormalities. The temporal changes of the diverse CT manifestations followed a specific pattern, which might indicate the progression and recovery of the illness. [7]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/1440/1*s5erFY4_5HdFSxC5sSI9BQ.png&#34; alt=&#34;img&#34;&gt;Fig. 6: Adopted from Wang and Dong et al. [9]&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Recognizing imaging patterns based on infection time course is paramount for not only understanding the pathophysiology and natural history of infection, but also for helping to predict patient progression and potential complication development. [11]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/1440/1*kp-Ce1GSU9YDuS14igqnAQ.png&#34; alt=&#34;img&#34;&gt;Fig. 7: Adopted from Bernheim et al. [11]&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/1440/1*GlNXjNXv7oAsm2MBJ1ivCA.png&#34; alt=&#34;img&#34;&gt;Fig.8: Adopted from Pan et al. [12]&lt;/p&gt;
&lt;h3 id=&#34;chest-x-ray-vs-rt-pcr&#34;&gt;&lt;strong&gt;Chest X-ray vs. RT-PCR&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Given the limited resources, and to minimize the risk of cross-infection [14], and contamination, clinicians and researchers have investigated &lt;strong&gt;whether a readily available diagnostic imaging, namely X-ray, could be used as a first-line triage tool&lt;/strong&gt;, and help in detecting abnormalities associated with COVID-19 in Chest X-rays, in particular, for asymptomatic patients.&lt;/p&gt;
&lt;p&gt;One of the interesting studies reported by Wong et al. [13] who have studied the appearance of COVID-19 in Chest X-ray, and its correlation with the key findings in the CT scans. Besides, they have investigated  the correlation of Chest X-ray and RT-PCR test.&lt;/p&gt;
&lt;p&gt;In their cohort of 64 patients from Hong Kong, they observed similar key characteristics, appeared in CT scans, such as bilateral, peripheral ground-glass opacity, and/or consolidations (cf. Fig. 9).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/1440/1*uJ-h7jvo2RNUD8A-IqO86w.png&#34; alt=&#34;img&#34;&gt;Fig. 9: Adapted from Wong et al. [13]&lt;/p&gt;
&lt;p&gt;In contrast to the high sensitivity reported for the CT scans, Wong et al. [13] reported a sensitivity of 69% for Chest X-ray, compared to 91% for the initial RT-PCR. The Chest X-ray abnormalities preceded the positive RT-PCR only in 9% (6 out of 64 patients). Examples on the latter scenario are demonstrated in Fig. 10 (A, and B).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/1440/1*FuB_zAuHcVmnYEmUJkoNdw.png&#34; alt=&#34;img&#34;&gt;Fig. 10: Adapted from Wong et al. [13]&lt;/p&gt;
&lt;p&gt;The remarkable low sensitivity indicates a high number of False Negative suggesting further investigation of the abnormalities change over time. Fig.11 shows the changes of severity score in Chest X-ray, where the peak score was reported in 10–12 days since symptoms onset.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/1440/1*8Y0vR85Wue5QMiaEaSz6JQ.png&#34; alt=&#34;img&#34;&gt;Fig.11: Adopted from Wong et al. [13]&lt;/p&gt;
&lt;p&gt;Surprisingly, 86% of the patients (24 out of 28) who had initial positive Chest X-ray, show positive findings on the CT as well. Whereas only one patient of the rest shows no findings in the Chest X-ray, the CT shows peripheral GOO (cf. Fig.12). &lt;strong&gt;These results suggest Chest X-ray might be helpful in monitoring and prognosis, but not recommended for screening.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/1440/1*iPWLY7G9dtwRfhrct4NHQw.png&#34; alt=&#34;img&#34;&gt;Fig. 12: Adapted from Wong et al. [13]&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;At this time, CT screening for the detection of COVID-19 is not recommended by most radiological societies. However, we anticipate that the use of CT in clinical management as well as incidental findings potentially attributable to COVID-19 will evolve. [15]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;community-acquired-pneumonia-cap-vs-covid-19&#34;&gt;&lt;strong&gt;Community Acquired Pneumonia (CAP) vs. COVID-19&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;So far, previous studies report high sensitivity in diagnosing COVID-19 from CT scans, however, with remarkable low specificity, e.g. 25%, and 56% in Ai et al. [6], and Caruso et al. [7], respectively. In other words, radiologists might misinterpret the CT scan and diagnose the patient with COVID-19.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;These studies have shown that COVID-19 often produces a CT pattern resembling organizing pneumonia, notably peripheral ground-glass opacities (GGO) and nodular or mass-like GGO that are often bilateral and multilobar (
&lt;a href=&#34;https://pubs.rsna.org/doi/10.1148/ryct.2020200152#r11&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;11&lt;/a&gt;). However, additional imaging findings have also been reported including linear, curvilinear or perilobular opacities, consolidation, and diffuse GGO, which can mimic several disease processes including other infections, inhalational exposures, and drug toxicities (
&lt;a href=&#34;https://pubs.rsna.org/doi/10.1148/ryct.2020200152#r12&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;12&lt;/a&gt;
&lt;a href=&#34;https://pubs.rsna.org/doi/10.1148/ryct.2020200152#r15&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;–15&lt;/a&gt;). [15]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To assess the performance of radiologists in differentiating COVID-19 from other viral infections, Bai and Hsieh et al. [16] collected a cohort of 424 chest CT scans; 52% with positive COVID-19 by RT-PCR test, and 48% with positive Respiratory Pathogen Panel for viral pneumonia. The cohort was blindly reviewed by three radiologists from China, and a subset of 58 patients were reviewed by four radiologists from the US. Overall, their results demonstrate that &lt;strong&gt;radiologists can distinguish COVID-19 from other viral pneumonia with moderate to high sensitivity 67–93%, and high specificity 93–100%.&lt;/strong&gt; Misinterpreted cases show either subtle or atypical findings in their CT scans (cf. Fig. 13). Key differences have been also reported by the radiologists.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Compared to non-COVID-19 pneumonia, COVID-19 pneumonia was more likely to have a peripheral distribution (80% vs. 57%, p&amp;lt;0.001), ground-glass opacity (91% vs. 68%, p&amp;lt;0.001), fine reticular opacity (56% vs. 22%, p&amp;lt;0.001), and vascular thickening (59% vs. 22%, p&amp;lt;0.001), but less likely to have a central+peripheral distribution (14.% vs. 35%, p&amp;lt;0.001), pleural effusion (4.1 vs. 39%, p&amp;lt;0.001) and lymphadenopathy (2.7% vs. 10.2%, p&amp;lt;0.001).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/1440/1*WWHZpIe7G8OnWhuHjaGIgw.png&#34; alt=&#34;img&#34;&gt;Fig.13:  Adapted from Bai and Hsieh et al. [16]&lt;/p&gt;
&lt;p&gt;To reduce the reporting variability and uncertainty which might arise due to incidental findings with other viral infections, e.g. influenza-A, Simpson and Kay et al. [15] put together a nice piece of work and suggestions on &lt;strong&gt;standardized CT reporting language of COVID-19, which could be considered as a good reference for structured reporting&lt;/strong&gt;. Examples of suggested reporting languages along with a few chest CT images are demonstrated in Fig. 14–16.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/1440/1*0oVLldjnqtvz00RHcBHKHw.jpeg&#34; alt=&#34;img&#34;&gt;Fig.14: Adopted from Simpson and Kay et al. [15]&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/1440/1*uhQ09ognFQ3QjGRf8BSoLg.png&#34; alt=&#34;img&#34;&gt;Fig. 15: Adapted from Simpson and Kay et al. [15]&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/1440/1*M5oT2KtuUWOX49RcO45kqg.png&#34; alt=&#34;img&#34;&gt;Fig. 16: Adapted from Simpson and Kay et al. [15]&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Future direction includes development of an artificial intelligence classifier that can further augment radiologist performance in combination with clinical information. [16]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;from-my-point-of-view-ai-has-the-potential-to&#34;&gt;From my point of view, AI has the potential to:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;identify the asymptomatic carriers of COVID-19&lt;/li&gt;
&lt;li&gt;detect and quantify the abnormalities in serial Chest CT/X-rays scans for prognosis purpose&lt;/li&gt;
&lt;li&gt;distinguish CAP from COVID-19 using Chest CT scans, and additional clinical information; age, gender, previous disorders, …etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;references&#34;&gt;References:&lt;/h3&gt;
&lt;p&gt;[1] Chung, M., Bernheim, A., Mei, X., Zhang, N., Huang, M., Zeng, X., Cui, J., Xu, W., Yang, Y., Fayad, Z.A. and Jacobi, A., 2020. CT imaging features of 2019 novel coronavirus (2019-nCoV). &lt;em&gt;Radiology&lt;/em&gt;, &lt;em&gt;295&lt;/em&gt;(1), pp.202–207. (
&lt;a href=&#34;https://pubs.rsna.org/doi/pdf/10.1148/radiol.2020200230&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[2] Fang, Y., Zhang, H., Xie, J., Lin, M., Ying, L., Pang, P. and Ji, W., 2020. Sensitivity of chest CT for COVID-19: comparison to RT-PCR. &lt;em&gt;Radiology&lt;/em&gt;, p.200432. (
&lt;a href=&#34;https://pubs.rsna.org/doi/10.1148/radiol.2020200432?fbclid=IwAR2EyI4QVRos1SzZvFCl4oMIY0Da06XMbFW1TRmr4P7g3lLyO634O6tBgFs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[3] Xie, X., Zhong, Z., Zhao, W., Zheng, C., Wang, F. and Liu, J., 2020. Chest CT for typical 2019-nCoV pneumonia: relationship to negative RT-PCR testing. &lt;em&gt;Radiology&lt;/em&gt;, p.200343. (
&lt;a href=&#34;https://pubs.rsna.org/doi/abs/10.1148/radiol.2020200343&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[4] Inui S, Fujikawa A, Jitsu M, Kunishima N, Watanabe S, Suzuki Y, Umeda S, Uwabe Y. Chest CT findings in cases from the cruise ship “Diamond Princess” with coronavirus disease 2019 (COVID-19). Radiology: Cardiothoracic Imaging. 2020 Mar 17;2(2):e200110. (
&lt;a href=&#34;https://pubs.rsna.org/doi/full/10.1148/ryct.2020200110&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[5] Simpson, S., Kay, F.U., Abbara, S., Bhalla, S., Chung, J.H., Chung, M., Henry, T.S., Kanne, J.P., Kligerman, S., Ko, J.P. and Litt, H., 2020. Radiological Society of North America Expert Consensus Statement on Reporting Chest CT Findings Related to COVID-19. Endorsed by the Society of Thoracic Radiology, the American College of Radiology, and RSNA. &lt;em&gt;Radiology: Cardiothoracic Imaging&lt;/em&gt;, &lt;em&gt;2&lt;/em&gt;(2), p.e200152. (
&lt;a href=&#34;https://pubs.rsna.org/doi/10.1148/ryct.2020200152&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[6] Ai, T., Yang, Z., Hou, H., Zhan, C., Chen, C., Lv, W., Tao, Q., Sun, Z. and Xia, L., 2020. Correlation of chest CT and RT-PCR testing in coronavirus disease 2019 (COVID-19) in China: a report of 1014 cases. &lt;em&gt;Radiology&lt;/em&gt;, p.200642. (
&lt;a href=&#34;https://pubs.rsna.org/doi/abs/10.1148/radiol.2020200642&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[7] Caruso, D., Zerunian, M., Polici, M., Pucciarelli, F., Polidori, T., Rucci, C., Guido, G., Bracci, B., de Dominicis, C. and Laghi, A., 2020. Chest CT features of COVID-19 in Rome, Italy. &lt;em&gt;Radiology&lt;/em&gt;, p.201237. (
&lt;a href=&#34;https://pubs.rsna.org/doi/full/10.1148/radiol.2020201237&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[8] Fang, Y., Zhang, H., Xie, J., Lin, M., Ying, L., Pang, P. and Ji, W., 2020. Sensitivity of chest CT for COVID-19: comparison to RT-PCR. &lt;em&gt;Radiology&lt;/em&gt;, p.200432. (
&lt;a href=&#34;https://pubs.rsna.org/doi/full/10.1148/radiol.2020200432&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[9] Wang, Y., Dong, C., Hu, Y., Li, C., Ren, Q., Zhang, X., Shi, H. and Zhou, M., 2020. Temporal changes of CT findings in 90 patients with COVID-19 pneumonia: a longitudinal study. &lt;em&gt;Radiology&lt;/em&gt;, p.200843. (
&lt;a href=&#34;https://pubs.rsna.org/doi/full/10.1148/radiol.2020200843&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[10] Guan, W.J., Ni, Z.Y., Hu, Y., Liang, W.H., Ou, C.Q., He, J.X., Liu, L., Shan, H., Lei, C.L., Hui, D.S. and Du, B., 2020. Clinical characteristics of coronavirus disease 2019 in China. &lt;em&gt;New England Journal of Medicine&lt;/em&gt;. (
&lt;a href=&#34;https://www.nejm.org/doi/full/10.1056/NEJMoa2002032&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[11] Bernheim, A., Mei, X., Huang, M., Yang, Y., Fayad, Z.A., Zhang, N., Diao, K., Lin, B., Zhu, X., Li, K. and Li, S., 2020. Chest CT findings in coronavirus disease-19 (COVID-19): relationship to duration of infection. &lt;em&gt;Radiology&lt;/em&gt;, p.200463. (
&lt;a href=&#34;https://pubs.rsna.org/doi/full/10.1148/radiol.2020200463&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[12] Pan, F., Ye, T., Sun, P., Gui, S., Liang, B., Li, L., Zheng, D., Wang, J., Hesketh, R.L., Yang, L. and Zheng, C., 2020. Time course of lung changes on chest CT during recovery from 2019 novel coronavirus (COVID-19) pneumonia. &lt;em&gt;Radiology&lt;/em&gt;, p.200370. (
&lt;a href=&#34;https://pubs.rsna.org/doi/pdf/10.1148/radiol.2020200370&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[13] Wong, H.Y.F., Lam, H.Y.S., Fong, A.H.T., Leung, S.T., Chin, T.W.Y., Lo, C.S.Y., Lui, M.M.S., Lee, J.C.Y., Chiu, K.W.H., Chung, T. and Lee, E.Y.P., 2020. Frequency and distribution of chest radiographic findings in COVID-19 positive patients. &lt;em&gt;Radiology&lt;/em&gt;, p.201160. (
&lt;a href=&#34;https://pubs.rsna.org/doi/10.1148/radiol.2020201160&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[14] American College of Radiology, 2020. ACR recommendations for the use of chest radiography and computed tomography (CT) for suspected COVID-19 infection. 
&lt;a href=&#34;https://www.acr.org/Advocacy-and-Economics/ACR-Position-Statements/Recommendations-for-Chest-Radiography-and-CT-for-Suspected-COVID19-Infection&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;ACR website&lt;/em&gt;&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;[15] Simpson, S., Kay, F.U., Abbara, S., Bhalla, S., Chung, J.H., Chung, M., Henry, T.S., Kanne, J.P., Kligerman, S., Ko, J.P. and Litt, H., 2020. Radiological Society of North America Expert Consensus Statement on Reporting Chest CT Findings Related to COVID-19. Endorsed by the Society of Thoracic Radiology, the American College of Radiology, and RSNA. &lt;em&gt;Radiology: Cardiothoracic Imaging&lt;/em&gt;, &lt;em&gt;2&lt;/em&gt;(2), p.e200152. (
&lt;a href=&#34;https://pubs.rsna.org/doi/10.1148/ryct.2020200152&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[16] Bai, H.X., Hsieh, B., Xiong, Z., Halsey, K., Choi, J.W., Tran, T.M.L., Pan, I., Shi, L.B., Wang, D.C., Mei, J. and Jiang, X.L., 2020. Performance of radiologists in differentiating COVID-19 from viral pneumonia on chest CT. &lt;em&gt;Radiology&lt;/em&gt;, p.200823. (
&lt;a href=&#34;https://pubs.rsna.org/doi/full/10.1148/radiol.2020200823&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Federated Learning in Healthcare</title>
      <link>https://albarqouni.github.io/project/federated-learning/</link>
      <pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/project/federated-learning/</guid>
      <description>&lt;p&gt;Deep Learning (DL) has emerged as a leading technology for accomplishing many challenging tasks showing outstanding performance in a broad range of computer vision and medical applications. However, this success comes at the cost of collecting and processing a massive amount of data, which often are not accessible, in Healthcare, due to privacy issues. Federated Learning (FL) has been recently introduced to allow training DL models without sharing the data. Instead, DL models at local hubs, &lt;em&gt;i.e.&lt;/em&gt; hospitals, share only the trained parameters with a centralized DL model, which is, in return, responsible for updating the local DL models as well.&lt;/p&gt;
&lt;p&gt;Our golas in this project is to develop novel models and algorithms for a ground-breaking new generation of deep FL, which can distill the knowledge from local hubs, &lt;em&gt;i.e.&lt;/em&gt; hospitals, and edges, &lt;em&gt;i.e.&lt;/em&gt; wearable devices, to provide personalized healthcare services.&lt;/p&gt;
&lt;p&gt;The principal &lt;strong&gt;challenges&lt;/strong&gt;, to overcome, concern the nature of medical data, namely data heterogeneity; severe class-imbalance, few amounts of annotated data, inter-/intra-scanners variability (domain shift), inter-/intra-observer variability (noisy annotations), system heterogeneity, and privacy issues (see the example below).&lt;/p&gt;
&lt;h3 id=&#34;collaboration&#34;&gt;Collaboration:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;funding&#34;&gt;Funding:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Soon&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Learn from Crowds</title>
      <link>https://albarqouni.github.io/project/learn-from-crowds/</link>
      <pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/project/learn-from-crowds/</guid>
      <description>&lt;p&gt;Today&amp;rsquo;s clinical procedures often generate a large amount of digital images requiring close inspection. Manual examination by physicians is time-consuming and machine learning in computer vision and pattern recognition is playing an increasing role in medical applications. In contrast to pure machine learning methods, crowdsourcing can be used for processing big data sets, utilising the collective brainpower of huge crowds. Since individuals in the crowd are usually no medical experts, preparation of medical data as well as an appropriate visualization to the user becomes indispensable. The concept of gamification typically allows for embedding non-game elements in a serious game environment, providing an incentive for persistent engagement to the crowd. Medical image analysis empowered by the masses is still rare and only a few applications successfully use the crowd for solving medical problems. The goal of this project is to bring the gamification and crowdsourcing to the Medical Imaging community.&lt;/p&gt;
&lt;h3 id=&#34;collaboration&#34;&gt;Collaboration:&lt;/h3&gt;
&lt;h3 id=&#34;funding&#34;&gt;Funding:&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Learn from Prior Knowledge</title>
      <link>https://albarqouni.github.io/project/learn-from-graph/</link>
      <pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/project/learn-from-graph/</guid>
      <description>&lt;p&gt;Together with our clinical and industry partners, we realized that there is a need to incorporate domain-specific knowledge and let the model &lt;em&gt;Learn from a Prior Knowledge&lt;/em&gt;. We first investigated modeling general priors, i.e., manifold assumptions, to learn powerful representations. Such representations achieved state-of-the-art on benchmark datasets, such as e IDRiD for Diabetic Retinopathy Early Detection (Sarhan &lt;em&gt;et al.&lt;/em&gt; 2019), and 7 Scenes for Camera Relocalization (Bui &lt;em&gt;et al.&lt;/em&gt; 2017). Then, we started looking into the laplacian graph, where prior knowledge can be modeled as a soft constraint, i.e., regularization, to learn feature representation that follows such manifold defined by graphs. We have shown in our ISBI (Kazi &lt;em&gt;et al.&lt;/em&gt; 2019a), MICCAI (Kazi &lt;em&gt;et al.&lt;/em&gt; 2019b), and IPMI (Kazi &lt;em&gt;et al.&lt;/em&gt; 2019) papers that leveraging prior knowledge such as proximity of ages, gender, and a few lab results, are of high importance in Alzheimer classification.&lt;/p&gt;
&lt;h3 id=&#34;collaboration&#34;&gt;Collaboration:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;funding&#34;&gt;Funding:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Siemens AG&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Learn to Adapt</title>
      <link>https://albarqouni.github.io/project/learn-to-adapt/</link>
      <pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/project/learn-to-adapt/</guid>
      <description>&lt;p&gt;To build domain-agnostic models that are generalizable to a different domain, i.e., scanners, we have investigated three directions; First, &lt;em&gt;Style Transfer&lt;/em&gt;, where the style/color of the source domain is transferred to match the target one.  Such style transfer is performed in the high-dimensional image space using adversarial learning, as shown in our papers on Histology Imaging (Lahiani &lt;em&gt;et al.&lt;/em&gt; 2019a, Lahiani &lt;em&gt;et al.&lt;/em&gt; 2019b, Shaban &lt;em&gt;et al.&lt;/em&gt; 2019). Second, &lt;em&gt;Domain Adaptation&lt;/em&gt;, where the distance between the features of the source and target domains are minimized. Such distance can be optimized in a supervised fashion, i.e., class aware, using angular cosine distance as shown in our paper on MS Lesion Segmentation in MR Imaging (Baur &lt;em&gt;et al.&lt;/em&gt; 2017), or in an unsupervised way, i.e., class agnostic, using adversarial learning as explained in our article on Left atrium Segmentation in Ultrasound Imaging (Degel &lt;em&gt;et al.&lt;/em&gt; 2018). Yet, another exciting direction that has been recently investigated in our paper (Lahiani &lt;em&gt;et al.&lt;/em&gt; 2019c) is to disentangle the feature that is responsible for the style and color from the one responsible for the semantics.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Baur_Degel_Shaban.jpeg&#34; alt=&#34;Baur et al. 2017, Degel et al. 2018, and Shaban et al. 2019&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;lahiani2019c.jpeg&#34; alt=&#34;Lahiani et al. 2019c&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;collaboration&#34;&gt;Collaboration:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Eldad Klaiman, Roche Diagnostics GmbH&lt;/li&gt;
&lt;li&gt;Georg Schummers and Matthias Friedrichs, TOMTEC Imaging Systems GmbH&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Learn to Learn</title>
      <link>https://albarqouni.github.io/project/learn-to-learn/</link>
      <pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/project/learn-to-learn/</guid>
      <description>&lt;p&gt;To build models that are transferable to different tasks or different data distributions, i.e., non i.i.d., we have investigated meta-learning approaches such as prototypical networks (PN) (Snell &lt;em&gt;et al.&lt;/em&gt; 2017). PN learns a class prototype from very few amounts of labeled data, e.g., 1-5 shots, and use the learned prototypes to perform the classification tasks. In the context of medical imaging, we were first to introduce Few-Shot Learning into the MIC community. We have shown in our recent ICML Workshop paper (Ayyad &lt;em&gt;et al.&lt;/em&gt; 2019) that our novel Semi-Supervised Few-Shot Learning achieves the state-of-the-art on benchmark datasets; Omniglot, miniImageNet, and TieredImageNet. Further, we have demonstrated in our recent paper (Parida &lt;em&gt;et al.&lt;/em&gt; 2019) that such concepts can be utilized in medical imaging segmentation with an extremely low budget of annotated data, e.g., bounding boxes, and better generalization capability, i.e., to new organs or anomalies, however, at the cost of less accurate segmentation. Yet, our proposed models have great potential in clinical practice where a novel application could come in, and only a very few annotations are required, to perform segmentation tasks. Further, such a learning paradigm has a great potential in Federated Learning, where the data acquired at different hospitals capture heterogeneous and non i.i.d data, i.e., various tasks, making proposed models suitable for such a problem.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Parida2019.jpeg&#34; alt=&#34;Parida et al. 2019&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;collaboration&#34;&gt;Collaboration:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Prof. 
&lt;a href=&#34;https://www.kaust.edu.sa/en/study/faculty/mohamed-elhoseiny&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mohamed Elhoseiny&lt;/a&gt;, 
&lt;a href=&#34;https://ai.facebook.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Facebook AI Research&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Learn to Reason and Explain</title>
      <link>https://albarqouni.github.io/project/learn-to-reason-and-explain/</link>
      <pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/project/learn-to-reason-and-explain/</guid>
      <description>&lt;p&gt;To build explainable AI models that are interpretable for our end-users, i.e., clinicians, we have investigated two research directions. First, we have utilized some visualization techniques to explain and interpret &amp;ldquo;black box&amp;rdquo; models by propagating back the gradient of the class of interest to the image space where you can see the relevant semantics, so-called Gradient Class Activation Maps (GradCAM). Sooner, we found out such techniques do not produce meaningful results. In other words, irrelevant semantics could be highly activated in GradCAM, yielding unreliable explanation tools. To overcome such a problem, we have introduced a robust optimization loss in our MICCAI paper (Khakzar &lt;em&gt;et al.&lt;/em&gt; 2019), which generated adversarial examples enforcing the network to only focus on relevant features and probably correlated with other examples belonging to the same class.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Khakzar2019.jpeg&#34; alt=&#34;Khakzar2019&#34;&gt;&lt;/p&gt;
&lt;p&gt;Second, we have investigated designing and building explainable models by i) uncertainty quantification and ii) disentangled feature representation. In the first category, we started understanding the uncertainty estimates generated by Monte-Carlo Dropout, the approximate of Bayesian Neural Networks, and other techniques, e.g. PointNet, in Camera Relocalization problem (Bui &lt;em&gt;et al.&lt;/em&gt; 2018), to shed light on the ambiguity present in the dataset. We took a step further, and use such uncertainty estimates to refine the segmentation in an unsupervised fashion (Soberanis-Mukul &lt;em&gt;et al.&lt;/em&gt; 2019, Bui &lt;em&gt;et al.&lt;/em&gt; 2019).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Sarhan2019.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Recently, we have investigated modeling the labels uncertainty, which is related to the inter-/intra-observer variability, and produced a metric to quantify such uncertainty. We have shown in our paper (Tomczack &lt;em&gt;et al.&lt;/em&gt; 2019) that such uncertainty can be rather disentangled from the model and data uncertainties, so-called, epistemic, and aleatoric uncertainties, respectively. We believe such uncertainty is of high importance to the referral systems. In the second category, we have studied the variational methods, and disentangled representations, where the assumption here that some generative factors, &lt;em&gt;e.g.&lt;/em&gt;, color, shape, and pathology, will be captured in the lower-dimensional latent space, and one can easily go through the manifold and generate tons of example by sampling from the posterior distribution. We were among the firsts who introduce such concepts in medical imaging by investigating the influence of residual blocks and adversarial learning on disentangled representation (Sarhan &lt;em&gt;et al.&lt;/em&gt; 2019). Our hypothesis that better reconstruction fidelity would force the network to model high resolution, which might have a positive influence on the disentangled representation, in particular, some pathologies.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Roger_Tomczack2019.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;collaboration&#34;&gt;Collaboration:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Dr. 
&lt;a href=&#34;https://scholar.google.de/citations?user=PmHOyT0AAAAJ&amp;amp;hl=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abouzar Eslami&lt;/a&gt;, Carl Zeiss Meditec AG&lt;/li&gt;
&lt;li&gt;PD. Dr. 
&lt;a href=&#34;https://scholar.google.de/citations?user=ELOVd8sAAAAJ&amp;amp;hl=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slobodan Ilic&lt;/a&gt;, Siemens AG&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;funding&#34;&gt;Funding:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Siemens AG&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Learn to Recognize</title>
      <link>https://albarqouni.github.io/project/learn-to-recognize/</link>
      <pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/project/learn-to-recognize/</guid>
      <description>&lt;p&gt;We started investigating Convolutional Neural Networks for Object Recognition in a supervised fashion, for example, mitotic figure detection in histology imaging (Albarqouni &lt;em&gt;et al.&lt;/em&gt; 2016), Catheter electrodes detection and depth estimation in Interventional Imaging (Baur &lt;em&gt;et al.&lt;/em&gt; 2016), femur fracture detection in radiology (Kazi &lt;em&gt;et al.&lt;/em&gt; 2017), in-depth layer X-ray synthesis (Albarqouni &lt;em&gt;et al.&lt;/em&gt; 2017), and pose estimation of mobile X-rays (Bui &lt;em&gt;et al.&lt;/em&gt; 2017). One of the first work which has been highly recognized and featured in the media is AggNet (Albarqouni &lt;em&gt;et al.&lt;/em&gt; 2016) for Mitotic figure detection in Histology Images. Although the network architecture was shallow, it was trained using millions of multi-scale RGB patches of histology images, achieving outstanding performance (ranked 3rd among 15 participants in AMIDA13 challenge).&lt;/p&gt;
&lt;p&gt;During our work, we found out such data-driven models demand a massive amount of annotated data, which might not be available in medical imaging and can not be mitigated by simple data augmentation. Besides, we found out such models are so sensitive to domain shift, i.e., different scanner, and methods such as domain adaptation is required. Therefore, we have focused our research directions to develop fully-automated, high accurate solutions that save export labor and efforts, and mitigate the challenges in medical imaging. For example,  i) the availability of a few annotated data, ii) low inter-/intra-observers agreement, iii) high-class imbalance, iv) inter-/intra-scanners variability and v) domain shift.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Shadi_Web_Images.016.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;To mitigate the problem of limited annotated data, we developed models that &lt;em&gt;Learn from a Few Examples&lt;/em&gt; by i) leveraging the massive amount of unlabeled data via semi-supervised techniques (Baur and Albarqouni &lt;em&gt;et al.&lt;/em&gt; 2017), ii) utilizing weakly labeled data, which is way cheaper than densely one (Kazi &lt;em&gt;et al.&lt;/em&gt; 2017), iii) generating more examples through modeling the data distribution (Baur &lt;em&gt;et al.&lt;/em&gt; 2018), and finally by iv) investigating unsupervised approaches (Baur &lt;em&gt;et al.&lt;/em&gt; 2018, Baur &lt;em&gt;et al.&lt;/em&gt; 2019).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Shadi_Web_Images.017.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;collaboration&#34;&gt;Collaboration:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Prof. 
&lt;a href=&#34;https://www.med.upenn.edu/apps/faculty/index.php/g275/p9161623&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Peter Nöel&lt;/a&gt;, Department of Radiology, 
&lt;a href=&#34;https://www.med.upenn.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Pennsylvania&lt;/a&gt;, USA&lt;/li&gt;
&lt;li&gt;Prof. 
&lt;a href=&#34;https://www.med.physik.uni-muenchen.de/personen/guests/dr_guillaume_landry/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Guillaume Landry&lt;/a&gt;, Department of Radiation Oncology, Medical Center of the University of Munich, Germany&lt;/li&gt;
&lt;li&gt;Dr. 
&lt;a href=&#34;https://www.neurokopfzentrum.med.tum.de/neuroradiologie/forschung_projekt_computational_imaging.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Benedikt Wiestler&lt;/a&gt;, TUM Neuroradiologie, 
&lt;a href=&#34;https://www.mri.tum.de/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Klinikum rechts der Isar&lt;/a&gt;, Germany&lt;/li&gt;
&lt;li&gt;Prof. Dr. med. 
&lt;a href=&#34;https://www.kernspin-maximilianstrasse.de/prof-dr-med-sonja-kirchhoff/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sonja Kirchhoff&lt;/a&gt;, 
&lt;a href=&#34;https://www.mri.tum.de/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Klinikum rechts der Isar&lt;/a&gt;, Germany&lt;/li&gt;
&lt;li&gt;Prof. 
&lt;a href=&#34;[https://www.ls2n.fr/annuaire/Diana%20MATEUS/&#34;&gt;Diana Mateus&lt;/a&gt;, 
&lt;a href=&#34;https://www.ec-nantes.fr/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ecole Centrale Nantes&lt;/a&gt;, France&lt;/li&gt;
&lt;li&gt;Prof. 
&lt;a href=&#34;https://www5.cs.fau.de/en/our-team/maier-andreas/projects/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Andreas Maier&lt;/a&gt;, 
&lt;a href=&#34;https://www.fau.de/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Friedrich-Alexander-Universität Erlangen-Nürnberg&lt;/a&gt;, Germany&lt;/li&gt;
&lt;li&gt;Prof. 
&lt;a href=&#34;https://health.uottawa.ca/people/fallavollita-pascal&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pascal Fallavollita&lt;/a&gt;, 
&lt;a href=&#34;https://www.uottawa.ca/en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ottawa University&lt;/a&gt;, Canada&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;funding&#34;&gt;Funding:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Siemens Healthineers&lt;/li&gt;
&lt;li&gt;Siemens AG&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Modelling Uncertainty in Deep Learning for Medical Applications</title>
      <link>https://albarqouni.github.io/project/uncertainty/</link>
      <pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/project/uncertainty/</guid>
      <description>&lt;p&gt;Deep Learning has emerged as a leading technology for accomplishing many challenging tasks showing outstanding performance in a broad range of applications in computer vision and medical applications. Despite its success and merit in recent state-of-the-art methods, DL tools still lack in robustness hindering its adoption in medical applications. Modeling uncertainty, through Bayesian Inference and Monte-Carlo dropout, has been successfully introduced to computer vision for better understanding the underlying deep learning models. In this proposal, we investigate modeling the uncertainty for medical applications given the well-known challenges in medical image analysis, namely severe class-imbalance, few amounts of labeled data, domain shift, and noisy annotations.&lt;/p&gt;
&lt;h3 id=&#34;collaboration&#34;&gt;Collaboration:&lt;/h3&gt;
&lt;p&gt;Prof. 
&lt;a href=&#34;http://people.ee.ethz.ch/~kender/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ender Konukoglu&lt;/a&gt;, 
&lt;a href=&#34;https://ee.ethz.ch/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Department of Information Technology and Electrical Engineerng&lt;/a&gt;, 
&lt;a href=&#34;https://ethz.ch/en.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ETH Zurich&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Prof. 
&lt;a href=&#34;http://wp.doc.ic.ac.uk/dr/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Daniel Rueckert&lt;/a&gt;, 
&lt;a href=&#34;http://www.imperial.ac.uk/computing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Department of Computing&lt;/a&gt;, 
&lt;a href=&#34;http://www.imperial.ac.uk/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imperial College London&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Prof. 
&lt;a href=&#34;http://campar.in.tum.de/Main/NassirNavab&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nassir Navab&lt;/a&gt;, 
&lt;a href=&#34;http://campar.in.tum.de/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Faculty of Informatics&lt;/a&gt;, 
&lt;a href=&#34;www.tum.de&#34;&gt;Technical University of Munich&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;funding&#34;&gt;Funding:&lt;/h3&gt;
&lt;p&gt;This project is supported by the 
&lt;a href=&#34;https://www.daad.de/de/studieren-und-forschen-in-deutschland/stipendien-finden/prime/prime-fellows-201819/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PRIME programme&lt;/a&gt; of the 
&lt;a href=&#34;www.daad.de&#34;&gt;German Academic Exchange Service (DAAD)&lt;/a&gt; with funds from the 
&lt;a href=&#34;www.bmbf.de&#34;&gt;German Federal Ministry of Education and Research (BMBF)&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Telemedicine in Palestine</title>
      <link>https://albarqouni.github.io/project/telemedicine/</link>
      <pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/project/telemedicine/</guid>
      <description>&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/BXqwyYh8hPU9Ub&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt; &lt;div style=&#34;margin-bottom:5px&#34;&gt; &lt;strong&gt; &lt;a href=&#34;//www.slideshare.net/sbaraqouni/telemedicine-in-palestine&#34; title=&#34;Telemedicine in Palestine&#34; target=&#34;_blank&#34;&gt;Telemedicine in Palestine&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href=&#34;https://www.slideshare.net/sbaraqouni&#34; target=&#34;_blank&#34;&gt;Shadi Nabil Albarqouni&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;
&lt;h3 id=&#34;collaboration&#34;&gt;Collaboration:&lt;/h3&gt;
&lt;h3 id=&#34;funding&#34;&gt;Funding:&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Uncertainty Aware Methods for Camera Pose Estimation and Relocalization</title>
      <link>https://albarqouni.github.io/project/bacatec/</link>
      <pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/project/bacatec/</guid>
      <description>&lt;p&gt;Camera pose estimation is the term for determining the 6-DoF rotation and translation parameters of a camera. It is now a key technology in enabling multitudes of applications such as augmented reality, autonomous driving, human computer interaction and robot guidance. For decades, vision scholars have worked on finding the unique solution of this problem. Yet, this trend is witnessing a fundamental change. The recent school of thought has begun to admit that for our highly complex and ambiguous real environments, obtaining a single solution is not sufficient. This has led to a paradigm shift towards estimating rather a range of solutions in the form of full probability or at least explaining the uncertainty of camera pose estimates. Thanks to the advances in Artificial Intelligence, this important problem can now be tackled via machine learning algorithms that can discover rich and powerful representations for the data at hand. In collaboration, TU Munich and Stanford University plan to devise and implement generative methods that can explain uncertainty and ambiguity in pose predictions. In particular, our aim is to bridge the gap between 6DoF pose estimation either from 2D images/3D point sets and uncertainty quantification through multimodal variational deep methods.&lt;/p&gt;
&lt;h3 id=&#34;collaboration&#34;&gt;Collaboration:&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;http://tbirdal.me/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dr. Tolga Birdal&lt;/a&gt;, 
&lt;a href=&#34;https://profiles.stanford.edu/leonidas-guibas&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. Leonidas Guibas&lt;/a&gt;, Stanford University&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://campar.in.tum.de/Main/MaiBui&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mai Bui&lt;/a&gt;, 
&lt;a href=&#34;%22#about%22&#34;&gt;Dr. Shadi Albarqouni&lt;/a&gt;, 
&lt;a href=&#34;http://campar.in.tum.de/WebHome&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. Nassir Navab&lt;/a&gt;, Technical University of Munich&lt;/p&gt;
&lt;h3 id=&#34;funding&#34;&gt;Funding:&lt;/h3&gt;
&lt;p&gt;This project is funded by the Bavaria California Technology Center (
&lt;a href=&#34;https://www.bacatec.de/en/gefoerderte_projekte.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BaCaTeC&lt;/a&gt;)&lt;/p&gt;
&lt;h3 id=&#34;heading&#34;&gt;&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Organizing Committee Member at MICCAI DART 2020</title>
      <link>https://albarqouni.github.io/talk/dart2020/</link>
      <pubDate>Wed, 01 Apr 2020 13:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/talk/dart2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Organizing Committee Member at MICCAI DCL 2020</title>
      <link>https://albarqouni.github.io/talk/dcl2020/</link>
      <pubDate>Wed, 01 Apr 2020 13:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/talk/dcl2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Uncertainty-based graph convolutional networks for organ segmentation refinement</title>
      <link>https://albarqouni.github.io/publication/soberanis-2019-uncertainty/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/soberanis-2019-uncertainty/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Seamless Virtual Whole Slide Image Synthesis and Validation Using Perceptual Embedding Consistency</title>
      <link>https://albarqouni.github.io/publication/lahiani-2020-seamless/</link>
      <pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/lahiani-2020-seamless/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Invited Talk: Towards Deep Federated Learning in Healthcare</title>
      <link>https://albarqouni.github.io/talk/ulm2019/</link>
      <pubDate>Fri, 17 Jan 2020 09:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/talk/ulm2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Modelling Labels Uncertainty in Medical Imaging</title>
      <link>https://albarqouni.github.io/talk/eth2020/</link>
      <pubDate>Wed, 15 Jan 2020 11:15:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/talk/eth2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Keynote Speaker: AI in Healthcare</title>
      <link>https://albarqouni.github.io/talk/ai4h/</link>
      <pubDate>Wed, 08 Jan 2020 09:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/talk/ai4h/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A learning without forgetting approach to incorporate artifact knowledge in polyp localization tasks</title>
      <link>https://albarqouni.github.io/publication/soberanis-2020-learning/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/soberanis-2020-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An objective comparison of detection and segmentation algorithms for artefacts in clinical endoscopy</title>
      <link>https://albarqouni.github.io/publication/ali-2020-objective/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/ali-2020-objective/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Uncertainty-Driven GCN Refinement Strategy for Organ Segmentation</title>
      <link>https://albarqouni.github.io/publication/mukul-2020-uncertainty/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/mukul-2020-uncertainty/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Attention Based Multiple Instance Learning for Classification of Blood Cell Disorders</title>
      <link>https://albarqouni.github.io/publication/sadafi-2020-attention/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/sadafi-2020-attention/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bayesian Skip-Autoencoders for Unsupervised Hyperintense Anomaly Detection in High Resolution Brain Mri</title>
      <link>https://albarqouni.github.io/publication/baur-2020-bayesian/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/baur-2020-bayesian/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Benefit of dual energy CT for lesion localization and classification with convolutional neural networks</title>
      <link>https://albarqouni.github.io/publication/shapira-2020-benefit/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/shapira-2020-benefit/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Domain Adaptation and Representation Transfer, and Distributed and Collaborative Learning</title>
      <link>https://albarqouni.github.io/publication/albarqouni-2020-domain/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/albarqouni-2020-domain/</guid>
      <description></description>
    </item>
    
    <item>
      <title>GANs for medical image analysis</title>
      <link>https://albarqouni.github.io/publication/kazeminia-2020-gans/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/kazeminia-2020-gans/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Image-to-Images Translation for Multi-Task Organ Segmentation and Bone Suppression in Chest X-Ray Radiography</title>
      <link>https://albarqouni.github.io/publication/eslami-2020-image/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/eslami-2020-image/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Inverse Distance Aggregation for Federated Learning with Non-IID Data</title>
      <link>https://albarqouni.github.io/publication/yeganeh-2020-inverse/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/yeganeh-2020-inverse/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Liver lesion localisation and classification with convolutional neural networks: a comparison between conventional and spectral computed tomography</title>
      <link>https://albarqouni.github.io/publication/shapira-2020-liver/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/shapira-2020-liver/</guid>
      <description></description>
    </item>
    
    <item>
      <title>On the Fairness of Privacy-Preserving Representations in Medical Applications</title>
      <link>https://albarqouni.github.io/publication/sarhan-2020-fairness-2/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/sarhan-2020-fairness-2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Polyp-artifact relationship analysis using graph inductive learned representations</title>
      <link>https://albarqouni.github.io/publication/soberanis-2020-polyp/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/soberanis-2020-polyp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Precise proximal femur fracture classification for interactive training and surgical planning.</title>
      <link>https://albarqouni.github.io/publication/jimenez-2020-precise/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/jimenez-2020-precise/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Retinal Layer Segmentation Reformulated as OCT Language Processing</title>
      <link>https://albarqouni.github.io/publication/tran-2020-retinal/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/tran-2020-retinal/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ROAM: Random Layer Mixup for Semi-Supervised Learning in Medical Imaging</title>
      <link>https://albarqouni.github.io/publication/bdair-2020-roam/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/bdair-2020-roam/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Scale-Space Autoencoders for Unsupervised Anomaly Segmentation in Brain MRI</title>
      <link>https://albarqouni.github.io/publication/baur-2020-scale/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/baur-2020-scale/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SteGANomaly: Inhibiting CycleGAN Steganography for Unsupervised Anomaly Detection in Brain MRI</title>
      <link>https://albarqouni.github.io/publication/baur-2020-steganomaly/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/baur-2020-steganomaly/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Understanding the effects of artifacts on automated polyp detection and incorporating that knowledge via learning without forgetting</title>
      <link>https://albarqouni.github.io/publication/kayser-2020-understanding/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/kayser-2020-understanding/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Invited Talk: Towards Deep Federated Learning in Healthcare</title>
      <link>https://albarqouni.github.io/talk/haicu2019/</link>
      <pubDate>Mon, 16 Dec 2019 09:30:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/talk/haicu2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Keynote Speaker: Towards Deep Federated Learning in Healthcare</title>
      <link>https://albarqouni.github.io/talk/guc2019/</link>
      <pubDate>Wed, 27 Nov 2019 11:15:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/talk/guc2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Organizing Committee Member at MICCAI DART 2019</title>
      <link>https://albarqouni.github.io/talk/dart2019/</link>
      <pubDate>Sun, 13 Oct 2019 16:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/talk/dart2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Organizing Committee Member at MICCAI COMPAY 2019</title>
      <link>https://albarqouni.github.io/talk/compay2019/</link>
      <pubDate>Sun, 13 Oct 2019 13:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/talk/compay2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Keynote Speaker: Towards Deep Federated Learning in Healthcare</title>
      <link>https://albarqouni.github.io/talk/icann2019/</link>
      <pubDate>Thu, 19 Sep 2019 09:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/talk/icann2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Method for determining a pose of an object in an environment of the object using multi task learning and control device</title>
      <link>https://albarqouni.github.io/publication/bui-2019-method/</link>
      <pubDate>Mon, 01 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/bui-2019-method/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Keynote Speaker: Deep Learning in Medical Imaging</title>
      <link>https://albarqouni.github.io/talk/zeiss2019/</link>
      <pubDate>Sun, 16 Jun 2019 13:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/talk/zeiss2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>AI meets COVID-19</title>
      <link>https://albarqouni.github.io/slides/_example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/slides/_example/</guid>
      <description>&lt;h1 id=&#34;brief-progress-of&#34;&gt;Brief Progress of&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic&lt;/a&gt; | 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;pathology-quantification&#34;&gt;Pathology Quantification:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;To be able to quantify the pathologies in thorax CT scans, one needs to segment the pathologies, and probably classify them into common ones characterizing the COVID-19, &lt;em&gt;e.g.&lt;/em&gt;,
&lt;ul&gt;
&lt;li&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;/li&gt;
&lt;/ul&gt;
Ground Glass Opacity (GGO)
&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;/li&gt;
&lt;/ul&gt;
Consolidations
&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;/li&gt;
&lt;/ul&gt;
Scarr
&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;/li&gt;
&lt;/ul&gt;
Pleueral Effusion
&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
   One 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   **Two** 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three 
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://spectrum.chat/academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Preliminary Meeting for the seminar on Federated Learning in Healthcare</title>
      <link>https://albarqouni.github.io/slides/federated/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/slides/federated/</guid>
      <description>&lt;p&gt;Seminar on&lt;/p&gt;
&lt;h2 id=&#34;span-stylecolororangefederated-learningspan-in-span-stylecolorgreenhealthcarespan&#34;&gt;&lt;span style=&#34;color:orange&#34;&gt;Federated Learning&lt;/span&gt; in &lt;span style=&#34;color:green&#34;&gt;Healthcare&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://albarqouni.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Shadi Albarqouni&lt;/a&gt;, PhD&lt;/p&gt;
&lt;p&gt;AI Young Investigator Group Leader at 
&lt;a href=&#34;https://www.helmholtz.ai/themenmenue/our-research/research-groups/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Helmholtz AI&lt;/a&gt; | TUM Junior Fellow at 
&lt;a href=&#34;http://campar.in.tum.de/Main/ShadiAlbarqouni&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TU Munich&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;content&#34;&gt;Content&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Brief introduction about Federated Learning&lt;/li&gt;
&lt;li&gt;It is the right thing at the right time!&lt;/li&gt;
&lt;li&gt;Course structure&lt;/li&gt;
&lt;li&gt;Registration&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;
&lt;a href=&#34;https://federated.withgoogle.com/#about&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;FL_overview.png&#34; alt=&#34;https://federated.withgoogle.com/#about&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;span style=&#34;color:orange&#34;&gt;&lt;strong&gt;Deep Learning&lt;/strong&gt; &lt;/span&gt; success comes at the cost of collecting and processing a &lt;span style=&#34;color:orange&#34;&gt;&lt;strong&gt;massive amount of data&lt;/strong&gt; &lt;/span&gt;, which often are not accessible due to &lt;span style=&#34;color:orange&#34;&gt;&lt;strong&gt;privacy issues&lt;/strong&gt; &lt;/span&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;span style=&#34;color:orange&#34;&gt;&lt;strong&gt;Federated Learning&lt;/strong&gt; &lt;/span&gt; has been recently introduced to allow training DL models &lt;span style=&#34;color:orange&#34;&gt;&lt;strong&gt;without sharing the data&lt;/strong&gt; &lt;/span&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&#34;FL.png&#34; alt=&#34;https://federated.withgoogle.com/#about&#34;&gt;
&lt;sub&gt;Taken from Rieke, N., Hancox, J., Li, W., Milletari, F., Roth, H.R., Albarqouni, S., Bakas, S., Galtier, M.N., Landman, B.A., Maier-Hein, K. and Ourselin, S., 2020. The future of digital health with federated learning. &lt;em&gt;NPJ digital medicine&lt;/em&gt;, &lt;em&gt;3&lt;/em&gt;(1), pp.1-7. &lt;/sub&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;The principal &lt;span style=&#34;color:orange&#34;&gt;&lt;strong&gt;challenges&lt;/strong&gt; &lt;/span&gt;, to overcome, concern the nature of medical data, namely&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;span class=&#34;fragment &#34; &gt;
 &lt;span style=&#34;color:orange&#34;&gt;**Data heterogeneity**&lt;/span&gt;; severe class-imbalance, few amounts of annotated data, inter-/intra-scanners variability (domain shift), and inter-/intra-observer variability (noisy annotations) 
&lt;/li&gt;
&lt;/ul&gt;
&lt;/span&gt;
- &lt;span class=&#34;fragment &#34; &gt;
   &lt;span style=&#34;color:orange&#34;&gt;**System Heterogeneity** &lt;/span&gt; 
&lt;/span&gt;
- &lt;span class=&#34;fragment &#34; &gt;
   &lt;span style=&#34;color:orange&#34;&gt;**Privacy-Issues** &lt;/span&gt; 
&lt;/span&gt;
- &lt;span class=&#34;fragment &#34; &gt;
   &lt;span style=&#34;color:orange&#34;&gt;**Communication Efficiency** &lt;/span&gt; 
&lt;/span&gt;
&lt;hr&gt;
&lt;h2 id=&#34;right-thing-at-the-right-time&#34;&gt;Right thing at the right time&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;Google.png&#34; alt=&#34;https://federated.withgoogle.com/#about&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&#34;SE1.png&#34; alt=&#34;https://federated.withgoogle.com/#about&#34;&gt;
&lt;img src=&#34;SE3.png&#34; alt=&#34;https://federated.withgoogle.com/#about&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&#34;FL_Paper.png&#34; alt=&#34;https://federated.withgoogle.com/#about&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;
&lt;a href=&#34;http://federated-learning.org/fl-neurips-2019/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;NeurIPS.png&#34; alt=&#34;https://federated.withgoogle.com/#about&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;
&lt;a href=&#34;http://federated-learning.org/fl-icml-2020/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;ICML.png&#34; alt=&#34;https://federated.withgoogle.com/#about&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;
&lt;a href=&#34;https://dcl-workshop.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;DCL_MICCAI.png&#34; alt=&#34;https://federated.withgoogle.com/#about&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;
&lt;a href=&#34;http://icfl.cc/SpicyFL/2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;NeurIPS20.png&#34; alt=&#34;https://federated.withgoogle.com/#about&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;course-structurehttpcamparintumdeviewchairteachingws20flhrequirements&#34;&gt;
&lt;a href=&#34;http://campar.in.tum.de/view/Chair/TeachingWs20FLH#Requirements&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Course Structure&lt;/a&gt;&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Basic Info.&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Type:&lt;/strong&gt;  Master Seminar (IN2107)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Language:&lt;/strong&gt; English&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SWS:&lt;/strong&gt; 2&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ECTS:&lt;/strong&gt; 5&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Webpage:&lt;/strong&gt; 
&lt;a href=&#34;https://albarqouni.github.io/courses/flhsose2021/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://albarqouni.github.io/courses/flhsose2021/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Time:&lt;/strong&gt; Fridays, 10 - 12&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Location:&lt;/strong&gt; Virtual Event&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Requirements:&lt;/strong&gt; Solid Background in Machine/Deep Learning&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Tutors&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;tutor.png&#34; alt=&#34;Team&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Objectives:&lt;/strong&gt;
Learn through &lt;span style=&#34;color:orange&#34;&gt;&lt;em&gt;read&lt;/em&gt;, &lt;em&gt;understand&lt;/em&gt;, &lt;em&gt;present&lt;/em&gt;, and &lt;em&gt;discuss&lt;/em&gt;&lt;/span&gt; many scientific papers&lt;sup&gt;1&lt;/sup&gt; tackling the challenges present in Federated Learning.&lt;/p&gt;
&lt;p&gt;&lt;sub&gt;&lt;sup&gt;1&lt;/sup&gt;Our pool of papers includes the ones published in NeurIPS, ICML, ICLR, IEEE TMI, MedIA, MICCAI, MIDL, and ISBI.&lt;/sub&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;schedule&#34;&gt;Schedule:&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;em&gt;Date&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Session&lt;/em&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;16.04.2021&lt;/td&gt;
&lt;td&gt;Federated Learning; Challenges, Methods, and Future&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;30.04.2021&lt;/td&gt;
&lt;td&gt;Data Heterogeneity I&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;14.05.2021&lt;/td&gt;
&lt;td&gt;Data Heterogeneity II&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;28.05.2021&lt;/td&gt;
&lt;td&gt;System Heterogeneity&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;11.06.2021&lt;/td&gt;
&lt;td&gt;Privacy-Issues&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;25.06.2021&lt;/td&gt;
&lt;td&gt;Explainability and Accountability&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;span class=&#34;fragment &#34; &gt;
  &lt;span style=&#34;color:orange&#34;&gt;**Presentation (60%)**:&lt;/span&gt; The selected paper is presented to the other participants (30 minutes presentation plus 10 minutes Q&amp;A) 
&lt;/li&gt;
&lt;/ul&gt;
&lt;/span&gt;
- &lt;span class=&#34;fragment &#34; &gt;
   &lt;span style=&#34;color:orange&#34;&gt;**Blog | Poster (30%)**:&lt;/span&gt; A blog post of 1000-1500 words excluding references should be submitted before the deadline 
&lt;/span&gt;
- &lt;span class=&#34;fragment &#34; &gt;
   &lt;span style=&#34;color:orange&#34;&gt;**Attendance (10%)**:&lt;/span&gt; Students are expected  to participate actively in all seminar sessions 
&lt;/span&gt;
&lt;hr&gt;
&lt;h3 id=&#34;insights-into-the-course-evaluation&#34;&gt;Insights into the course evaluation?&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;If you are interested in this seminsr course, please write a brief motivation paragraph (few lines) showing your interest and your background in Machine/Deep Learning and send it with a subject “FLH_Motivation”, to Shadi Albarqouni (
&lt;a href=&#34;mailto:shadi.albarqouni@tum.de&#34;&gt;shadi.albarqouni@tum.de&lt;/a&gt;). Deadline is 16.02.2021.&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
   &lt;span style=&#34;color:orange&#34;&gt;Don’t forget to register at TUM matching system 11.02 to 16.02.2021: register via [matching.in.tum.de](matching.in.tum.de)&lt;/span&gt; 
&lt;/span&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>Preliminary Meeting for the seminar on Federated Learning in Healthcare</title>
      <link>https://albarqouni.github.io/slides/pres/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/slides/pres/</guid>
      <description>&lt;h2 id=&#34;span-stylecolororangepresentationspan-and-span-stylecolorgreenblog-post-spanguidelines&#34;&gt;&lt;span style=&#34;color:orange&#34;&gt;Presentation&lt;/span&gt; and &lt;span style=&#34;color:green&#34;&gt;Blog Post &lt;/span&gt;Guidelines&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://albarqouni.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Shadi Albarqouni&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;AI Young Investigator Group Leader at 
&lt;a href=&#34;https://www.helmholtz.ai/themenmenue/our-research/research-groups/albarqounis-group/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Helmholtz AI&lt;/a&gt; | TUM Junior Fellow at 
&lt;a href=&#34;http://campar.in.tum.de/Main/ShadiAlbarqouni&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TU Munich&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;05.11.2020&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;span-stylecolororangepresentationspan-guidelines&#34;&gt;&lt;span style=&#34;color:orange&#34;&gt;Presentation&lt;/span&gt; Guidelines&lt;/h2&gt;
&lt;hr&gt;
&lt;ol start=&#34;0&#34;&gt;
&lt;li&gt;
&lt;span class=&#34;fragment &#34; &gt;
 &lt;span style=&#34;color:orange&#34;&gt;**First Slide**:&lt;/span&gt; Please have the title of the paper, authors (&lt;3), conference proceedings, or journal, and the name of the presenter. 
&lt;/li&gt;
&lt;/ol&gt;
&lt;/span&gt;
```
Example: 
&lt;p&gt;The Future of Digital Health with Federated Learning&lt;/p&gt;
&lt;p&gt;Rieke et al., Nature Digital Medicine, 2020&lt;/p&gt;
&lt;p&gt;Presenter: Firstname Surname&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
---
1. &amp;lt;span class=&amp;quot;fragment &amp;quot; &amp;gt;
    &amp;lt;span style=&amp;quot;color:orange&amp;quot;&amp;gt;**Introduction**:&amp;lt;/span&amp;gt; In the first few slides, you need to introduce the subject to the audience. A brief **background** (big picture) and a few **related works** (more concise) would help you to position your paper in the big picture. *It is quite important to talk about the key conclusions at the very beginning*.  The **rationale** for the paper, i.e. *why you did the work?*, has to be addressed by the end of the Intro. slides. 
&amp;lt;/span&amp;gt;

---
2. &amp;lt;span class=&amp;quot;fragment &amp;quot; &amp;gt;
    &amp;lt;span style=&amp;quot;color:orange&amp;quot;&amp;gt;**Methodology**:&amp;lt;/span&amp;gt; You need to explain the method in details, if possible. Start with an overview of the **framework** (e.g. flow chart); input, output, and core components, before you dive deeper into the **key contributions**; e.g. design architecture, objective functions, ...etc. Details that might distract the audience can be moved to the backup slides. In short, explain *how did you do it?*
&amp;lt;/span&amp;gt;
---

3. &amp;lt;span class=&amp;quot;fragment &amp;quot; &amp;gt;
    &amp;lt;span style=&amp;quot;color:orange&amp;quot;&amp;gt;**Experiments and Results**:&amp;lt;/span&amp;gt; You need to explain the **experimental designs**; datasets, evaluation metrics, and training setup, and the **rationale behind them**, before you show the **key results**.  The figures should be clearly labeled, e.g. explain the figures axes before you describe the results addressing the question *what did you find?*
&amp;lt;/span&amp;gt;
---

4. &amp;lt;span class=&amp;quot;fragment &amp;quot; &amp;gt;
    &amp;lt;span style=&amp;quot;color:orange&amp;quot;&amp;gt;**Conclusion &amp;amp; Future Work**:&amp;lt;/span&amp;gt; Discuss the results (your interpretation), before you list the concluding reamrks, learned lessons, and future research directions. 
&amp;lt;/span&amp;gt;

---

5. &amp;lt;span class=&amp;quot;fragment &amp;quot; &amp;gt;
    &amp;lt;span style=&amp;quot;color:orange&amp;quot;&amp;gt;**Group Discussion**:&amp;lt;/span&amp;gt; This is the most important part where you need to list a few major things that you need to discuss with the group, for example: 
	- *How the paper could be improved?* e.g. critique on the proposed method, design choices, missing experiments, or inappropriate evaluation metrics. 
	- *How the paper could be applied in medical domain?* e.g. challenges in healthcare. 
	- Have a look at the reviewers feedback, if available, e.g. [openreview](https://openreview.net/)
 
&amp;lt;/span&amp;gt;

---

6. &amp;lt;span class=&amp;quot;fragment &amp;quot; &amp;gt;
    &amp;lt;span style=&amp;quot;color:orange&amp;quot;&amp;gt;**Needless to Say**:&amp;lt;/span&amp;gt;  
	- Read the paper carefully, and look for complementary materials; blog posts, videos, or code repo. to better understand the paper. 
	- Be mindful of time.  You have 30 mins for points 1-4, and 15 mins for point 5. As a rule of thumb, # slides &amp;lt; given time slot in mins.
	- Build a compelling story and try to engage your audience. 
	- List the References in the footer of the corresponding slide 
	- &amp;lt;span style=&amp;quot;color:orange&amp;quot;&amp;gt;Practice, practice, practice&amp;lt;/span&amp;gt;

&amp;lt;/span&amp;gt;

---
## &amp;lt;span style=&amp;quot;color:green&amp;quot;&amp;gt;Blog Post &amp;lt;/span&amp;gt; Guidelines

---
&amp;gt; The secret to getting into the deep learning community is high quality blogging. Read 5 different blog posts about the same subject and then try to synthesize your own view. Don’t just write something ok, either — take 3 or 4 full days on a post and try to make it as short and simple (yet complete) as possible. 
-- [Andrew Trask](https://hackernoon.com/interview-with-deep-learning-researcher-and-leader-of-openmined-andrew-trask-77cd33570a8c), DeepMind

---

This is a free-style blog post! One can hardly enforce guidelines. However, I personally liked the *Dos and Don&#39;ts* appeared in this [blog post](https://www.fast.ai/2019/05/13/blogging-advice/).  Here some examples
- [What is Federated Learning?
](https://medium.com/@ODSC/what-is-federated-learning-99c7fc9bc4f5)
- [Federated Learning: A Guide to Collaborative Training with Decentralized Sensitive Data – Part 1](https://www.inovex.de/blog/federated-learning-collaborative-training-part-1/)
- [Do we need deep graph neural networks?](https://towardsdatascience.com/do-we-need-deep-graph-neural-networks-be62d3ec5c59)
- [FACT Diagnostic: How to Better Understand Trade-offs Involving Group Fairness](https://blog.ml.cmu.edu/2020/10/12/fact-diagnostic-how-to-better-understand-trade-offs-involving-group-fairness/)
- Other blog sites: [CMU ML Blog](https://blog.ml.cmu.edu/), [BAIR Blog](https://bair.berkeley.edu/blog/)

---
## Helpful resourses 
- [How to read a paper?](https://web.stanford.edu/class/ee384m/Handouts/HowtoReadPaper.pdf) --Three-pass method
- [How to read a research paper?](https://www.eecs.harvard.edu/~michaelm/postscripts/ReadPaper.pdf)
- [How to Read Scientific Papers Quickly &amp;amp; Efficiently](https://medium.com/@drewdennis/how-to-read-scientific-papers-quickly-efficiently-e7030c4018fa)
- [How to write a blog post from your journal article?](https://medium.com/advice-and-help-in-authoring-a-phd-or-non-fiction/how-to-write-a-blogpost-from-your-journal-article-6511a3837caa)
- [Advice for Better Blog Posts](https://www.fast.ai/2019/05/13/blogging-advice/)
---

![PhDComics](PhDComics.png)
Image source: [phdcomics.com](http://phdcomics.com/comics/archive.php?comicid=719). @Jorge Cham

---
Should you have any questions, please drop me an email at shadi.albarqouni@tum.de

[@ShadiAlbarqouni](https://twitter.com/ShadiAlbarqouni)&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Keynote Speaker: Artificial Intelligence. Just Math?</title>
      <link>https://albarqouni.github.io/talk/neocolam2019/</link>
      <pubDate>Sat, 26 Jan 2019 09:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/talk/neocolam2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Adaptive image-feature learning for disease classification using inductive graph networks</title>
      <link>https://albarqouni.github.io/publication/burwinkel-2019-adaptive/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/burwinkel-2019-adaptive/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Adversarial Networks for Camera Pose Regression and Refinement</title>
      <link>https://albarqouni.github.io/publication/bui-2019-adversarial/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/bui-2019-adversarial/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Domain Adaptation and Representation Transfer and Medical Image Learning with Less Labels and Imperfect Data: First MICCAI Workshop, DART 2019, and First International Workshop, MIL3ID 2019, Shenzhen, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 13 and 17, 2019, Proceedings</title>
      <link>https://albarqouni.github.io/publication/wang-2019-domain/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/wang-2019-domain/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Focal loss for artefact detection in medical endoscopy</title>
      <link>https://albarqouni.github.io/publication/kayser-2019-focal/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/kayser-2019-focal/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Fusing unsupervised and supervised deep learning for white matter lesion segmentation</title>
      <link>https://albarqouni.github.io/publication/baur-2019-fusing/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/baur-2019-fusing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Graph Convolution Based Attention Model for Personalized Disease Prediction</title>
      <link>https://albarqouni.github.io/publication/kazi-2019-graph/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/kazi-2019-graph/</guid>
      <description></description>
    </item>
    
    <item>
      <title>InceptionGCN: receptive field aware graph convolutional network for disease prediction</title>
      <link>https://albarqouni.github.io/publication/kazi-2019-inceptiongcn/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/kazi-2019-inceptiongcn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Investigation of Focal Loss in Deep Learning Models For Femur Fractures Classification</title>
      <link>https://albarqouni.github.io/publication/lotfy-2019-investigation/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/lotfy-2019-investigation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learn to estimate labels uncertainty for quality assurance</title>
      <link>https://albarqouni.github.io/publication/tomczack-2019-learn/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/tomczack-2019-learn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learn to Segment Organs with a Few Bounding Boxes</title>
      <link>https://albarqouni.github.io/publication/parida-2019-learn/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/parida-2019-learn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning interpretable disentangled representations using adversarial vaes</title>
      <link>https://albarqouni.github.io/publication/sarhan-2019-learning/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/sarhan-2019-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning Interpretable Features via Adversarially Robust Optimization</title>
      <link>https://albarqouni.github.io/publication/khakzar-2019-learning/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/khakzar-2019-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning-based x-ray image denoising utilizing model-based image simulations</title>
      <link>https://albarqouni.github.io/publication/hariharan-2019-learning/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/hariharan-2019-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Machine Learning and Medical Engineering for Cardiovascular Health and Intravascular Imaging and Computer Assisted Stenting: First International Workshop, MLMECH 2019, and 8th Joint International Workshop, CVII-STENT 2019, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 13, 2019, Proceedings</title>
      <link>https://albarqouni.github.io/publication/liao-2019-machine/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/liao-2019-machine/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MRI to CT Translation with GANs</title>
      <link>https://albarqouni.github.io/publication/kaiser-2019-mri/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/kaiser-2019-mri/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multi-scale Microaneurysms Segmentation Using Embedding Triplet Loss</title>
      <link>https://albarqouni.github.io/publication/sarhan-2019-multi/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/sarhan-2019-multi/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Perceptual Embedding Consistency for Seamless Reconstruction of Tilewise Style Transfer</title>
      <link>https://albarqouni.github.io/publication/lahiani-2019-perceptual/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/lahiani-2019-perceptual/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Preliminary results of DSA denoising based on a weighted low-rank approach using an advanced neurovascular replication system</title>
      <link>https://albarqouni.github.io/publication/hariharan-2019-preliminary/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/hariharan-2019-preliminary/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Self-attention equipped graph convolutions for disease prediction</title>
      <link>https://albarqouni.github.io/publication/kazi-2019-self/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/kazi-2019-self/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Semi-Supervised Few-Shot Learning with Local and Global Consistency</title>
      <link>https://albarqouni.github.io/publication/navab-2019-semi/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/navab-2019-semi/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Semi-Supervised Few-Shot Learning with Prototypical Random Walks</title>
      <link>https://albarqouni.github.io/publication/ayyad-2019-semi/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/ayyad-2019-semi/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Staingan: Stain style transfer for digital histological images</title>
      <link>https://albarqouni.github.io/xtarx.github.io/StainGAN/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/xtarx.github.io/StainGAN/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Towards an Interactive and Interpretable CAD System to Support Proximal Femur Fracture Classification</title>
      <link>https://albarqouni.github.io/publication/jimenez-2019-towards/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/jimenez-2019-towards/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Virtualization of tissue staining in digital pathology using an unsupervised deep learning approach</title>
      <link>https://albarqouni.github.io/publication/lahiani-2019-virtualization/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/lahiani-2019-virtualization/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Determination of the pose of an x-ray unit relative to an object using a digital model of the object</title>
      <link>https://albarqouni.github.io/publication/albarqouni-2018-determination/</link>
      <pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/albarqouni-2018-determination/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Invited Talk: Can Deep Learning Models be Trained with Annotations Collected via Crowdsourcing?</title>
      <link>https://albarqouni.github.io/talk/ecp2018/</link>
      <pubDate>Sun, 09 Sep 2018 16:05:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/talk/ecp2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>7th Joint International Workshop, CVII-STENT (Intravascular Imaging and Computer Assisted Stenting) 2018 and Third International Workshop, LABELS (Large-Scale Annotation of Biomedical Data and Expert Label Synthesis) 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Proceedings</title>
      <link>https://albarqouni.github.io/publication/stoyanov-20187-th/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/stoyanov-20187-th/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A photon recycling approach to the denoising of ultra-low dose X-ray sequences</title>
      <link>https://albarqouni.github.io/publication/hariharan-2018-photon/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/hariharan-2018-photon/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Capsule networks against medical imaging data challenges</title>
      <link>https://albarqouni.github.io/publication/jimenez-2018-capsule/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/jimenez-2018-capsule/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Deep autoencoding models for unsupervised anomaly segmentation in brain MR images</title>
      <link>https://albarqouni.github.io/publication/baur-2018-deep/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/baur-2018-deep/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Deep learning with synthetic data for free water elimination in diffusion MRI</title>
      <link>https://albarqouni.github.io/publication/molina-2018-deep/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/molina-2018-deep/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Domain and geometry agnostic CNNs for left atrium segmentation in 3D ultrasound</title>
      <link>https://albarqouni.github.io/publication/degel-2018-domain/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/degel-2018-domain/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Fine-Tuning Deep Learning by Crowd Participation</title>
      <link>https://albarqouni.github.io/publication/albarqouni-2018-fine/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/albarqouni-2018-fine/</guid>
      <description></description>
    </item>
    
    <item>
      <title>GANs for medical image analysis</title>
      <link>https://albarqouni.github.io/publication/kazeminia-2018-gans/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/kazeminia-2018-gans/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Generalizing multistain immunohistochemistry tissue segmentation using one-shot color deconvolution deep neural networks</title>
      <link>https://albarqouni.github.io/publication/lahiani-2018-generalizing/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/lahiani-2018-generalizing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Generating highly realistic images of skin lesions with GANs</title>
      <link>https://albarqouni.github.io/publication/baur-2018-generating/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/baur-2018-generating/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Intraoperative stent segmentation in X-ray fluoroscopy for endovascular aortic repair</title>
      <link>https://albarqouni.github.io/publication/breininger-2018-intraoperative/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/breininger-2018-intraoperative/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Intravascular Imaging and Computer Assisted Stenting and Large-scale Annotation of Biomedical Data and Expert Label Synthesis: 7th Joint International Workshop, CVII-STENT 2018 and Third International Workshop, LABELS 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Proceedings</title>
      <link>https://albarqouni.github.io/publication/stoyanov-2018-intravascular/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/stoyanov-2018-intravascular/</guid>
      <description></description>
    </item>
    
    <item>
      <title>LABELS 2018 Preface</title>
      <link>https://albarqouni.github.io/publication/sznitman-2018-labels/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/sznitman-2018-labels/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multiple device segmentation for fluoroscopic imaging using multi-task learning</title>
      <link>https://albarqouni.github.io/publication/breininger-2018-multiple/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/breininger-2018-multiple/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Scene coordinate and correspondence learning for image-based localization</title>
      <link>https://albarqouni.github.io/publication/bui-2018-scene/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/bui-2018-scene/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Weakly-supervised localization and classification of proximal femur fractures</title>
      <link>https://albarqouni.github.io/publication/jimenez-2018-weakly/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/jimenez-2018-weakly/</guid>
      <description></description>
    </item>
    
    <item>
      <title>When regression meets manifold learning for object recognition and pose estimation</title>
      <link>https://albarqouni.github.io/publication/bui-2018-regression/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/bui-2018-regression/</guid>
      <description></description>
    </item>
    
    <item>
      <title>X-ray Depthmaps: Revealing the Hidden Structures</title>
      <link>https://albarqouni.github.io/publication/demirci-2018-x/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/demirci-2018-x/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Automatic classification of proximal femur fractures based on attention models</title>
      <link>https://albarqouni.github.io/publication/kazi-2017-automatic/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/kazi-2017-automatic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer</title>
      <link>https://albarqouni.github.io/publication/bejnordi-2017-diagnostic/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/bejnordi-2017-diagnostic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Intravascular Imaging and Computer Assisted Stenting, and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</title>
      <link>https://albarqouni.github.io/publication/cardoso-2017-intravascular/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/cardoso-2017-intravascular/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Machine Learning for Biomedical Applications: From Crowdsourcing to Deep Learning</title>
      <link>https://albarqouni.github.io/publication/albarqouni-2017-machine/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/albarqouni-2017-machine/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Semi-supervised deep learning for fully convolutional networks</title>
      <link>https://albarqouni.github.io/publication/baur-2017-semi/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/baur-2017-semi/</guid>
      <description></description>
    </item>
    
    <item>
      <title>X-ray in-depth decomposition: Revealing the latent structures</title>
      <link>https://albarqouni.github.io/publication/albarqouni-2017-x/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/albarqouni-2017-x/</guid>
      <description></description>
    </item>
    
    <item>
      <title>X-Ray PoseNet: 6 DoF pose estimation for mobile X-Ray devices</title>
      <link>https://albarqouni.github.io/publication/bui-2017-x/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/bui-2017-x/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Anomaly Detection</title>
      <link>https://albarqouni.github.io/codes/anomaly-detection/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/codes/anomaly-detection/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Capsule Networks</title>
      <link>https://albarqouni.github.io/codes/capsule/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/codes/capsule/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Polyp and Artifact Detector</title>
      <link>https://albarqouni.github.io/codes/polyp-detection/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/codes/polyp-detection/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ROAM</title>
      <link>https://albarqouni.github.io/codes/roam/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/codes/roam/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Stain Normlization</title>
      <link>https://albarqouni.github.io/codes/stain-normalization/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/codes/stain-normalization/</guid>
      <description>&lt;h1 id=&#34;staingan&#34;&gt;StainGAN&lt;/h1&gt;
&lt;p&gt;StainGAN implementation based on Cycle-Consistency Concept&lt;/p&gt;
&lt;p&gt;For more information visit 
&lt;a href=&#34;https://xtarx.github.io/StainGAN/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;structure&#34;&gt;Structure&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Stain-Transfer Model&lt;/li&gt;
&lt;li&gt;Pre-processing.&lt;/li&gt;
&lt;li&gt;Post-processing.&lt;/li&gt;
&lt;li&gt;Evaluation&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;datasets&#34;&gt;Datasets&lt;/h2&gt;
&lt;p&gt;The evaluation was done using the Camelyon16 challenge (
&lt;a href=&#34;https://camelyon16.grand-challenge.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://camelyon16.grand-challenge.org/&lt;/a&gt;) consisting of 400 whole-slide images collected
in two different labs in Radboud University Medical Center (lab 1) and University
Medical Center Utrecht (lab 2). Otsu thresholding was used to remove the
background, Afterwards, 40, 000 256 × 256 patches were generated on the x40
magnification level, 30, 000 were used for training and 10, 000 used for validation
from lab 1 and 10, 000 patches were generated for testing from lab 2.&lt;/p&gt;
&lt;p&gt;Patches can be found here: 
&lt;a href=&#34;https://campowncloud.in.tum.de/index.php/s/iGgQ9vdHiMZsFJB?path=%2FStainGAN_camelyon16&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://campowncloud.in.tum.de/index.php/s/iGgQ9vdHiMZsFJB?path=%2FStainGAN_camelyon16&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Any use of the dataset or anypart of the code should be cited&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;If you use this code for your research, please cite our papers.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{shaban2019staingan,
 author = {Shaban, M Tarek and Baur, Christoph and Navab, Nassir and Albarqouni, Shadi},
 booktitle = {2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)},
 organization = {IEEE},
 pages = {953--956},
 title = {Staingan: Stain style transfer for digital histological images},
 year = {2019}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h2&gt;
&lt;p&gt;Code is inspired by 
&lt;a href=&#34;https://github.com/pytorch/examples/tree/master/dcgan&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pytorch-DCGAN&lt;/a&gt; and 
&lt;a href=&#34;https://github.com/junyanz/CycleGAN&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CycleGAN&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Aggnet: deep learning from crowds for mitosis detection in breast cancer histology images</title>
      <link>https://albarqouni.github.io/publication/albarqouni-2016-aggnet/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/albarqouni-2016-aggnet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Cathnets: detection and single-view depth prediction of catheter electrodes</title>
      <link>https://albarqouni.github.io/publication/baur-2016-cathnets/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/baur-2016-cathnets/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Deep Learning and Data Labeling for Medical Applications: First International Workshop, LABELS 2016, and Second International Workshop, DLMIA 2016, Held in Conjunction with MICCAI 2016, Athens, Greece, October 21, 2016, Proceedings</title>
      <link>https://albarqouni.github.io/publication/carneiro-2016-deep/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/carneiro-2016-deep/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Parsing human skeletons in an operating room</title>
      <link>https://albarqouni.github.io/publication/belagiannis-2016-parsing/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/belagiannis-2016-parsing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Playsourcing: a novel concept for knowledge creation in biomedical research</title>
      <link>https://albarqouni.github.io/publication/albarqouni-2016-playsourcing/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/albarqouni-2016-playsourcing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Single-view X-ray depth recovery: toward a novel concept for image-guided interventions</title>
      <link>https://albarqouni.github.io/publication/albarqouni-2016-single/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/albarqouni-2016-single/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Structure-preserving color normalization and sparse stain separation for histological images</title>
      <link>https://albarqouni.github.io/publication/vahadane-2016-structure/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/vahadane-2016-structure/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Gradient projection for regularized cryo-electron tomographic reconstruction</title>
      <link>https://albarqouni.github.io/publication/albarqouni-2015-gradient/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/albarqouni-2015-gradient/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multi-scale Graph-based Guided Filter for De-noising Cryo-Electron Tomographic Data</title>
      <link>https://albarqouni.github.io/publication/albarqouni-2015-multi/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/albarqouni-2015-multi/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Structure-preserved color normalization for histological images</title>
      <link>https://albarqouni.github.io/publication/vahadane-2015-structure/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/vahadane-2015-structure/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Developing MATLAB software for PV and battery sizing for lighting projects in Gaza Strip, Palestine</title>
      <link>https://albarqouni.github.io/publication/hussein-2013-developing/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/hussein-2013-developing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Developing MATLAB software for PV and battery sizing for lighting projects in Gaza Strip, Palestine</title>
      <link>https://albarqouni.github.io/publication/hussein-2012-developing/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/hussein-2012-developing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Developing empirical models for estimating global solar radiation in Gaza Strip, Palestine</title>
      <link>https://albarqouni.github.io/publication/hussein-2010-developing/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/hussein-2010-developing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Re-Evaluation and Re-Design Stand-Alone PV Solar Lighting Projects in Gaza</title>
      <link>https://albarqouni.github.io/publication/albarqouni-2010-re/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/albarqouni-2010-re/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Re-evaluation and re-design stand-alone PV solar lighting projects in Gaza Strip, Palestine</title>
      <link>https://albarqouni.github.io/publication/hussein-2010-re/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/hussein-2010-re/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Study on the Optimum Tilt Angle and Orientation for Photovoltaic Panels and Feasibility Study of One axis-two positions tracking Solar PV in Palestine</title>
      <link>https://albarqouni.github.io/publication/hussein-2010-study/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/hussein-2010-study/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Steps towards Establishing Telemedicine Center in Palestine</title>
      <link>https://albarqouni.github.io/publication/albarqouni-2009-steps/</link>
      <pubDate>Thu, 01 Jan 2009 00:00:00 +0000</pubDate>
      <guid>https://albarqouni.github.io/publication/albarqouni-2009-steps/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
